[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A scalable toolbox for exposing indirect discrimination in insurance rates",
    "section": "",
    "text": "Introduction\nThis online supplement provides code and reproducible examples for the article titled ‘A scalable toolbox for exposing indirect discrimination in insurance rates.’",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#executive-summary",
    "href": "index.html#executive-summary",
    "title": "A scalable toolbox for exposing indirect discrimination in insurance rates",
    "section": "Executive summary",
    "text": "Executive summary\nAccording to actuarial standards of practice, insurance pricing relies on grouping policyholders by risk to set adequate premiums. Modern predictive models, especially machine learning, excel at detecting statistical associations to differentiate risks, but they can learn spurious or undesired correlations. This raises concerns when socioeconomic or demographic factors may (intentionally or inadvertently) affect the fairness of insurance pricing.\nFairness in insurance is difficult to operationalize due to its ambiguity. Fairness metrics from the machine learning literature lack the segment-specific relevance actuaries require and are expressed in abstract units that obscure real-world consequences. For actuaries to intervene, proxy effects and unfair biases must be quantified in insurance-relevant terms: dollars and people.\nIn this paper, we focus on fairness in actuarial pricing. We study the situation where insurance rates should be fair with respect to a categorical (or discretized) sensitive variable, such as race or economic status, and the latter is fully observed (despite the possible privacy challenges). Our main contributions are listed below.\n\nWe argue that actuarial fairness, solidarity, and causality form the three core dimensions of fairness in insurance pricing:\n\nActuarial fairness aligns premiums with expected losses, mitigating cross-subsidies,\nSolidarity aligns premiums across protected groups, mitigating disparities,\nCausality ensures models capture only true risk factors, mitigating proxy effects.\n\nWe translate these dimensions into a five-point spectrum of premiums:\n\nThe best-estimate premium is the most accurate predictor of losses using all available information, including the sensitive variable,\nThe unaware premium is the most accurate predictor of losses using all information except the sensitive variable,\nThe aware premium is the most accurate predictor of losses when controlling for the sensitive variable,\nThe corrective premium is the most accurate predictor that enforces similar premium distributions across levels of the sensitive variable.\nThe hyperaware premium is the most accurate approximation of the corrective premium that does not directly discriminate on the sensitive variable,\n\nWe define actuarially relevant local metrics that quantify the monetary impact of unfairness at the policyholder level.\n\nProxy vulnerability is the difference between unaware and aware premiums. It locally measures how much the allowed variables pick up the signal of a missing sensitive variable.\n\nWe define post pricing local metrics to evaluate the fairness of any pricing structure relative to the estimated spectrum.\nWe partition policyholders to expose the segments in which unfair discrimination is most severe.\nWe integrate these components into a fairness assessment framework that identifies which segments to investigate (via partitioning) and what to measure.\nWe illustrate our approach with a large case study inspired by industry practice. The analysis relies a real dataset of 768,000 vehicles insured in Québec (2016–2017), covering at-fault material damage claims. We examine the fairness of a pseudo commercial price with respect to discretized credit score: low (vulnerable group) vs high.\n\nProxy vulnerability is both material and skewed: while most policyholders may receive a modest rebate, a vulnerable minority of them could face 15–30% overpricing if the regulation only requires that the sensitive variable be omitted,\nOur integrated framework (6  Integrated framework) illustrates that fairness in insurance pricing can be assessed efficiently, with minimal analyst effort. The framework provides simultaneous diagnostics from the three fairness dimensions, translates unfairness into dollar terms at the individual level, and highlights disparities across population segments. Designed for routine portfolio monitoring, our toolbox delivers valuable insights whether or not the sensitive attribute is included in pricing, provided it is available for assessment. The toolbox’s scalability, across large datasets and rich covariate sets, makes fairness operationalizable for actuaries: intuitive, practical, and encompassing the three fairness dimensions.\n\n\nKeywords: Dimensions of fairness; Practical benchmarking; Systematic disparity detection; Comprehensive framework; Operationalizable fairness;",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "A scalable toolbox for exposing indirect discrimination in insurance rates",
    "section": "Outline",
    "text": "Outline\nThis online supplement contains six chapters:\n\n1  Example setup and simulations: Defines the three scenarios and their simulation setup.\n2  Estimating the five fairness premiums: Describes how to estimate the premium spectrum from simulated data.\n4  Measuring the dimensions of fairness: Measures and visualizes the three fairness dimensions.\n3  Actuarial local metrics: Derives pre- and post-pricing fairness metrics, including proxy vulnerability.\n5  Partitioning: Identifies systemic unfairness via partitioning, with repeated experiments.\n6  Integrated framework: Presents a unified approach for fairness assessment and monitoring.\n\n\n\n\nRelated section\nThis sidebar links to the corresponding section of the main paper.\n\n\n\n\n\n\n\nOptimal transport and Wasserstein distance : technical details\n\n\n\nLet \\(Z\\) be a (possibly multivariate) quantity of interest. Let \\(\\mu\\) and \\(\\nu\\) be two laws of \\(Z\\) (finite \\(p\\)-th moments, \\(p\\ge1\\)).\nProbabilistic definition (pairing).\nOver all joint laws with the right marginals, \\[\nW_p(\\mu,\\nu)\n=\\Big(\\inf_{\\,\\mathcal L(Z^{(0)})=\\mu,\\ \\mathcal L(Z^{(1)})=\\nu}\\\n\\mathbb E\\big(\\,\\lVert Z^{(0)}-Z^{(1)}\\rVert^p\\,\\big)\\Big)^{1/p}.\n\\] Reading: the smallest average \\(p\\)-power gap under any admissible pairing.\nUnits: same as \\(Z\\); \\(W_p=0\\) iff \\(\\mu=\\nu\\).\nUnivariate formula (what we compute).\nLet \\(F\\) and \\(G\\) be the CDFs of \\(\\mu\\) and \\(\\nu\\), with quantiles \\(F^{-1}\\) and \\(G^{-1}\\). Then \\[\nW_p^p(\\mu,\\nu)=\\int_{0}^{1}\\!\\big|F^{-1}(u)-G^{-1}(u)\\big|^p\\,du,\n\\qquad\nW_p(\\mu,\\nu)=\\left(\\int_{0}^{1}\\!\\big|F^{-1}-G^{-1}\\big|^p\\,du\\right)^{1/p}.\n\\] For \\(p=1\\), \\[\nW_1(\\mu,\\nu)=\\int_{0}^{1}\\!\\big|F^{-1}(u)-G^{-1}(u)\\big|\\,du\n=\\int_{\\mathbb R}\\!\\big|F(x)-G(x)\\big|\\,dx.\n\\] Actuarial take: \\(W_1\\) is the average absolute gap between matched quantiles (interpretable in dollars if \\(Z\\) is a monetary amount).\nEmpirical, univariate (equal sample sizes).\nGiven samples \\(z^{(0)}_{1:n}\\) from \\(\\mu\\) and \\(z^{(1)}_{1:n}\\) from \\(\\nu\\), sort: \\[\nz^{(0)}_{(1)}\\le\\cdots\\le z^{(0)}_{(n)},\\qquad\nz^{(1)}_{(1)}\\le\\cdots\\le z^{(1)}_{(n)}.\n\\] Then \\[\n\\widehat W_p^p=\\frac{1}{n}\\sum_{i=1}^{n}\\big|z^{(1)}_{(i)}-z^{(0)}_{(i)}\\big|^p.\n\\]\nUnequal exposures (weighted quantiles).\nPick a grid size \\(m\\in\\mathbb N\\) and midpoints \\(u_k=(k-\\tfrac12)/m\\) for \\(k=1,\\dots,m\\) (so \\(\\Delta u=1/m\\)).\nIf \\(Q_1\\) and \\(Q_0\\) are the (exposure-weighted) quantile functions of the two groups, then \\[\n\\widehat W_p^{\\,p}\\ \\approx\\ \\frac{1}{m}\\sum_{k=1}^{m}\\big|Q_1(u_k)-Q_0(u_k)\\big|^p.\n\\]\nWhy this works for actuaries: it compares entire distributions (center and tails), reports in natural units, and is stable—ideal for contrasting groups or monitoring shifts.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_simul_dataset.html",
    "href": "1_simul_dataset.html",
    "title": "1  Example setup and simulations",
    "section": "",
    "text": "1.1 Setup of the three scenarios\nWe begin with loading a few packages:\nConsider potential explanatory variables \\(X_1\\), \\(X_2\\), \\(D\\) to predict \\(Y\\). Let \\(X_1\\) be normally distributed with mean \\(1\\) and variance \\(9\\). Let \\(X_2\\) be a discrete random variable \\(P(X_2 = k) = 1/4\\), for \\(k \\in \\{1, 2, 3, 4\\}\\). Assume \\(D\\) is a binary sensitive characteristic with \\(P(D = 1) = 1/2\\).\nTo analyze proxy effects and potential unfairness, we examine three scenarios, each defined by a different combination of conditional distributions of \\((Y|X_1, X_2, D)\\) and of \\((D|X_1, X_2).\\)\nFor scenarios 1 and 2, the propensity for \\(D = 1\\) is the step function illustrated in the left panel of Figure 2.1. This propensity, \\(P(D = 1|X_1 = x_1, X_2 = x_2)\\), represents the likelihood that individuals with risk profile \\((x_1, x_2)\\) belong to group \\(D = 1\\). It governs the dependence between \\(D\\) and permitted covariates, that is, the core of proxy effects. In scenario 3, the shape of \\(P(D = 1|X_1, X_2)\\) is more complex as shown in the right panel of Figure 2.1.\nFor scenarios 1 and 2, specific equations of the propensity for \\(D = 1\\) is given by \\[\\begin{equation*}\n        P(D = 1|X_1, X_2) =  0.05  + 0.1 X_2 + 0.4 \\times \\mathbf{1}\\{X_1 &gt; E(X_1)\\},\n\\end{equation*}\\] Where \\(\\mathbf{1}\\{A\\} = 1\\) if \\(A\\) is true, and \\(0\\) otherwise. This is a step function with respect to \\(X_2\\), because \\(X_2\\) is discrete. For scenario 3, the propensity is \\[\\begin{equation*}\n         P(D = 1|X_1, X_2) = F^{-1}_B(\\text{expit}[0.05 \\{X_1 - E(X_1)\\}^2 - \\delta]),\n\\end{equation*}\\] where \\(\\text{expit}(x) = \\text{exp}(x)/\\{1 +\\text{exp}(x) \\}\\), the constant \\(\\delta\\) upholds \\(P(D = 1) = 0.5\\), and \\(B\\) is a random variable with distribution \\(\\text{Beta}(\\alpha = f(X_2), \\beta = f(X_2))\\), where \\[\\begin{equation*}\n        f(x) = 999 \\times \\mathbf{1}\\{x = 1\\} + \\mathbf{1}\\{x = 2\\} + 0.4 \\times \\mathbf{1}\\{x = 3\\} + 0.2 \\times \\mathbf{1}\\{x = 4\\}.\n\\end{equation*}\\]\nR functions for the theoretical propensity\nprob_D_unscaled &lt;- function(X1, X2, D = 1, params){\n  lowX1 &lt;- X1 &lt; params[['g_0']]\n  \n  if(params[['scen']] == 3){\n    to_return_eta &lt;- (0.05 * (X1 - params[['g_0']])^2) \n    to_return &lt;- exp(to_return_eta)/(1 + exp(to_return_eta))\n  } else if(params[['scen']] %in% c(1,2)){\n    to_return &lt;- 0.45  + 0.1 * X2 - 0.4 * lowX1\n  }\n  \n  p1 &lt;- to_return\n  \n  if(length(D) == 1){\n    if(D == 1) p &lt;- p1\n    if(D == 0) p &lt;- 1 - p1\n  } else if(length(D) == length(p1)){\n    p &lt;- ifelse(D == 1, p1, 1 - p1)\n  }\n  \n  return(p)\n}\n\nprob_D_margin &lt;- function(d, X2_val = 1:4, params){\n  poss_X2 &lt;- 1:4\n  \n  u_X1 &lt;- seq(0.01, 0.99, length.out = 99)\n  poss_X1 &lt;- qnorm(u_X1, mean = params[['g_0']],\n                   sd = params[['sd_X']])\n  \n  prob_table &lt;- expand.grid('X2' = poss_X2,\n                            'X1' = poss_X1)\n  prob_table$w &lt;- 1/99 * 0.25\n  prob_table$prob &lt;- prob_D_unscaled(prob_table$X1, prob_table$X2,\n                                     D = rep(1, nrow(prob_table)), params = params)\n  \n  to_calc &lt;- prob_table %&gt;% filter(X2 %in% X2_val) %&gt;% \n    mutate(w = w * 4/length(X2_val))\n  p1 &lt;- sum(to_calc$w * to_calc$prob) / sum(to_calc$w)\n  \n  return(\n    ifelse(d == 1, p1, 1- p1)\n  )\n}\n\nprob_D &lt;- function(X1, X2, D = 1, params){\n  lowX1 &lt;- X1 &lt; params[['g_0']]\n  \n  if(params[['scen']] == 3){\n    \n    marg_x2 &lt;- sapply(1:4, function(x2_value){\n      prob_D_margin(d = 1, X2_val = x2_value, params = params)\n      })\n    marg_x2_vec &lt;- sapply(X2, function(x2_val){\n      marg_x2[x2_val]\n    })\n    \n    unsc_probs &lt;- prob_D_unscaled(X1 = X1,\n                                  X2 = X2,\n                                  D = 1,\n                                  params = params)\n    unstr_probs &lt;- unsc_probs - (marg_x2_vec - 0.5)\n    qty_alpha &lt;- ifelse(X2 == 1, 999, \n                        ifelse(X2 == 3, \n                               0.4, \n                               ifelse(X2 == 4, \n                                      0.2, 1)))\n    to_return &lt;- qbeta(unstr_probs, qty_alpha, qty_alpha)\n  } else if(params[['scen']] %in% c(1,2)){\n    to_return &lt;- 0.45  + 0.1 * X2 - 0.4 * lowX1\n  }\n  \n  p1 &lt;- to_return\n  \n  if(length(D) == 1){\n    if(D == 1) p &lt;- p1\n    if(D == 0) p &lt;- 1 - p1\n  } else if(length(D) == length(p1)){\n    p &lt;- ifelse(D == 1, p1, 1 - p1)\n  }\n  \n  return(p)\n}\nFor illustration, we assume that the claim propensity is normally distributed with \\(\\sigma^2 = 45\\) and \\[\\begin{equation} \\label{eq:ex_distY}\nE(Y|X_1, X_2, D) = 100 + 3 X_1 \\{(1 - \\gamma_D) + \\gamma_D D\\} + 15 D,\n\\end{equation}\\] with \\(\\gamma_D = 0\\) (no interaction) for scenario 1 and \\(\\gamma_D = 0.5\\) (interaction) for scenarios 2 and 3. Note that \\(X_2\\) is not a true risk factor for \\(Y\\) in all three scenarios.\nR function of the theoretical expectation\nEsp_Y &lt;- function(X1, X2, D, params){\n  params[['b_0']] +\n    params[['b_x']] * X1 * ((1 - params[['int_d']]) + params[['int_d']] * D) +\n    params[['b_d']] * D \n}\nWe then define set of parameters consistent with the three scenarios.\nScenario parameters\nparms_original &lt;- list('b_0' = 100, # beta0 for P(Y|X, D)\n              'b_d' = 15, # betaD for P(Y|X, D)\n              'b_x' = 4, # betaX for P(Y|X, D)\n              'b_A' = 0, # betaA for P(Y|X, D) (moral hazard)\n              'sd_Y' = sqrt(45), # sd for  P(Y|X, D) (gaussian)\n                       \n              'g_0' = 1, # gamma0 for P(X)\n              'int_d' = 0, # for interaction\n              'scen' = 1, # for propensity\n              'sd_X' = 3 # sd for P(X|D) (gaussian))\n              )\n\nparms &lt;- list('Scenario1' = parms_original,\n              'Scenario2' = {tmp &lt;- parms_original; tmp[['scen']] &lt;- 2; tmp[['int_d']] &lt;- 0.5; tmp},\n              'Scenario3' = {tmp &lt;- parms_original; tmp[['scen']] &lt;- 3; tmp[['int_d']] &lt;- 0.5; tmp})\n\nrm(parms_original)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Example setup and simulations</span>"
    ]
  },
  {
    "objectID": "2_training_spectrum.html",
    "href": "2_training_spectrum.html",
    "title": "2  Estimating the five fairness premiums",
    "section": "",
    "text": "2.1 Best-estimate premium\nIn this section, we provide an example of detailed methodology for estimating the benchmarks premiums from the spectrum of fairness defined in Section 4.1 of the main paper. It complements the estimation of Section 4.1.\nIn this paper, premiums are estimated using lightgbm algorithms from Ke et al. (2017), a package that supports common actuarial distributions (Tweedie, Poisson, Gamma, etc.) and was found to have excellent predictive performance compared to other boosting algorithm in Chevalier and Côté (2024). Given lightgbm’s flexibility, we advocate for careful variable preselection to:\nWe optimize hyperparameters with a validation set to prevent overfitting. Key hyperparameters in lightgbm are the number of trees (num_iterations), regularization (lambda_l1, lambda_l2) for complexity control, and subsampling percentages (feature_fraction, bagging_fraction) .\nThe data used to construct prices and the estimated spectrum should align to ensure comparability. In the Case Study of the main paper, we conduct a posteriori benchmarking, as the study period is historical. A posteriori assessment can reveal segments where the model misaligned from fairness dimensions after the fact. In contrast, conducting benchmarking concurrently with the development of a new pricing model provides an estimate of how well the commercial price aligns with fairness dimensions for future periods, though the true fairness implications of the commercial price will only become evident retrospectively.\nIn what follows, we detail how we estimate each benchmark premium in our framework.\nThe best estimate premium \\(\\widehat{\\mu}^B(\\mathbf{x}, d)\\) serves as the anchor. Thus, its estimation is crucial for our framework. It provides the best predictor of the response variable \\(Y\\) when differentiating risks based on \\((X, D)\\). It can be derived from indicated rates or data-driven (what we do) estimates as detailed in Complement 5 of the main paper.\nTraining the data-driven best-estimate price\nsource('___lgb_best_estimate.R')\n\n## clean the pred repo\nunlink(file.path('preds', \"*_best_estimate.json\"))\n\n# Define grid for hyperparameter optimization\n  hyperparameter_grid &lt;- expand.grid(\n    learning_rate = c(0.01),\n    feature_fraction = c(0.75),\n    bagging_fraction = c(0.75),\n    max_depth = c(5),\n    lambda_l1 = c(0.5),\n    lambda_l2 = c(0.5)\n  )\n\nbest_lgb &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name){\n  list_df &lt;- list('train' = sims[[name]],\n                  'valid' = valid[[name]],\n                  'test' = test[[name]])\n  the_best_estimate_lightgbm_fun(list_data = list_df, \n                                 name = name,\n                                 hyperparameter_grid = hyperparameter_grid)\n})\n\n\nBest for scenario:  Scenario1  \nhyperparam  1 / 1 \nBest valid mse: 53.28979  \noptimal ntree: 605  \nTraining time: 15.16073  sec. \nBest for scenario:  Scenario2  \nhyperparam  1 / 1 \nBest valid mse: 53.43552  \noptimal ntree: 540  \nTraining time: 15.29007  sec. \nBest for scenario:  Scenario3  \nhyperparam  1 / 1 \nBest valid mse: 52.30437  \noptimal ntree: 518  \nTraining time: 13.96573  sec.\nTraining the best-estimate price on experimental data (for later use in section 5)\n# Define grid for hyperparameter optimization\nhyperparameter_grid_sims &lt;- expand.grid(\n    learning_rate = c(0.01),\n    bagging_fraction = c(0.75),\n    max_depth = c(6),\n    lambda_l1 = c(0.5),\n    lambda_l2 = c(0.5)\n  )\n\nbest_sims &lt;- lapply(seq_along(sim_samples), function(idx){\n    list_df &lt;- list('train' = sim_samples[[idx]]$train,\n                  'valid' = sim_samples[[idx]]$valid,\n                  'test' = sim_samples[[idx]]$test)\n  the_best_estimate_lightgbm_fun(list_data = list_df,\n                                 name = paste0('sim', idx),\n                                 hyperparameter_grid = hyperparameter_grid_sims)\n})",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "4_local.html",
    "href": "4_local.html",
    "title": "4  Actuarial local metrics",
    "section": "",
    "text": "Note",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actuarial local metrics</span>"
    ]
  },
  {
    "objectID": "5_partitioning.html",
    "href": "5_partitioning.html",
    "title": "5  Partitioning",
    "section": "",
    "text": "5.1 Supervised partitioning for fairness insights\nFive essential components guide supervised partitions: - The data should represent the population for which fairness is intended, whether the full portfolio, a representative sample, or a specific subpopulation - The partitioning feature space might include allowed and sensitive variables to reveal “minorities within minorities”. Alternatively, focusing on permissible variables ensures future usability, even if sensitive attributes become inaccessible. - The response variable is the key fairness metric, such as fairness range or proxy vulnerability (Sec. 5 of the main paper), guiding the supervised partitioning. - The loss function defines the partitioning objective: MSE targets average differences across segments, while quantile loss highlights the most at-risk individuals. - The algorithm implements the partitioning. While CART is fast, it is only locally optimal. Advanced options like evolutionary trees (evtree in R; Grubinger, Zeileis, and Pfeiffer (2014)) or OSRT Lin et al. (2022) aim for global optimality while preserving tractability. \\end{itemize}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Partitioning</span>"
    ]
  },
  {
    "objectID": "3_dimensions.html",
    "href": "3_dimensions.html",
    "title": "3  Dimensions of fairness",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "6_integrated_framework.html",
    "href": "6_integrated_framework.html",
    "title": "6  Integrated framework",
    "section": "",
    "text": "6.1 Download Disparity Tables",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Integrated framework</span>"
    ]
  },
  {
    "objectID": "1_simul_dataset.html#theoretical-computation-of-the-spectrum-of-fairness",
    "href": "1_simul_dataset.html#theoretical-computation-of-the-spectrum-of-fairness",
    "title": "1  Example setup and simulations",
    "section": "1.2 Theoretical computation of the spectrum of fairness",
    "text": "1.2 Theoretical computation of the spectrum of fairness\nWe create function to calculate theoretically all pricing benchmarks, which involves multiple complex function for the theoretical expression of the corrective premium\n\n\nGrid definition for upcoming graphics\nu_to_cover &lt;- c(seq(0.005, 0.03, length.out = 10),\n                seq(0.035, 0.965, length.out = 80),\n                seq(0.97, 0.995, length.out = 10))\n\ngrid_to_test &lt;- expand.grid(x1 = parms$Scenario1[['g_0']] +\n                              parms$Scenario1[['sd_X']] *\n                              qnorm(u_to_cover),\n                            x2 = 1:4,\n                            d = 0:1)\n\n\n\n\nR functions for numerical integration of the corrective premium\n## Inverse of the expectation, takes an expectation, D and a set of parameters as argument, returns corresponding 'x'. \ninv_Esp_Y &lt;- function(s, D, params){\n  (s - (params[['b_0']] + params[['b_d']] * D))/(\n    params[['b_x']] * ((1 - params[['int_d']]) + params[['int_d']] * D)\n  )\n}\n\n# Precompute a grid for p_d_x1\nprecompute_p_d_x1 &lt;- function(params, x1_grid = seq(-12, 15, length.out = 300)) {\n  # Expand the grid\n  grid &lt;- expand.grid(d = c(0, 1), x1 = x1_grid, x2 = 1:4)\n  \n  # Compute probabilities and densities in a vectorized manner\n  grid$prob &lt;- prob_D(X1 = grid$x1, X2 = grid$x2, D = grid$d, params = params)\n  #grid$dens &lt;- dnorm(grid$x1, mean = params[['g_0']], sd = params[['sd_X']])\n  \n  # Aggregate over X2\n  aggregated &lt;- aggregate(prob ~ d + x1, data = grid, FUN = mean)\n  colnames(aggregated)[3] &lt;- \"p_d_x1\"  # Rename the result column\n  \n  return(aggregated)\n}\n\ncreate_p_d_x1_interpolators &lt;- function(grid) {\n  # Split the grid by d\n  grid_split &lt;- split(grid, grid$d)\n  \n  # Create interpolation functions for d = 0 and d = 1\n  interpolators &lt;- lapply(grid_split, function(subgrid) {\n    approxfun(subgrid$x1, subgrid$p_d_x1,\n              rule = c(2, 2))  # Rule = 2 ensures extrapolation\n  })\n  \n  return(interpolators)\n}\n\n\np_d_x1 &lt;- function(d, x1, params, interpolator) {\n  \n  integrand &lt;- function(x1_val){\n    interpolator(x1_val) * dnorm(x1_val,\n                                 mean = params[['g_0']],\n                                 sd = params[['sd_X']])\n  }\n  \n  # Perform numerical integration over x1\n  lower_bound &lt;- pmin(x1 - 2, qnorm(0.0005, mean = params[['g_0']], sd = params[['sd_X']]))\n  upper_bound &lt;- pmin(x1, qnorm(0.99999, mean = params[['g_0']], sd = params[['sd_X']]))\n  \n  result &lt;- pracma::quad(Vectorize(integrand),\n                         xa = lower_bound,\n                         xb = upper_bound)\n  \n  return(result)\n}\n\nF_S_D &lt;- function(s, D, params, interpolators) {\n  # Compute x1(s)\n  x1s &lt;- inv_Esp_Y(s, D = D, params = params)\n  \n  # Compute P(X1 &lt; x1s)\n  # p_x1s &lt;- pnorm(x1s, mean = params[['g_0']], sd = params[['sd_X']])\n  \n  to_return &lt;- mapply(\n    function(d, x1){\n      \n      interp &lt;- interpolators[[as.character(d)]]\n      p_d_x1(d, x1, params,\n             interpolator = interp) / \n        p_d_x1(d, 99, params,\n               interpolator = interp)},\n    D, x1s\n  )\n  # Compute F_S_D\n  return(to_return)\n}\n\n\ninverse_F_S_D &lt;- function(p, D, params, tolerance = 1e-6, max_iter = 100, interpolators) {\n  # Validate p\n  if (any(p &lt; 0 | p &gt; 1)) stop(\"p must be between 0 and 1\")\n  \n  # Define the root-finding function\n  find_root &lt;- function(one_p, one_D) {\n    tryCatch({\n      uniroot(\n        function(s) F_S_D(s, one_D, params, interpolators) - one_p,  # Use interpolators in F_S_D\n        interval = c(50, 165),  # Specify the interval for s\n        tol = tolerance,\n        maxiter = max_iter\n      )$root\n    }, error = function(e) NA)  # Return NA if uniroot fails\n  }\n  \n  # Vectorize over both p and D using mapply\n  return(mapply(find_root, one_p = p, one_D = D))\n}\n\nmaps_to_corr_theo &lt;- function(mu_B, D, params = params){\n  # Interpolate p_d_x1 for each D and x1s\n  # Compute x1(s)\n  x1_grid &lt;- seq(qnorm(0.0005, mean = params[['g_0']],\n                       sd = params[['sd_X']]),\n                 qnorm(0.99999, mean = params[['g_0']],\n                       sd = params[['sd_X']]),\n                 length.out = 100)\n  precomputed_grid &lt;- precompute_p_d_x1(params, x1_grid)\n  # Step 2: Create interpolators\n  interpolators &lt;- create_p_d_x1_interpolators(precomputed_grid)\n  \n  u_d &lt;- pmin(pmax(F_S_D(mu_B, D, params,\n                         interpolators),\n                   0.005), 0.995)\n\n  \n  \n  ## mu_A = E(mu_B) unconditional\n  inverse_values &lt;- lapply(0:1, function(d){\n    inverse_F_S_D(p = u_d, D = rep(d, length(u_d)),\n                  params = params,\n                  interpolators = interpolators)\n    })\n  rowSums(do.call(cbind, inverse_values)) * 0.5\n}\n\n\n\n\nGeneric premium spectrum and metrics builder\nlevels_for_premiums &lt;- c(\"mu_B\", \"mu_U\", \"mu_A\", \"mu_H\", \"mu_C\")\nlabels_for_premiums &lt;- c(\"$\\\\mu^B$\",\"$\\\\mu^U$\", \"$\\\\mu^A$\", \"$\\\\mu^H$\", \"$\\\\mu^C$\")\n\n## four ingredients for the 5 families of premiums\npremium_generator &lt;- function(best, pdx, maps_to_corr, marg){\n  list(\"mu_B\" = function(x1, x2, d){\n    \n    ## simple call of the best estimate model\n    best(data.frame(X1 = x1,\n                    X2 = x2,\n                    D = d))\n  }, \"mu_U\" = function(x1, x2, d = NULL){\n    \n    ## Explicit inference : mu_U = E(mu_B|X)\n    tab_best &lt;- sapply(0:1, function(dl){\n      best(data.frame(X1 = x1,\n                    X2 = x2,\n                    # X3 = x3,\n                    D = rep(dl, length(x1)))) \n      }) \n    tab_pdx &lt;- sapply(0:1, function(dl){\n      pdx(data.frame(X1 = x1,\n                    X2 = x2,\n                    D = rep(dl, length(x1))))\n      })\n    \n    (tab_best * tab_pdx) %&gt;% apply(1, sum)\n  }, \"mu_A\" = function(x1, x2, d = NULL){\n    \n    ## mu_A = E(mu_B) unconditional\n    sapply(0:1, function(d){best(data.frame(X1 = x1,\n                    X2 = x2,\n                    D = rep(d, length(x1))))*\n       marg[d + 1]}) %&gt;% apply(1, sum)\n  }, \"mu_H\" = function(x1, x2, d = NULL){\n    \n    ## Here we cheated by using our mapping of mu_C\n    ## explicit inference of mu_C : mu_H = E(mu_C|X)\n    tab_corr &lt;- sapply(0:1, function(dl){\n      sb &lt;- best(data.frame(X1 = x1,\n                    X2 = x2,\n                    D = rep(dl, length(x1))))\n      maps_to_corr(data.frame(mu_B = sb, D = dl))\n      }) \n    tab_pdx &lt;- sapply(0:1, function(dl){\n      pdx(data.frame(X1 = x1,\n                    X2 = x2,\n                    D = rep(dl, length(x1))))\n      })\n    \n    (tab_corr * tab_pdx) %&gt;% apply(1, sum)\n  }, \"mu_C\" = function(x1, x2, d = NULL){\n    \n    ## mu_C = T^{d}(mu_B(x, d))\n    mu_b &lt;- best(data.frame(X1 = x1,\n                    X2 = x2,\n                    # X3 = x3,\n                    D = d))\n    maps_to_corr(data.frame(mu_B = mu_b, D = d))\n  })\n}\n\nlevels_for_quants &lt;- c('proxy_vuln', 'risk_spread', 'fair_range', 'parity_cost')\n\nquant_generator &lt;- function(premiums){\n  list('proxy_vuln' = function(x1, x2, d){\n    premiums$mu_U(x1 = x1, x2 = x2) - \n      premiums$mu_A(x1 = x1, x2 = x2)\n  },\n  'risk_spread' = function(x1, x2, x3, d){\n    to_minmax &lt;- data.frame(risk1 = premiums$mu_B(x1 = x1,\n                                              x2 = x2, \n                                              d = 1),\n                            risk0 = premiums$mu_B(x1 = x1,\n                                              x2 = x2, \n                                              d = 0))\n    apply(to_minmax, 1, max) - apply(to_minmax, 1, min) \n  },\n  'fair_range' = function(x1, x2, d){\n    to_minmax &lt;- data.frame(mu_b = premiums$mu_B(x1 = x1, x2 = x2, \n                                           d = d),\n                           mu_u = premiums$mu_U(x1 = x1, x2 = x2, \n                                          d = NULL),\n                           mu_a = premiums$mu_A(x1 = x1, x2 = x2, \n                                          d = NULL),\n                           mu_h = premiums$mu_H(x1 = x1, x2 = x2,\n                                          d = NULL),\n                           mu_c = premiums$mu_C(x1 = x1, x2 = x2, \n                                          d = d))\n    \n    apply(to_minmax, 1, max) - apply(to_minmax, 1, min) \n  },\n  'parity_cost' = function(x1, x2, d){\n    premiums$mu_C(x1 = x1, x2 = x2,\n              d = d) -\n      premiums$mu_B(x1 = x1, x2 = x2, \n                d = d)\n  })\n}\n\n\n\n\nTheoric premium function definition\npremiums_theo &lt;- setNames(nm = names(parms)) %&gt;% lapply(function(name){\n  best_fun_theo &lt;- function(newdata){\n    Esp_Y(X1 = newdata$X1, \n           X2 = newdata$X2, \n           D = newdata$D, \n           params = parms[[name]])\n  }\n  pdx_fun_theo &lt;- function(newdata){\n    prob_D(X1 = newdata$X1, \n           X2 = newdata$X2, \n           D = newdata$D, \n           params = parms[[name]])\n  }\n  maps_to_corr_fun_theo &lt;- function(newdata){\n    maps_to_corr_theo(mu_B = newdata$mu_B,\n                      D = newdata$D,\n                      params = parms[[name]])\n  }\n  marg &lt;- sapply(0:1, function(d){\n    if(name != 'Scenario3'){\n      prob_D_margin(d, params = parms[[name]])\n    } else {\n      0.5\n    }\n    })\n  \n  premium_generator(best = best_fun_theo, \n                  pdx = pdx_fun_theo, \n                  maps_to_corr = maps_to_corr_fun_theo, \n                  marg = marg)\n})\n\nquants_theo &lt;- setNames(nm = names(parms)) %&gt;% lapply(function(name){\n  quant_generator(premiums = premiums_theo[[name]])\n})\n\n\nWe apply the theoretical pricing functions from all families on a small grid dataset to visualize the results\n\n\n\n\n\n\nNote\n\n\n\nAll results were obtained on a single thread under R 4.4.2. Numerical integration for the following code chunk take approximately one hour on an Intel(R) Core(TM) Ultra 7 165H CPU with 32 GB of RAM.\n\n\n\n\nComputation of theoretical premiums across the grid\ndf_to_g_theo_file &lt;- \"preds/df_to_g_theo.json\"\n\n## If the folder do not exist... \nif (!dir.exists('preds')) {\n  dir.create('preds')\n}\n\n# Check if the JSON file exists\nif (file.exists(df_to_g_theo_file)) {\n  message(sprintf(\"[%s] File exists. Reading df_to_g from %s\", Sys.time(), df_to_g_theo_file))\n  df_to_g_theo &lt;- fromJSON(df_to_g_theo_file)\n} else {\n\ndf_to_g_theo &lt;- setNames(nm = names(parms)) %&gt;% lapply(function(name) {\n  message(sprintf(\"[%s] Processing: %s\", Sys.time(), name))\n\n  # Start time for this scenario\n  start_time &lt;- Sys.time()\n  \n  # Compute theoretical premiums\n  message(sprintf(\"[%s] Step 3: Computing theoretical premiums\", Sys.time()))\n  theo_premiums &lt;- setNames(object = levels_for_premiums, nm = paste0(levels_for_premiums, '_t')) %&gt;%\n    sapply(function(s) {\n      message(sprintf(\"[%s] Computing theoretical premium: %s\", Sys.time(), s))\n      premiums_theo[[name]][[s]](\n        x1 = grid_to_test$x1,\n        x2 = grid_to_test$x2,\n        d = grid_to_test$d\n      )\n    })\n  \n  # Compute theoretical PDX\n  message(sprintf(\"[%s] Step 4: Computing theoretical PDX\", Sys.time()))\n  pdx_t_results &lt;- prob_D(\n    X1 = grid_to_test$x1,\n    X2 = grid_to_test$x2,\n    D = grid_to_test$d,\n    params = parms[[name]]\n  )\n  \n  # Combine results\n  message(sprintf(\"[%s] Step 5: Combining results\", Sys.time()))\n  result &lt;- data.frame(\n    grid_to_test,\n    theo_premiums,\n    pdx_t = pdx_t_results\n  )\n  \n  # Log completion time\n  end_time &lt;- Sys.time()\n  message(sprintf(\"[%s] Finished processing: %s (Duration: %.2f seconds)\", end_time, name, as.numeric(difftime(end_time, start_time, units = \"secs\"))))\n  \n  return(result)\n})\n\n# Save the entire df_to_g object to JSON\n  message(sprintf(\"[%s] Saving df_to_g_theo to %s\", Sys.time(), df_to_g_theo_file))\n  toJSON(df_to_g_theo, pretty = TRUE, auto_unbox = TRUE) %&gt;% write(df_to_g_theo_file)\n}\n\n\n\n\nComputation of theoretical local metrics across the grid\ngrid_stats_path_theo &lt;- 'preds/preds_grid_stats_theo.json'\n\n# Check and load or compute preds_grid_stats\nif (file.exists(grid_stats_path_theo)) {\n  preds_grid_stats_theo &lt;- fromJSON(grid_stats_path_theo)\n} else {\n  preds_grid_stats_theo &lt;- setNames(nm = names(parms)) %&gt;% \n    lapply(function(name) {\n      data.frame(df_to_g_theo[[name]], \n                 'risk_spread_t' = quants_theo[[name]][['risk_spread']](x1 = df_to_g_theo[[name]]$x1,\n                                         x2 = df_to_g_theo[[name]]$x2,\n                                         d = df_to_g_theo[[name]]$d),\n                 'proxy_vuln_t' = quants_theo[[name]][['proxy_vuln']](x1 = df_to_g_theo[[name]]$x1,\n                                         x2 = df_to_g_theo[[name]]$x2,\n                                         d = df_to_g_theo[[name]]$d),\n                 \n                 ## computationally quicker (only) for the theoretical case because we want to avoid numerical integration when calling mu_C. \n                 'fair_range_t' = (apply(df_to_g_theo[[name]][c('mu_B_t', 'mu_U_t', 'mu_A_t', 'mu_H_t', 'mu_C_t')], 1, max) -\n                                     apply(df_to_g_theo[[name]][c('mu_B_t', 'mu_U_t', 'mu_A_t', 'mu_H_t', 'mu_C_t')], 1, min)) |&gt;  unname(),\n                 'parity_cost_t' = df_to_g_theo[[name]][['mu_C_t']] - df_to_g_theo[[name]][['mu_B_t']])\n    })\n  toJSON(preds_grid_stats_theo, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(grid_stats_path_theo)\n}\n\n\n\n\nR code producing the propensity illustration across scenarios\nto_save_pdx_t_perpop &lt;- names(df_to_g_theo)[-1] %&gt;% \n  lapply(function(name){\n    cols &lt;- the_CAS_colors\n    pop_id &lt;- which(names(df_to_g_theo) == name)\n    \n    ## keep only axis for last plot\n    if(pop_id == 2){ # If it's the last, apply correct xlabels\n      the_y_scale &lt;- ylim(0,1)\n      the_y_label &lt;- latex2exp::TeX(\"$P(D = 1|x_1, x_2)$\")\n    } else { # otherwise, remove everything\n      the_y_scale &lt;- scale_y_continuous(labels = NULL, limits = c(0,1))\n      the_y_label &lt;- NULL\n    }\n    \n    the_pops &lt;- c(NA, 'Scenario 1 and 2', 'Scenario 3')\n    \n    ## lets graph\n    df_to_g_theo[[name]] %&gt;% \n      mutate(the_population = name) %&gt;% \n  filter(x1 &gt;= -9, x1 &lt;= 11,\n         d == 1) %&gt;% \n  group_by(x1, x2) %&gt;% summarise(pdx = mean(pdx_t)) %&gt;%  ungroup %&gt;% \n  ggplot(aes(x = x1, y = pdx,\n             lty = factor(x2),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(x2),\n             color = factor(x2))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '31', '21', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(2, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"),\n       y = the_y_label,\n       title = latex2exp::TeX(the_pops[pop_id])) + \n  scale_x_continuous(breaks = c(-3:3)*3 + 1)  + # see above\n  theme_minimal() + \n  the_y_scale +\n  theme(plot.title = element_text(hjust=0.5))\n  }) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           nrow = 1,\n                           widths = 15, heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nif (!dir.exists('figs')) dir.create('figs')\nggsave(filename = \"figs/graph_pdx_t_perpop.png\",\n       plot = to_save_pdx_t_perpop,\n       height = 3.25,\n       width = 7.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 1.1: Propensity in terms of \\(x_1\\) and \\(x_2\\) for simulations\n\n\n\n\n\n\nFigure 1.2 shows three premiums: best-estimate, unaware, and aware. The best-estimate premium in red correctly ignores \\(X_2\\) but directly discriminates on \\(D\\): for a given value of \\(x_1\\), an individual with \\(d = 1\\) (dashed) pays a higher premium than one with \\(d = 0\\) (solid). The unaware premium in orange exploits \\(X_2\\) (different line widths) to reveal information about the omitted \\(D\\), mirroring the shape of the propensity in Figure 2.1. By controlling for \\(D\\), the aware premium (green) prevents proxy effects without differentiating prices on \\(D\\) or \\(X_2\\).\n\n\nR code producing the best-estimate, unaware, aware illustration.\nto_save_premiumsdense_BUA_t_perpop &lt;- names(df_to_g_theo) %&gt;% lapply(function(pop_name){\n  \n  ## the colors\n    cols &lt;- RColorBrewer::brewer.pal(5, 'Spectral')[1:3] %&gt;%  colorspace::darken(0.25)\n      # the_CAS_colors_full[c(6, 5, 2)]\n  pop_id &lt;- which(names(parms) == pop_name)\n  \n  ## keep only axis for last plot\n    if(pop_name == head(names(df_to_g_theo), 1)){ # If it's the last, apply correct xlabels\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = scales::dollar,\n                     limits = c(90, 140))\n      the_y_label &lt;- \n  latex2exp::TeX(\"$\\\\mu(x_1, x_2, d)$\")\n    } else { # otherwise, remove everything\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = NULL,\n                     limits = c(90, 140))\n      the_y_label &lt;- NULL\n    }\n  \n  to_illustrate &lt;- df_to_g_theo[[pop_name]] %&gt;%\n  reshape2::melt(measure.vars = paste0(levels_for_premiums[1:3], '_t')) %&gt;% \n    mutate(variable = factor(variable,\n                             levels = paste0(levels_for_premiums[1:3], '_t'),\n                             labels = labels_for_premiums[1:3])) %&gt;% \n  filter(x1 &lt;= 10, x1 &gt;= -8) \n  # group_by(x1, d, variable) %&gt;% summarise(value = mean(value)) %&gt;%  ungroup %&gt;% \n  \n  ## mask part of the line \n  to_illustrate$value[to_illustrate$pdx_t &lt; 0.1] &lt;- NA\n  \n  to_illustrate %&gt;% \n  ggplot(aes(x = x1, y = value,\n             lty = factor(d),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(d),\n             color = factor(variable))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '32'), name = latex2exp::TeX('$d$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$\\\\mu$'), labels = latex2exp::TeX) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.35, 0.7), name = latex2exp::TeX('$d$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"), y = the_y_label,\n       title = paste0('Scenario ', pop_id)) +\n  scale_x_continuous(breaks = c( -3, 0, 3, 6), limits = c(-4, 7)) +\n  the_y_scale +\n  theme_minimal()\n}) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           ncol = 3,\n                           widths = c(18, 15, 15), heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nggsave(filename = \"figs/graph_premiumsdense_BUA_t_perpop.png\",\n       plot = to_save_premiumsdense_BUA_t_perpop,\n       height = 4,\n       width = 10.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nRelated section\nThe five families of the Spectrum of fairness are described in Section 4.2 of the main paper\n\n\n\n\nFigure 1.2: Best-estimate \\(\\mu^B\\) (red), unaware \\(\\mu^U\\) (orange), and aware \\(\\mu^A\\) (green) premiums for scenarios 1, 2, and 3 in the Example as a function of \\(x_1\\), \\(x_2\\), and \\(d\\).\n\n\n\n\n\n\nFigure 1.3 illustrates the aware, hyperaware, and corrective premiums. The aware in olive green is the same as in Figure 1.2. The hyperaware in forest green uses \\(X_2\\) as a proxy for \\(D\\) to approach the corrective premium in blue. The latter transforms the best-estimate premium to enforce demographic parity. In Scenarios 1 and 2, this induces positive discrimination, reversing the order of \\(d = 1\\) (dashed) and \\(d = 0\\) (solid) curves compared to Figure 1.2. In Scenario 3, achieving parity requires only slight adjustments to the best-estimate, producing the crossing pattern in the third panel of Figure 1.3.\n\n\nR code producing the aware, hyperaware, corrective illustration.\nto_save_premiumsdense_AHC_t_perpop &lt;- names(df_to_g_theo) %&gt;% lapply(function(pop_name){\n  \n  ## the colors\n    cols &lt;- RColorBrewer::brewer.pal(5, 'Spectral')[3:5] %&gt;%  colorspace::darken(0.25)\n  pop_id &lt;- which(names(parms) == pop_name)\n  \n  ## keep only axis for last plot\n    if(pop_name == head(names(df_to_g_theo), 1)){ # If it's the last, apply correct xlabels\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = scales::dollar,\n                     limits = c(90, 130))\n      the_y_label &lt;- \n  latex2exp::TeX(\"$\\\\mu(x_1, x_2, d)$\")\n    } else { # otherwise, remove everything\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = NULL,\n                     limits = c(90, 130))\n      the_y_label &lt;- NULL\n    }\n  \n  to_illustrate &lt;- df_to_g_theo[[pop_name]] %&gt;%\n  reshape2::melt(measure.vars = paste0(levels_for_premiums[3:5], '_t')) %&gt;% \n    mutate(variable = factor(variable,\n                             levels = paste0(levels_for_premiums[3:5], '_t'),\n                             labels = labels_for_premiums[3:5])) %&gt;% \n  filter(x1 &lt;= 10, x1 &gt;= -8) \n  # group_by(x1, d, variable) %&gt;% summarise(value = mean(value)) %&gt;%  ungroup %&gt;% \n  \n  ## mask part of the line \n  to_illustrate$value[to_illustrate$pdx_t &lt; 0.1] &lt;- NA\n  \n  to_illustrate %&gt;% \n  ggplot(aes(x = x1, y = value,\n             lty = factor(d),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(d),\n             color = factor(variable))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '32'), name = latex2exp::TeX('$d$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$\\\\mu$'), labels = latex2exp::TeX) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.35, 0.7), name = latex2exp::TeX('$d$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"), y = the_y_label,\n       title = paste0('Scenario ', pop_id)) +\n  scale_x_continuous(breaks = c( -3, 0, 3, 6), limits = c(-4, 7)) +\n  the_y_scale +\n  theme_minimal()\n}) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           ncol = 3,\n                           widths = c(18, 15, 15),\n                            heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nggsave(filename = \"figs/graph_premiumsdense_AHC_t_perpop.png\",\n       plot = to_save_premiumsdense_AHC_t_perpop,\n       height = 4,\n       width = 10.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 1.3: Aware \\(\\mu^A\\), hyperaware \\(\\mu^H\\), and corrective \\(\\mu^C\\) premiums for scenarios 1, 2, and 3 in the Example as a function of \\(x_1\\), \\(x_2\\), and \\(d\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Example setup and simulations</span>"
    ]
  },
  {
    "objectID": "1_simul_dataset.html#simulating-from-the-scenarios",
    "href": "1_simul_dataset.html#simulating-from-the-scenarios",
    "title": "1  Example setup and simulations",
    "section": "1.3 Simulating from the scenarios",
    "text": "1.3 Simulating from the scenarios\nWe start with the generic simulating function, which takes desired number of simulation n and a set of parameters params as argument.\n\n\nGeneric R function for simulations from our example\nsimulate_ex_art &lt;- function(n = 1e6,\n                            params){\n  \n  ## dependent (X, D)\n  X1 &lt;- params[['g_0']] + params[['sd_X']] *\n    rnorm(n)\n  \n  X2 &lt;- sample(1:4, size = n, replace = TRUE)\n  \n  pD &lt;- prob_D(X1, X2, D = rep(1, n), params = params)\n  \n  D &lt;- rbinom(n, size = 1, prob = pD)\n  \n  ## Final elements\n  Y &lt;- rnorm(n, mean = Esp_Y(X1, X2, D, params), \n             sd = params[['sd_Y']] + D)\n  \n  ## Ok goodbye now\n  to_return &lt;- data.frame('D' = D,\n                          'X1' = X1,\n                          'X2' = X2,\n                          'Y' = Y)\n  \n  return(to_return)\n}\n\n\nWe simulate 1e5/1e4/1e4 observations for train/valid/test per scenario.\n\n\nSimulations of the three scenarios\nn &lt;- 1e5\n\n## the simulation files\nfiles_sims &lt;- paste0('simuls/',\n                     c('train', 'valid', 'test'),\n                     '_scenarios.json')\n\n## If the folder do not exist... \nif (!dir.exists('simuls')) {\n  dir.create('simuls')\n}\n\n## We read the simulations, or we do them and save them (json for general data format and easy communication with Python)\nif(file.exists(files_sims[1])){# load\n  sims &lt;- jsonlite::fromJSON(files_sims[1])\n} else {# simulate and write\n  set.seed(321)\n  sims &lt;- parms %&gt;% lapply(function(parameters){\n  simulate_ex_art(n = n, params = parameters)\n  })\n  sims %&gt;% jsonlite::toJSON(., pretty = TRUE) %&gt;% write(files_sims[1])\n}\n\nif(file.exists(files_sims[2])){ # load\n  valid &lt;- jsonlite::fromJSON(files_sims[2])\n} else { # simulate and write\n  set.seed(123)\n  valid &lt;- parms %&gt;% lapply(function(parameters){\n  simulate_ex_art(n = n/10, params = parameters)\n})\n  valid %&gt;% jsonlite::toJSON(., pretty = TRUE) %&gt;% write(files_sims[2])\n}\n\nif(file.exists(files_sims[3])){ # load\n  test &lt;- jsonlite::fromJSON(files_sims[3])\n} else { # simulate and write\n  set.seed(132)\n  test &lt;- parms %&gt;% lapply(function(parameters){\n  simulate_ex_art(n = n/10, params = parameters)\n  })\n  test %&gt;% jsonlite::toJSON(., pretty = TRUE) %&gt;% write(files_sims[3])\n}\n\n\n\n\nAdditional set of simulations for later use (section 5)\nfile_sims &lt;- 'simuls/sim_study.json'\nif(file.exists(file_sims)){ # load\n  sim_samples &lt;- jsonlite::fromJSON(file_sims)\n} else { # simulate and write\n  set.seed(132)\n  sim_samples &lt;- lapply(1:100, function(idx){\n  nsmall &lt;- 2e3\n  \n  list('train' = simulate_ex_art(n = nsmall, params = parms$Scenario1),\n      'valid' = simulate_ex_art(n = nsmall/4, params = parms$Scenario1),\n      'test' = simulate_ex_art(n = nsmall/4, params = parms$Scenario1))\n  })\n  names(sim_samples) &lt;- paste0('sim_', 1:100)\n  \n  sim_samples %&gt;% jsonlite::toJSON(., pretty = TRUE) %&gt;% write(file_sims)\n}\n\n\n\n\nComputation of theoretical premiums across the simulated samples (for later use)\npop_to_g_theo_file &lt;- \"preds/pop_to_g_theo.json\"\n\ncreate_1d_interpolators &lt;- function(grid_data, premium_names) {\n  # Create interpolators for each premium in premium_names\n  interpolators &lt;- lapply(premium_names, function(premium_name) {\n    split(grid_data, list(grid_data$d, grid_data$x2)) %&gt;%\n      lapply(function(group) {\n        # Ensure data is sorted by x1 for interpolation\n        group &lt;- group[order(group$x1), ]\n        \n        # Define bounds\n        min_x1 &lt;- min(group$x1)\n        max_x1 &lt;- max(group$x1)\n        \n        # Create 1D interpolation function\n        interpolator &lt;- approxfun(group$x1, group[[premium_name]], rule = 2)  # Rule = 2 for extrapolation\n        \n        # Wrap the interpolator with bounds checking\n        function(x) {\n          x &lt;- pmin(pmax(x, min_x1), max_x1)  # Clip x to the grid range\n          interpolator(x)\n        }\n      })\n  })\n  \n  # Name the list by premium names\n  names(interpolators) &lt;- premium_names\n  return(interpolators)\n}\n\ninterpolate_to_simulated &lt;- function(simulated_data, interpolators) {\n  # Initialize a list to store interpolated results for each premium\n  interpolated_results &lt;- list()\n  \n  # Iterate over premium names\n  for (premium_name in names(interpolators)) {\n    interpolated_premiums &lt;- numeric(nrow(simulated_data))\n    \n    # Iterate over unique (D, X2) combinations in the simulated data\n    unique_combinations &lt;- unique(simulated_data[, c(\"D\", \"X2\")])\n    \n    for (i in seq_len(nrow(unique_combinations))) {\n      combo &lt;- unique_combinations[i, ]\n      d_value &lt;- combo$D\n      x2_value &lt;- combo$X2\n      \n      # Select the appropriate interpolator\n      interpolator_key &lt;- paste(d_value, x2_value, sep = \".\")\n      interpolator &lt;- interpolators[[premium_name]][[interpolator_key]]\n      \n      # Find rows matching this combination\n      matching_rows &lt;- which(simulated_data$D == d_value & simulated_data$X2 == x2_value)\n      \n      # Apply interpolation on X1 for these rows\n      interpolated_premiums[matching_rows] &lt;- interpolator(simulated_data$X1[matching_rows])\n    }\n    \n    # Store the interpolated results\n    interpolated_results[[premium_name]] &lt;- interpolated_premiums\n  }\n  \n  # Combine results into a data frame\n  interpolated_df &lt;- do.call(cbind, interpolated_results)\n  return(interpolated_df)\n}\n\n# Check if the JSON file exists\nif (file.exists(pop_to_g_theo_file)) {\n  message(sprintf(\"[%s] File exists. Reading pop_to_g_theo from %s\", Sys.time(), pop_to_g_theo_file))\n  pop_to_g_theo &lt;- fromJSON(pop_to_g_theo_file)\n} else {\n\npop_to_g_theo &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name) {\n  message(sprintf(\"[%s] Processing: %s\", Sys.time(), name))\n  \n  # Start time for this simulation\n  start_time &lt;- Sys.time()\n  \n  list_data &lt;- list('train' = sims[[name]],\n                    'valid' = valid[[name]],\n                    'test' = test[[name]])\n  \n  result &lt;- setNames(nm = names(list_data)) %&gt;% lapply(function(nm){\n    \n    data &lt;- list_data[[nm]]\n    \n  # Step 3: Compute theoretical premiums (EXCEPT muC AND muC, we interpolate here)\n  message(sprintf(\"[%s] Step 3: Computing theoretical premiums (no muC and muC)\", Sys.time()))\n  theo_premiums_except_muH_muC &lt;- setNames(object = setdiff(levels_for_premiums, c(\"mu_H\", \"mu_C\")), nm = paste0(setdiff(levels_for_premiums, c(\"mu_H\", \"mu_C\")), '_t')) %&gt;%\n    sapply(function(s) {\n      message(sprintf(\"[%s] Computing theoretical premium: %s\", Sys.time(), s))\n      premiums_theo[[name]][[s]](\n        x1 = data$X1,\n        x2 = data$X2,\n        d = data$D\n      )\n    })\n  \n  # Step 3b: interpolate from the grid to get the muC_t and muC_t values \n  message(sprintf(\"[%s] Step 3b: Interpolating mu_C and mu_H from grid\", Sys.time()))\n  interpolators &lt;- create_1d_interpolators(df_to_g_theo[[name]], c(\"mu_H_t\", \"mu_C_t\"))\n  interpolated_muH_muC_t &lt;- interpolate_to_simulated(data, interpolators)\n  \n  # Step 4: Compute theoretical PDX\n  message(sprintf(\"[%s] Step 4: Computing theoretical PDX\", Sys.time()))\n  pdx_t_results &lt;- prob_D(\n    X1 = data$X1,\n    X2 = data$X2,\n    D = data$D,\n    params = parms[[name]]\n  )\n  \n  # Combine results\n  message(sprintf(\"[%s] Step 5: Combining results\", Sys.time()))\n  data.frame(\n    data,\n    theo_premiums_except_muH_muC,\n    interpolated_muH_muC_t,\n    pdx_t = pdx_t_results\n  )\n  }) \n  \n  \n  # Log completion time\n  end_time &lt;- Sys.time()\n  message(sprintf(\"[%s] Finished processing: %s (Duration: %.2f seconds)\", end_time, name, as.numeric(difftime(end_time, start_time, units = \"secs\"))))\n  \n  return(result)\n})\n\n# Save the entire df_to_g object to JSON\n  message(sprintf(\"[%s] Saving pop_to_g_theo to %s\", Sys.time(), pop_to_g_theo_file))\n  toJSON(pop_to_g_theo, pretty = TRUE, auto_unbox = TRUE) %&gt;% write(pop_to_g_theo_file)\n}\n\n\n\n\nComputation of theoretical premiums across the experiment samples (for later use in section 5)\nsims_to_g_theo_file &lt;- \"preds/sims_to_g_theo.json\"\n\n# Check if the JSON file exists\nif (file.exists(sims_to_g_theo_file)) {\n  message(sprintf(\"[%s] File exists. Reading sims_to_g_theo from %s\", Sys.time(), sims_to_g_theo_file))\n  sims_to_g_theo &lt;- fromJSON(sims_to_g_theo_file)\n} else {\n\nsims_to_g_theo &lt;- setNames(object = seq_along(sim_samples), \n                      nm = names(sim_samples)) %&gt;% lapply(function(idx) {\n  message(sprintf(\"[%s] Processing: %s\", Sys.time(), paste0(idx, '/', length(sim_samples))))\n  \n  samples_to_ret &lt;- setNames(nm = names(sim_samples[[idx]])) %&gt;% lapply(function(nm){\n    data &lt;- sim_samples[[idx]][[nm]]\n  \n  # Step 3: Compute theoretical premiums (EXCEPT muH AND muC, we interpolate here)\n  message(sprintf(\"[%s] Step 3: Computing theoretical premiums (no muC and muH)\", Sys.time()))\n  theo_premiums_except_muH_muC &lt;- setNames(object = setdiff(levels_for_premiums, c(\"mu_H\", \"mu_C\")), nm = paste0(setdiff(levels_for_premiums, c(\"mu_H\", \"mu_C\")), '_t')) %&gt;%\n    sapply(function(s) {\n      message(sprintf(\"[%s] Computing theoretical premium: %s\", Sys.time(), s))\n      premiums_theo[['Scenario1']][[s]](\n        x1 = data$X1,\n        x2 = data$X2,\n        d = data$D\n      )\n    })\n  \n  # Step 3b: interpolate from the grid to get the SH_t and SC_t values \n  message(sprintf(\"[%s] Step 3b: Interpolating SC and SH from grid\", Sys.time()))\n  interpolators &lt;- create_1d_interpolators(df_to_g_theo[['Scenario1']], c(\"mu_H_t\", \"mu_C_t\"))\n  interpolated_muH_muC_t &lt;- interpolate_to_simulated(data, interpolators)\n  \n  # Step 4: Compute theoretical PDX\n  message(sprintf(\"[%s] Step 4: Computing theoretical PDX\", Sys.time()))\n  pdx_t_results &lt;- prob_D(\n    X1 = data$X1,\n    X2 = data$X2,\n    D = data$D,\n    params = parms[['Scenario1']]\n  )\n  \n  # Combine results\n  message(sprintf(\"[%s] Step 5: Combining results\", Sys.time()))\n  result &lt;- data.frame(\n    data,\n    theo_premiums_except_muH_muC,\n    interpolated_muH_muC_t,\n    pdx_t = pdx_t_results\n  )\n    \n  })\n  \n  return(samples_to_ret)\n})\n\n# Save the entire df_to_g object to JSON\n  message(sprintf(\"[%s] Saving sims_to_g_theo to %s\", Sys.time(), sims_to_g_theo_file))\n  toJSON(sims_to_g_theo, pretty = TRUE, auto_unbox = TRUE) %&gt;% write(sims_to_g_theo_file)\n}\n\n\n\n\nComputation of theoretical local metrics across the simulated samples (for later use)\npop_stats_path_theo &lt;- 'preds/preds_pop_stats_theo.json'\n\n# Check and load or compute preds_pop_stats\nif (file.exists(pop_stats_path_theo)) {\n  preds_pop_stats_theo &lt;- fromJSON(pop_stats_path_theo)\n} else {\n  preds_pop_stats_theo &lt;- setNames(nm = names(sims)) %&gt;% \n    lapply(function(name) {\n      setNames(nm = names(pop_to_g_theo[[name]])) %&gt;%  \n        lapply(function(set) {\n          local_df &lt;- pop_to_g_theo[[name]][[set]]\n          \n          data.frame(local_df, \n                 'risk_spread_t' = quants_theo[[name]][['risk_spread']](x1 = local_df$X1,\n                                         x2 = local_df$X2,\n                                         d = local_df$D),\n                 'proxy_vuln_t' = quants_theo[[name]][['proxy_vuln']](x1 = local_df$X1,\n                                         x2 = local_df$X2,\n                                         d = local_df$D),\n                 \n                 ## computationally quicker (only) for the theoretical case because we want to avoid numerical integration when calling mu_C. \n                 'fair_range_t' = (apply(local_df[c('mu_B_t', 'mu_U_t', 'mu_A_t', 'mu_H_t', 'mu_C_t')], 1, max) -\n                                     apply(local_df[c('mu_B_t', 'mu_U_t', 'mu_A_t', 'mu_H_t', 'mu_C_t')], 1, min)) |&gt;  unname(),\n                 'parity_cost_t' = local_df[['mu_C_t']] - local_df[['mu_B_t']])\n        })\n    })\n  toJSON(preds_pop_stats_theo, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(pop_stats_path_theo)\n}\n\n\n\n\nComputation of theoretical local metrics across the experiment samples (for later use in section 5)\nsims_stats_path_theo &lt;- 'preds/preds_sims_stats_theo.json'\n\n# Check and load or compute preds_pop_stats\nif (file.exists(sims_stats_path_theo)) {\n  preds_pop_stats_theo &lt;- fromJSON(sims_stats_path_theo)\n} else {\n  preds_sims_stats_theo &lt;- setNames(nm = names(sims_to_g_theo)) %&gt;% \n    lapply(function(idx) {\n      setNames(nm = names(sims_to_g_theo[[idx]])) %&gt;%  \n        lapply(function(set) {\n          local_df &lt;- sims_to_g_theo[[idx]][[set]]\n          \n          data.frame(local_df, \n                 'risk_spread_t' = quants_theo[['Scenario1']][['risk_spread']](x1 = local_df$X1,\n                                         x2 = local_df$X2,\n                                         d = local_df$D),\n                 'proxy_vuln_t' = quants_theo[['Scenario1']][['proxy_vuln']](x1 = local_df$X1,\n                                         x2 = local_df$X2,\n                                         d = local_df$D),\n                 \n                 ## computationally quicker (only) for the theoretical case because we want to avoid numerical integration when calling mu_C. \n                 'fair_range_t' = (apply(local_df[c('mu_B_t', 'mu_U_t', 'mu_A_t', 'mu_H_t', 'mu_C_t')], 1, max) -\n                                     apply(local_df[c('mu_B_t', 'mu_U_t', 'mu_A_t', 'mu_H_t', 'mu_C_t')], 1, min)) |&gt;  unname(),\n                 'parity_cost_t' = local_df[['mu_C_t']] - local_df[['mu_B_t']])\n        })\n    })\n  toJSON(preds_sims_stats_theo, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(sims_stats_path_theo)\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Example setup and simulations</span>"
    ]
  },
  {
    "objectID": "3_local.html",
    "href": "3_local.html",
    "title": "3  Actuarial local metrics",
    "section": "",
    "text": "3.1 Pre pricing local metrics\nPursuing our simple example with three scenarios, we can dissect proxy vulnerability using its two key components: the risk spread and propensity. With constant risk spread (scenario 1, top on Figure 3.1), the shape of proxy vulnerability depends solely on the propensity function. Scenario 1 yields eight distinct values of proxy vulnerability, as the constant risk spread limits variation to the eight possible values of propensity.\nWith a variable risk spread (scenario 2, middle of Figure 3.1), proxy vulnerability takes continuous values, even if the propensity has a finite number of values. As \\(X_1\\) increases, the proxy vulnerability moves further away from zero. Its sign depends on whether \\(P(D = 1 \\mid \\mathbf{X} = \\mathbf{x})\\) exceeds its unconditional counterpart (which happens for \\(x_1 &gt; 1\\)).\nIn scenario 3, the propensity reveals that extreme values of \\(X_1\\) – both high and low – can indicate membership in the sensitive group \\(D = 1\\). However, this alone is insufficient to identify vulnerable subgroups. The proxy vulnerability bottom panel in Figure 3.1 shows that large proxy vulnerability arises only for large \\(X_1\\) values. For low \\(X_1\\) values, while the model is capable of identifying protected subpopulations (\\(D\\)), the risk spread is too narrow to yield material proxy effects. Finally, large \\(X_1\\) values paired with \\(X_2 = 1\\) (solid yellow) gives a propensity of \\(D=1\\) around 1/2, and thus no capacity to exploit the risk spread. This highlights the joint roles of risk spread and propensity in understanding proxy vulnerability.\nR code producing the theoretical proxy dissection graph.\n(setNames(nm = names(preds_grid_stats)) %&gt;% lapply(function(name){\n  ## the colors\n   ## the colors\n    cols &lt;- the_CAS_colors\n    pop_id &lt;- which(names(preds_grid_stats) == name)\n    \n    local_to_g &lt;- preds_grid_stats[[name]] %&gt;% \n  filter(x1 &lt;= 8, x1 &gt;= -5, d == 1) \n    \n    \n  gg_risk_spread &lt;- local_to_g %&gt;% \n  ggplot(aes(x = x1, y = risk_spread_t,\n             color = factor(x2),\n             group = factor(x2),\n             lty = factor(x2),\n             linewidth = factor(x2),\n             alpha = factor(x2))) + \n  geom_line() +\n  theme_minimal() + \n  labs(x = '',\n       y = latex2exp::TeX(\"$\\\\Delta_{risk}(x_1, x_2)$\")) + \n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linetype_manual(values = c('solid', '31', '21', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +    \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  scale_y_continuous(labels = scales::dollar, breaks = c(10, 20), limits = c(5, 28)) + \n  scale_x_continuous(breaks = c(-3:3)*3 + 1, labels = NULL) + \n    theme(axis.title.y = element_text(size = 8))\n  \n  ## lets graph\n    gg_pdx &lt;- local_to_g %&gt;% \n  ggplot(aes(x = x1, y = pdx_t,\n             lty = factor(x2),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(x2),\n             color = factor(x2))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '31', '21', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"),\n       y = latex2exp::TeX(\"$P(D = 1|x_1, x_2)$\")) + \n  scale_x_continuous(breaks = c(-3:3)*3 + 1)  + # see above\n  theme_minimal() + \n  scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(0, 1))  +\n  theme(axis.title.y = element_text(size = 8))\n  \n  gg_proxy_vuln &lt;- local_to_g %&gt;% \n  ggplot(aes(x = x1, y = proxy_vuln_t,\n             color = factor(x2),\n             group = factor(x2),\n             lty = factor(x2),\n             linewidth = factor(x2),\n             alpha = factor(x2))) + \n  geom_line() +\n  theme_classic() + \n  labs(x = latex2exp::TeX('$x_1$'),\n       y = latex2exp::TeX(\"$\\\\Delta_{proxy}(x_1, x_2)$\"),\n       title = paste0('Scenario ', pop_id)) + \n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linetype_manual(values = c('solid', '41', '32', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) + \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  scale_y_continuous(labels = scales::dollar, breaks = c(-5, 0, 5, 10), limits = c(-6, 15)) + \n  geom_abline(slope = 0, intercept = 0, lty = '32', color= 'black', size= 0.7, alpha = 0.3)+\n  scale_x_continuous(breaks = c(-3:3)*3 + 1) # see above\n  \n  gg_left &lt;- ggpubr::ggarrange(plotlist = list(gg_risk_spread + theme(axis.title.y = element_text(size = 10), legend.position = ''),\n                                               gg_pdx + theme(axis.title.y = element_text(size = 10), legend.position = '')),\n                    nrow = 2)\n  \nggpubr::ggarrange(plotlist = list(gg_left, gg_proxy_vuln),\n                    ncol = 2, widths = c(2, 3))\n}) %&gt;% \n  ggpubr::ggarrange(plotlist = .,\n            nrow = 3, \n            common.legend = T,\n            legend = 'right')) %&gt;% \nggsave(filename = \"figs/graph_proxy_dissect_t_perpop.png\",\n       plot = .,\n       height = 8.25,\n       width = 7.00,\n       units = \"in\",\n       device = \"png\", dpi = 500)\nFigure 3.2 depicts the estimated version of Figure 3.1. Ignoring estimation variability, the findings remains. While the simplicity of the example setup makes it intuitive to visualize for which values of \\((x_1, x_2)\\) proxy vulnerability is the highest, the next Chapter 5 will discuss how partitioning may help to uncover subpopulations with the highest proxy vulnerability.\nR code producing the estimated proxy dissection graph.\n(setNames(nm = names(preds_grid_stats)) %&gt;% lapply(function(name){\n  ## the colors\n   ## the colors\n    cols &lt;- the_CAS_colors\n    pop_id &lt;- which(names(preds_grid_stats) == name)\n    \n    local_to_g &lt;- preds_grid_stats[[name]] %&gt;% \n  filter(x1 &lt;= 8, x1 &gt;= -5, d == 1) \n    \n    \n  gg_risk_spread &lt;- local_to_g %&gt;% \n  ggplot(aes(x = x1, y = risk_spread,\n             color = factor(x2),\n             group = factor(x2),\n             lty = factor(x2),\n             linewidth = factor(x2),\n             alpha = factor(x2))) + \n  geom_line() +\n  theme_minimal() + \n  labs(x = '',\n       y = latex2exp::TeX(\"$\\\\widehat{\\\\Delta}_{risk}(x_1, x_2)$\")) + \n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linetype_manual(values = c('solid', '31', '21', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +    \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  scale_y_continuous(labels = scales::dollar, breaks = c(10, 20), limits = c(5, 28)) + \n  scale_x_continuous(breaks = c(-3:3)*3 + 1, labels = NULL) + \n    theme(axis.title.y = element_text(size = 8))\n  \n  ## lets graph\n    gg_pdx &lt;- local_to_g %&gt;% \n  ggplot(aes(x = x1, y = pdx,\n             lty = factor(x2),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(x2),\n             color = factor(x2))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '31', '21', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"),\n       y = latex2exp::TeX(\"$\\\\widehat{P}(D = 1|x_1, x_2)$\")) + \n  scale_x_continuous(breaks = c(-3:3)*3 + 1)  + # see above\n  theme_minimal() + \n  scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(0, 1))  +\n  theme(axis.title.y = element_text(size = 8))\n  \n  gg_proxy_vuln &lt;- local_to_g %&gt;% \n  ggplot(aes(x = x1, y = proxy_vuln,\n             color = factor(x2),\n             group = factor(x2),\n             lty = factor(x2),\n             linewidth = factor(x2),\n             alpha = factor(x2))) + \n  geom_line() +\n  theme_classic() + \n  labs(x = latex2exp::TeX('$x_1$'),\n       y = latex2exp::TeX(\"$\\\\widehat{\\\\Delta}_{proxy}(x_1, x_2)$\"),\n       title = paste0('Scenario ', pop_id)) + \n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linetype_manual(values = c('solid', '41', '32', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) + \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  scale_y_continuous(labels = scales::dollar, breaks = c(-5, 0, 5, 10), limits = c(-6, 15)) + \n  geom_abline(slope = 0, intercept = 0, lty = '32', color= 'black', size= 0.7, alpha = 0.3)+\n  scale_x_continuous(breaks = c(-3:3)*3 + 1) # see above\n  \n  gg_left &lt;- ggpubr::ggarrange(plotlist = list(gg_risk_spread + theme(axis.title.y = element_text(size = 10), legend.position = ''),\n                                               gg_pdx + theme(axis.title.y = element_text(size = 10), legend.position = '')),\n                    nrow = 2)\n  \nggpubr::ggarrange(plotlist = list(gg_left, gg_proxy_vuln),\n                    ncol = 2, widths = c(2, 3))\n}) %&gt;% \n  ggpubr::ggarrange(plotlist = .,\n            nrow = 3, \n            common.legend = T,\n            legend = 'right')) %&gt;% \nggsave(filename = \"figs/graph_proxy_dissect_perpop.png\",\n       plot = .,\n       height = 8.25,\n       width = 7.00,\n       units = \"in\",\n       device = \"png\", dpi = 500)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Actuarial local metrics</span>"
    ]
  },
  {
    "objectID": "3_local.html#pre-pricing-local-metrics",
    "href": "3_local.html#pre-pricing-local-metrics",
    "title": "3  Actuarial local metrics",
    "section": "",
    "text": "Related section\nThe pre-pricing local metrics are described in Section 5.1 of the main paper.\n\n\n\n\n\n\n\n\nFigure 3.1: Risk spread (top left) and propensity (bottom left) are the components of the theoretical proxy vulnerability (right panel) for the three scenarios (vertical blocks) of the example.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Risk spread (top left) and propensity (bottom left) are the components of the estimated proxy vulnerability (right panel) for the three scenarios (vertical blocks) of the exampe.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Actuarial local metrics</span>"
    ]
  },
  {
    "objectID": "3_local.html#post-pricing-local-metrics",
    "href": "3_local.html#post-pricing-local-metrics",
    "title": "3  Actuarial local metrics",
    "section": "3.2 Post pricing local metrics",
    "text": "3.2 Post pricing local metrics\n\n\n\nRelated section\nThe post-pricing local metrics are described in Section 5.2 of the main paper.\n\nGiven a commercial price, one may leverage the spectrum of fairness to better grasp the farness implication of the commercial price. We start by constructing a fictive commercial price to illustrate.\n\n3.2.1 Construction a ``given’’ price for fairness evaluation\nTo illustrate how fairness considerations interact with real-world ratemaking, we replicated realistic practical decisions. We assume no direct discrimination on \\(D\\). We cap premiums for \\(X_1 &gt; 6\\) and group levels \\(X_2=1\\) and \\(X_2=3\\) due to low exposure for the former. We then train a lightgbm model to predict \\(Y\\), forming the technical premiums. The commercial adjustments are targeted discounts of 15% when \\(X_1 &lt; 0\\) and 10% when \\(X_2 = 2\\), reflecting pricing incentives for perceived lower-risk groups. Finally, the commercial price is globally rebalanced at the level of the best-estimate price. We end up with a pricing function \\(\\pi(x_1, x_2)\\) for which we want to assess fairness with respect to \\(D\\).\n\n\nTraining the fictive pricing function\nsource('___lgb_given_tariff.R')\n\n## clean the pred repo\nunlink(file.path('preds', \"*_best_estimate.json\"))\ngiven_lgb &lt;- setNames(nm = names(preds_grid_stats)) %&gt;% lapply(function(name){\n  list_df &lt;- list('train' = sims[[name]],\n                  'valid' = valid[[name]],\n                  'test' = test[[name]])\n  the_given_tarif_lightgbm_fun(list_data = list_df, \n                                 name = name)\n})\n\n\nGiven tariff for scenario:  Scenario1  \nBest valid mse: 98.3712  \noptimal ntree: 634  \nTraining time: 22.3974  sec. \nGiven tariff for scenario:  Scenario2  \nBest valid mse: 122.3908  \noptimal ntree: 384  \nTraining time: 17.05309  sec. \nGiven tariff for scenario:  Scenario3  \nBest valid mse: 126.9877  \noptimal ntree: 395  \nTraining time: 18.53165  sec. \n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause the price \\(\\pi(x_1, x_2)\\) does not discriminate directly on \\(D\\), it does not make a lot of sense to compute the excess lift local metric.\n\n\n\n\nComputing post pricing metrics on the populations\ncompute_mub0_mub1 &lt;- function(data, mua_col, mub_col, d_col, pd) {\n  # Validate input\n  if (!is.data.frame(data)) stop(\"Input `data` must be a data frame.\")\n  if (!(mua_col %in% colnames(data))) stop(\"mu_A column not found in the dataset.\")\n  if (!(mub_col %in% colnames(data))) stop(\"mu_B column not found in the dataset.\")\n  if (!(d_col %in% colnames(data))) stop(\"D column not found in the dataset.\")\n  if (length(pd) != 2 || any(pd &lt;= 0) || sum(pd) != 1) stop(\"PD must be a valid probability vector of length 2 summing to 1.\")\n  \n  # Extract the columns\n  mu_A &lt;- data[[mua_col]]\n  mu_B &lt;- data[[mub_col]]\n  D &lt;- data[[d_col]]\n  \n  # Compute SB0 and SB1\n  muB0 &lt;- ifelse(D == 1, \n                (mu_A - pd[2] * mu_B) / pd[1],  # Formula for SB0 when D = 1\n                mu_B)                          # SB0 = SB when D = 0\n  muB1 &lt;- ifelse(D == 0, \n                (mu_A - pd[1] * mu_B) / pd[2],  # Formula for SB1 when D = 0\n                mu_B)                          # SB1 = SB when D = 1\n  \n  # Return the modified dataset with SB0 and SB1\n  return(\n    list(muB0, muB1)\n  )\n}\n\npregroup_pop_stats &lt;- setNames(nm = names(preds_pop_stats)) %&gt;% lapply(function(name){\n    setNames(nm = names(preds_pop_stats[[name]])) %&gt;% lapply(function(the_set){\n      the_data &lt;- preds_pop_stats[[name]][[the_set]]\n      the_data$prem &lt;- NULL\n      the_data$eb &lt;- NULL; the_data$eu &lt;- NULL; the_data$ea &lt;- NULL; the_data$eh &lt;- NULL; the_data$ec &lt;- NULL;\n      the_data$rb &lt;- NULL; the_data$ru &lt;- NULL; the_data$ra &lt;- NULL; the_data$rh &lt;- NULL; the_data$rc &lt;- NULL; the_data$r &lt;- NULL\n      \n      mu_b1b0 &lt;- compute_mub0_mub1(the_data, 'mu_A', 'mu_B', 'D', c(0.5, 0.5))\n      \n      data.frame(the_data, \n                 'prem' = given_lgb[[name]]$pred_fun(the_data)) %&gt;% \n                mutate('eb' = prem - mu_B, \n                        'eu' = prem - mu_U,\n                        'ea' = prem - mu_A,\n                        'eh' = prem - mu_H,\n                        'ec' = prem - mu_C,\n                        'rb' = eb &gt; 0,\n                        'ru' = eu &gt; 0,\n                        'ra' = ea &gt; 0,\n                        'rh' = eh &gt; 0,\n                        'rc' = ec &gt; 0,\n                        'r' = rb + ru + ra + rh + rc,\n           'comm_load' = ea,\n           'comm_burden' = ea/mu_A, \n           'mu_B1' = mu_b1b0[[2]], \n           'mu_B0' = mu_b1b0[[1]],\n           'implied_prop' = (prem - mu_B0)/(mu_B1 - mu_B0))\n    })  \n})\n\ntoJSON(pregroup_pop_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% write('preds/pregroup_pop_stats.json')\n\n\n\n\nComputing post pricing metrics on the grid\npregroup_grid_stats &lt;- setNames(nm = names(preds_grid_stats)) %&gt;% lapply(function(name){\n    the_data &lt;- preds_grid_stats[[name]] \n    \n        the_data$prem &lt;- NULL\n    the_data$eb &lt;- NULL; the_data$eu &lt;- NULL; the_data$ea &lt;- NULL; the_data$eh &lt;- NULL; the_data$ec &lt;- NULL;\n    the_data$rb &lt;- NULL;the_data$ru &lt;- NULL; the_data$ra &lt;- NULL; the_data$rh &lt;- NULL; the_data$rc &lt;- NULL; the_data$r &lt;- NULL\n    \n    mu_b1b0 &lt;- compute_mub0_mub1(the_data, 'mu_A', 'mu_B', 'd', c(0.5, 0.5))\n    \n    the_data$prem &lt;- NULL\n  data.frame(the_data, \n            'prem' = given_lgb[[name]]$pred_fun(the_data %&gt;% \n      mutate(X1 = x1, X2 = x2))) %&gt;% \n    mutate('eb' = prem - mu_B, \n            'eu' = prem - mu_U,\n            'ea' = prem - mu_A,\n            'eh' = prem - mu_H,\n            'ec' = prem - mu_C,\n            'rb' = eb &gt; 0,\n            'ru' = eu &gt; 0,\n            'ra' = ea &gt; 0,\n            'rh' = eh &gt; 0,\n            'rc' = ec &gt; 0,\n            'r' = rb + ru + ra + rh + rc,\n           'comm_load' = ea, \n           'comm_burden' = ea/mu_A, \n           'muB1' = mu_b1b0[[2]], \n           'muB0' = mu_b1b0[[1]],\n           'implied_prop' = (prem - muB0)/(muB1 - muB0))\n})\n\ntoJSON(pregroup_grid_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% write('preds/pregroup_grid_stats.json')\n\n\n\n\n3.2.2 Commercial burden\nIn Figure 3.3, we plot for scenario 3 the pricing function \\(\\pi\\) (solid line) in terms of \\(x_1\\) and \\(x_2\\) (panel), along with the corresponding aware premium (dashed line). The gap between the two is the commercial burden, which we highlight with the color scale. As expected for scenario 3, the individuals with \\(x_2 = 4\\) and high values of \\(x_1\\) bear the highest commercial burden despite the plateau for \\(x_2 &gt; 6\\). The discount introduced for \\(x_1 &lt; 0\\) does lower premiums, but implies a loading for \\(x_1&gt;0\\) (when balancing the rates), which further adds commercial burden for individuals on the right of the last panel of Figure 3.3, regardless of the insurer’s intent.\n\n\nR code producing the commercial burden illustration.\n## Parse latex in facet \nappender &lt;- function(string) {\n  if (length(string) &gt; 1) {\n    return(sapply(string, latex2exp::TeX))\n  } else {\n    return(latex2exp::TeX(string))\n  }\n}\n\n\n# Generate 50 discrete percentage levels from 0 to 0.75\nnum_levels &lt;- 25\nmax_val &lt;- 0.1\nmin_val &lt;- -1 * max_val\npct_levels &lt;- seq(0, max_val, length.out = num_levels)\n\n# Create positive and negative threshold mappings\npos_thresholds &lt;- setNames(pct_levels, paste0(\"cload_\", seq_len(num_levels)))\nneg_thresholds &lt;- setNames(-pct_levels, paste0(\"cload_\", seq_len(num_levels), \"_down\"))\n\n# Combine both sets of thresholds\nall_thresholds &lt;- c(pos_thresholds, neg_thresholds)\n\n# Define color mapping for each threshold\ncolor_palette &lt;- colorRampPalette(\n  c(\"#91CF60\", \"#FFFFBF\", \"#FC8D59\", 'firebrick4'),\n  bias = 1.5\n  )(num_levels) \nfill_levels &lt;- names(all_thresholds)\n\nlibrary(cowplot)\n\nto_save_giventariff_perpop &lt;- names(pregroup_grid_stats) %&gt;% \n  lapply(function(name){\n    pop_id &lt;- which(names(pregroup_grid_stats) == name)\nthe_df &lt;- pregroup_grid_stats[[name]] %&gt;%\n  filter(d == 1) %&gt;%\n  mutate(x2 = factor(x2,\n                     levels = 1:4,\n                     labels = paste0('$\\\\x_2 = $', 1:4)))\n\nthe_df$prem[the_df$pdx &lt; 0.03] &lt;- NA\n\nthe_df$factor_cload &lt;- factor(ifelse(the_df$comm_load &lt;0,\n                                                             '1', '2')\n                                                      )\n\n# Apply generalized transformation using a for loop\nfor (col_name in names(all_thresholds)) {\n  col_value &lt;- all_thresholds[[col_name]]\n  \n   # Check condition for each row\n  if(grepl(\"_down$\", col_name)){\n    condition &lt;-  the_df$comm_burden &lt; col_value\n  } else {\n    condition &lt;-  the_df$comm_burden &gt; col_value\n  } \n  \n  # Compute the new column values based on the condition\n  the_df[[col_name]] &lt;- ifelse(condition, the_df$comm_load, NA_real_)\n}\n\n\n\n# Create the base plot\nthe_plot &lt;- the_df %&gt;%\n  ggplot(aes(x = x1, y = prem, group = factor(x2))) +\n  \n  scale_y_continuous(labels= scales::dollar, limits = c(85, 155)) +\n  facet_grid(~factor(x2), \n             labeller = as_labeller(appender, \n                                    default = label_parsed,\n                                    multi_line = TRUE)) +\n  theme_classic() +\n  labs(y = 'Premium', x = latex2exp::TeX('$x_1$'),\n       title = latex2exp::TeX(paste0('Scenario ', pop_id))) +\n  scale_x_continuous(breaks = c(-3, 0, 3, 6), limits = c(-4, 7))\n\n# Generate and add geom_ribbon layers dynamically inside ggplot\nfor (col_name in c(head(fill_levels, num_levels), tail(fill_levels, num_levels))) {\n  \n  temp_data &lt;- the_df \n    temp_data$fill_factor &lt;- factor(col_name, levels = fill_levels)  # Ensure fill uses a factor\n  \n  if(grepl(\"_down\", col_name)) {\n    temp_data$y_min &lt;- the_df[[col_name]] + the_df$mu_A\n    temp_data$y_max &lt;- (1 + all_thresholds[col_name])  * the_df$mu_A\n  } else {\n    temp_data$y_min &lt;- (1 + all_thresholds[col_name]) * the_df$mu_A\n    temp_data$y_max &lt;- the_df[[col_name]] + the_df$mu_A\n  }\n  \n  the_plot &lt;- the_plot +\n    geom_ribbon(\n      data = temp_data,\n      aes(\n        x = x1, \n        ymax = y_max,\n        ymin = y_min,\n        group = x2,\n        fill =fill_factor\n      ),\n      inherit.aes = FALSE,\n      alpha = 1\n    )\n  rm(temp_data)\n}\n\n# Apply color mapping\nthe_plot &lt;- the_plot +\n  scale_fill_manual(\n    values = setNames(c(color_palette, color_palette), levels(fill_levels)),\n    # breaks = ,\n    # labels = c(),\n    guide = 'none'\n    #,labels = scales::label_number(accuracy = 0.01)\n  ) +\n  geom_line(aes(y = mu_A, linetype = \"mu_A\", color = \"mu_A\", linewidth = \"mu_A\"), alpha = 0.8) +\n  geom_line(aes(linetype = \"prem\", color = \"prem\", linewidth = \"prem\"), alpha = 0.8) +\nscale_linetype_manual(\n  values = c(\"mu_A\" = \"21\", \"prem\" = \"solid\"),\n  labels = c(\"mu_A\" = latex2exp::TeX('$\\\\widehat{\\\\mu}^A(x_1)$'),\n             \"prem\" = latex2exp::TeX('$\\\\pi(x_1, x_2)$')),\n  name = latex2exp::TeX(\"Premium\")) + \n   scale_color_manual(\n  values = c(\"mu_A\" = \"grey50\", \"prem\" = \"black\"),\n  labels = c(\"mu_A\" = latex2exp::TeX('$\\\\widehat{\\\\mu}^A(x_1)$'),\n             \"prem\" = latex2exp::TeX('$\\\\pi(x_1, x_2)$')),\n  name = latex2exp::TeX(\"Premium\")) + \n  scale_alpha_manual(\n  values = c(\"mu_A\" = 1, \"prem\" = 0.45),\n  labels = c(\"mu_A\" = latex2exp::TeX('$\\\\widehat{\\\\mu}^A(x_1)$'),\n             \"prem\" = latex2exp::TeX('$\\\\pi(x_1, x_2)$')),\n  name = latex2exp::TeX(\"Premium\")) + \n  scale_linewidth_manual(\n  values = c(\"mu_A\" = 1, \"prem\" = 1.2),\n  labels = c(\"mu_A\" = latex2exp::TeX('$\\\\widehat{\\\\mu}^A(x_1)$'),\n             \"prem\" = latex2exp::TeX('$\\\\pi(x_1, x_2)$')),\n  name = latex2exp::TeX(\"Premium\"))+\n  guides(fill = guide_colorbar(barwidth = 10, barheight = 0.5)) + \n  theme(legend.position = \"right\") \n\n# Define the fake gradient legend (purely visual)\n\nlegend_df &lt;- data.frame(y = seq(min_val, max_val, length.out = 100), x = 1)  \nfake_legend_plot &lt;- ggplot(legend_df, aes(x = x, y = y, fill = y)) +\n  geom_tile() +\n  scale_fill_gradientn(\n    colors = c( rev(color_palette),  color_palette),  \n    limits = c(-max_val, max_val),\n    name = \"Commercial burden\",\n    breaks = c(-max_val, 0, max_val),  \n    labels = c(paste0(scales::percent(-max_val)), \n               paste0(round(0, 3)), \n               paste0(scales::percent(max_val)))\n  ) +\n  theme_void() \n\n\ntrue_legend &lt;- cowplot::get_legend(the_plot)\nfake_legend &lt;- cowplot::get_legend(fake_legend_plot)\n\ncombined_legend &lt;- ggpubr::ggarrange(fake_legend, true_legend, ncol = 1, nrow = 2)\n\nhide_legend &lt;- !(pop_id == 2)\n\n# Function to create a white space placeholder\nwhite_space &lt;- ggplot() + \n  theme_void() + \n  theme(plot.background = element_rect(fill = \"white\", color = \"white\"))  # Ensures white background\n\nlegend_to_use &lt;- if (hide_legend) white_space else combined_legend\n\nfinal_plot &lt;- (the_plot + theme(legend.position = '')) %&gt;% \n  ggpubr::ggarrange(.,\n                    legend_to_use,\n                    widths = c(4, 0.75))\n\nggsave(filename = paste0(\"figs/graph_giventariff_commload_\", name, \".png\"),\n       plot = (the_plot + theme(legend.position = '')) %&gt;% \n  ggpubr::ggarrange(.,\n                    combined_legend,\n                    widths = c(4, 1)),\n       height = 3.75,\n       width = 9.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n       \nreturn(final_plot)\n  }) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           nrow = 3,\n                           widths = 15, heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nggsave(filename = \"figs/graph_giventariff_commload.png\",\n       plot = to_save_giventariff_perpop,\n       height = 10.75,\n       width = 9.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 3.3: Commercial price (solid line) and aware premium (dashed line) of the three scenarios (rows) in terms of \\(x_1\\) and \\(x_2\\) (panel), with the commercial burden colored.\n\n\n\n\n\n\n\n\n3.2.3 Implied propensity\nIn the top row of Figure 3.4, we depict the pricing function \\(\\pi(x_1, x_2)\\) (colored lines), the best-estimate premium \\(\\mu^B(x_1, d)\\) for \\(d=0\\) (large solid gray) and \\(d=1\\) (thin solid gray), and the aware premium \\(\\mu^A(x_1)\\) (dashed line). The colors correspond to grouped values of the implied propensity (section 5.2.2 of the main article), which is illustrated in the bottom row of Figure 3.4. We see in blue and red that the implied propensity is not bounded by \\(0\\) and \\(1\\), and highlights segments warranting attention.\n\n\nR code producing the implied propensity illustration.\nto_save_giventariff_perpop &lt;- names(pregroup_grid_stats) %&gt;% \n  lapply(function(name){\n    pop_id &lt;- which(names(pregroup_grid_stats) == name)\n    pregroup_grid_stats[[name]]$prem[pregroup_grid_stats[[name]]$pdx &lt; 0.03] &lt;- NA\n    pregroup_grid_stats[[name]]$factor_imp_prop &lt;- factor(ifelse(pregroup_grid_stats[[name]]$implied_prop &lt;0,\n                                                             '1',\n                                                             ifelse(pregroup_grid_stats[[name]]$implied_prop &lt;0.5,\n                                                             '2',\n                                                             ifelse(pregroup_grid_stats[[name]]$implied_prop &lt; 1,\n                                                             '3',\n                                                             '4')\n                                                             ))\n                                                      )\n## Top plot\nthe_plot &lt;- pregroup_grid_stats[[name]]  %&gt;% \n  mutate(x2 = factor(x2,\n                             levels = 1:4,\n                             labels = paste0('$\\\\x_2 = $', 1:4))) %&gt;% \n          ggplot(aes(x= x1,\n                     y = prem,\n             lwd = factor(d),\n             alpha= factor(d),\n             color = factor_imp_prop,\n             group = factor(d))) +\n  # geom_line() + \n  facet_grid(~factor(x2), \n             labeller = as_labeller(appender, \n                            default = label_parsed,\n                            multi_line = TRUE)) + \n  theme_classic() + \n  scale_color_brewer(palette = 'Spectral', guide = NULL) + \n  scale_alpha_manual(values = c('1' = 0.95,\n                                '0' = 0.45), name = latex2exp::TeX('$d$')) + \n  scale_linewidth_manual(values = c('1' = 1.25,\n                                    '0' = 2.5), name = latex2exp::TeX('$d$')) + \n  labs(y = 'Premium', x = '',\n       title =  latex2exp::TeX(paste0('Scenario ', pop_id))) +\n  scale_x_continuous(labels = NULL, breaks = NULL, limits = c(-4, 7))  \n\nfor (family in c('mu_B', 'mu_A')) { #c('SB', 'SU', 'SA', 'SH', 'SC')) {\n  for (sens in c(0, 1)) {\n    \n    ## filter\n    filtered_data &lt;- pregroup_grid_stats[[name]] %&gt;% \n      filter(d == sens) %&gt;% \n      mutate(family = family,\n             x2 = factor(x2,\n                         levels = 1:4,\n                         labels = paste0('$\\\\x_2 = $', 1:4)))\n    \n    filtered_data$pred &lt;- filtered_data[[family]]\n    filtered_data$pred[filtered_data$pdx &lt; 0.1] &lt;- NA\n    \n    ## aes specific to 'd'\n    the_lwd &lt;- ifelse(sens == 1, 0.65, 1.4)\n    the_alpha &lt;- ifelse(sens == 1, 0.85, 0.75)\n    the_color &lt;- ifelse(sens == 1, 'grey60', 'grey80')\n    ## add the plot\n    the_plot &lt;- the_plot +\n      geom_line(data = filtered_data,\n      mapping = aes(x = x1, y = pred,\n                    group = factor(1),\n                    linetype = family),\n      inherit.aes = FALSE,\n      lwd = the_lwd,\n      alpha = the_alpha,\n      color = the_color) \n  }\n}\n\nthe_lty_values &lt;- c('mu_B' = \"solid\",\n                    'mu_A' = \"32\")\n\n\n# Add legend layers manually\nthe_plot &lt;- the_plot +\n  geom_line(lineend = \"round\", linejoin = \"round\") +\n  scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = scales::dollar,\n                     limits = c(90, 140))+\n  scale_linetype_manual(\n    values = the_lty_values,\n    labels = c(latex2exp::TeX('$\\\\widehat{\\\\mu}^B$'),\n               latex2exp::TeX('$\\\\widehat{\\\\mu}^A$')),\n    name = latex2exp::TeX(\"$Premium \\\\ \\\\ \\\\ \\\\ \\\\ $\")\n  ) + \n  guides(\n    linetype = guide_legend(\n      order = 1,\n      override.aes = list(\n        color = \"grey70\",  # Set grey color for linetype legend\n        alpha = 1,         # Enforce alpha = 1\n        lwd = 0.7 # Enforce alpha = 1 for linetype legend\n      )\n    ),\n    linewidth = guide_legend(\n      order = 1,\n      override.aes = list(color = 'grey70')),\n    alpha = guide_legend(\n      order = 1,\n      override.aes = list(color = 'grey70'))\n    ) + \n  \n  theme(plot.margin = unit(c(10, 5.5, 0, 5.5), \"pt\"))\n\n\n## Bottom plot\nthe_plot2 &lt;- pregroup_grid_stats[[name]]  %&gt;% \n  mutate(x2 = factor(x2,\n                             levels = 1:4,\n                             labels = paste0('$\\\\x_2 = $', 1:4))) %&gt;% \n          ggplot(aes(x= x1,\n                     y = implied_prop,\n             lwd = factor(d),\n             alpha= factor(d),\n             color = factor_imp_prop,\n             group = factor(d))) +\n  facet_grid(~factor(x2), \n             labeller = as_labeller(appender, \n                            default = label_parsed,\n                            multi_line = TRUE)) + \n  theme_classic() + \n  scale_color_brewer(palette = 'Spectral', labels = c('1' = '&lt; 0',\n                                                      '2' = latex2exp::TeX('in $[0, P(D = 1)]$'),\n                                                      '3' = latex2exp::TeX('in $[P(D = 1), 1]$'),\n                                                      '4' = '&gt; 1'),\n                     name = 'Imp. propensity') + \n  scale_alpha_manual(values = c('1' = 0.95,\n                                '0' = 0.45), guide = NULL) + \n  scale_linewidth_manual(values = c('1' = 1.25,\n                                    '0' = 2.5), guide = NULL) + \n  labs(y = 'Propensity', x = latex2exp::TeX('$x_1$')) +\n  scale_x_continuous( breaks = c(-3, 0, 3, 6), limits = c(-4, 7))  +\nannotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0,\n           fill = RColorBrewer::brewer.pal(4, 'Spectral')[1], alpha = 0.1) +\n  \n    annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 0, ymax = 0.5,\n           fill = RColorBrewer::brewer.pal(4, 'Spectral')[2], alpha = 0.1) +\n  \n    annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 0.5, ymax = 1,\n           fill = RColorBrewer::brewer.pal(4, 'Spectral')[3], alpha = 0.1) +\n  \n    annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 1, ymax = Inf,\n           fill = RColorBrewer::brewer.pal(4, 'Spectral')[4], alpha = 0.1) +\n   geom_hline(yintercept = c(0, 1),\n             linetype = \"dotted\",\n             color = \"black\",\n             linewidth = 0.5,\n             inherit.aes = FALSE)  \n  \nfor (family in c('mu_A', 'mu_B')) {\n    ## filter\n    filtered_data &lt;- pregroup_grid_stats[[name]] %&gt;% \n      filter(d == 1) %&gt;% \n      mutate(x2 = factor(x2,\n                         levels = 1:4,\n                         labels = paste0('$\\\\x_2 = $', 1:4)),\n             family = family)\n    \n    if(family == 'mu_B'){\n      filtered_data$pred &lt;- filtered_data$pdx\n    } else if(family == 'mu_A'){\n      filtered_data$pred &lt;- 0.5\n    }\n    filtered_data$pred[filtered_data$pdx &lt; 0.1] &lt;- NA\n    \n    ## aes specific to 'd'\n    the_lwd &lt;-  0.65\n    the_alpha &lt;- 0.85\n    the_color &lt;- 'grey60'\n    \n    ## add the plot\n    the_plot2 &lt;- the_plot2 +\n      geom_line(data = filtered_data,\n      mapping = aes(x = x1, y = pred,\n                    group = factor(1),\n                    lty = family),\n      inherit.aes = FALSE,\n      lwd = the_lwd,\n      alpha = the_alpha,\n      color = the_color)  \n}\n\nthe_lty_values &lt;- c('mu_A' = \"32\",\n                    'mu_B' = \"solid\")\n\n\n# Add legend layers manually\nthe_plot2 &lt;- the_plot2 +\n  geom_line(lineend = \"round\", linejoin = \"round\") +\n  scale_y_continuous(labels = scales::percent, \n                     breaks = c(0, 0.5, 1), limits = c(-0.2, 1.30)) + \n  scale_linetype_manual(\n  values = the_lty_values,\n  labels = c(latex2exp::TeX('$\\\\widehat{P}(D= 1)$'),\n             latex2exp::TeX('$\\\\widehat{P}(D= 1|x_1, x_2)$')),\n  name = \"Propensity\") + \n  \n  \n   guides(\n    linetype = guide_legend(\n      order = 2,\n      override.aes = list(\n        color = \"grey70\",  \n        alpha = 1,        \n        lwd = 1 \n        )\n    ),\n    color = guide_legend(\n      order = 1,\n      override.aes = list(lwd = 1.5))) + \n  theme(plot.margin = unit(c(0, 5.5, 0, 5.5), \"pt\"),\n        strip.background = element_blank(),\n    strip.text = element_blank())\n\nfinal_fig &lt;- ggpubr::ggarrange(the_plot, the_plot2,\n                  nrow = 2, \n                  heights = c(3, 2))\n\nggsave(filename = paste0(\"figs/graph_giventariff_imp_prop_\", name, \".png\"),\n       plot = final_fig,\n       height = 5.25,\n       width = 8.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n       \nreturn(final_fig)\n  }) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           nrow = 3,\n                           widths = 15, heights = 1)\n\n\nggsave(filename = \"figs/graph_giventariff_imp_prop.png\",\n       plot = to_save_giventariff_perpop,\n        height = 14.75,\n       width = 10.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 3.4: Top row: commercial price (colored), best-estimate (solid gray) and aware (dashed gray) premiums for every scenario in terms of \\(x_1\\), \\(x_2\\) (panel) and \\(d\\) (line width). The colors represent the interval in which the implied propensity lies. Bottom row: corresponding implied propensity (colored), estimated propensity (solid gray), and proportion of \\(D=1\\) (dashed gray).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Actuarial local metrics</span>"
    ]
  },
  {
    "objectID": "4_dimensions.html",
    "href": "4_dimensions.html",
    "title": "4  Measuring the dimensions of fairness",
    "section": "",
    "text": "4.1 Measuring actuarial fairness disparities\nIn the case study of the main paper, we briefly discuss how we quantified disparities using the wasserstein distance. We briefly discuss technical limitations in the measurement of the different disparities regarding the three dimensions of fairness: actuarial fairness, solidarity, and causality.\nActuarial fairness concerns differences in expected loss ratios. Disparity implies cross-subsidies; a violation of actuarial fairness. We compute individual loss ratios as\n\\[\nLR_i = Y_i / \\text{premium}_i.\n\\]\nWe then compare their distributions across protected groups using the Wasserstein distance. Since premiums are strictly positive, individual loss ratios are well-defined.\nIn cases where claims contain many zeros, aggregated loss ratios over small subpopulations are more stable. We apply this approach in the main paper when appropriate. In our Gaussian case study, individual-level ratios suffice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring the dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "4_dimensions.html#measuring-solidarity-disparities",
    "href": "4_dimensions.html#measuring-solidarity-disparities",
    "title": "4  Measuring the dimensions of fairness",
    "section": "4.2 Measuring solidarity disparities",
    "text": "4.2 Measuring solidarity disparities\nSolidarity demands similar premiums across protected groups. Disparity is measured via the Wasserstein distance between premium distributions: A Wasserstein distance strictly greater than 0 indicates a breach in solidarity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring the dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "4_dimensions.html#measuring-causality-disparities",
    "href": "4_dimensions.html#measuring-causality-disparities",
    "title": "4  Measuring the dimensions of fairness",
    "section": "4.3 Measuring causality disparities",
    "text": "4.3 Measuring causality disparities\nTo assess causal disparity, we follow Lindholm et al. (2024) by measuring individual ``proxy effects’’. Following Côté, Côté, and Charpentier (2024), we measure proxy effects as the difference between a given premium and the aware premium. We then compare their distributions across protected groups. This aligns with the proxy parity criterion of Côté, Côté, and Charpentier (2024). Implictly, we assume that our estimate of the aware premium is unbiased, otherwise proxy effect assessment may be highly unreliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring the dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "4_dimensions.html#disparity-measurement-per-dimension",
    "href": "4_dimensions.html#disparity-measurement-per-dimension",
    "title": "4  Measuring the dimensions of fairness",
    "section": "4.4 Disparity measurement per dimension",
    "text": "4.4 Disparity measurement per dimension\n\n\nCode for the measurement of disparities per dimension\nthe_wass_table &lt;- setNames(nm = names(pregroup_pop_stats)) |&gt; lapply(function(the_scenario){\n  the_data &lt;- pregroup_pop_stats[[the_scenario]]$valid\n    \n  setNames(nm = c('mu_B', 'mu_U', 'mu_A', 'mu_H', 'mu_C', 'prem')) |&gt; \n  sapply(function(the_prem){\n        the_data$prem_to_eval = the_data[[the_prem]]\n    df_to_eval &lt;- the_data |&gt; \n      mutate(to_measure_actuarialfairness = Y/prem_to_eval, # This is the individual loss ratio on which parity per group D is desired\n             to_measure_solidarity = prem_to_eval, # This is the premium on which parity per group D is desired\n             to_measure_causality = prem_to_eval - mu_A_t # This is the 'proxy effect' on which parity per group D is desired\n             ) |&gt; \n      dplyr::select(D, \n                    to_measure_actuarialfairness,\n                    to_measure_solidarity,\n                    to_measure_causality)\n    \n    \n    lr_disparity &lt;- wasserstein_by_group(values = df_to_eval$to_measure_actuarialfairness,\n                                         group = df_to_eval$D)\n    prem_disparity &lt;- wasserstein_by_group(values = df_to_eval$to_measure_solidarity,\n                                           group = df_to_eval$D)\n    proxy_disparity &lt;- wasserstein_by_group(values = df_to_eval$to_measure_causality,\n                                            group = df_to_eval$D)\n    \n    return(\n      c('lr_disparity' = round(lr_disparity, 4),\n        'prem_disparity' = round(prem_disparity, 3),\n        'proxy_disparity' = round(proxy_disparity, 3))\n)\n  }) |&gt;  t()\n})\n\n\n\nScenario 1Scenario 2Scenario 3\n\n\n\n\nCode that generate the measurment table for scenario 1\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(DT)\n\ndatatable(\n  the_wass_table[['Scenario1']] |&gt; \n    as.data.frame() |&gt; \n    mutate(Premium = rownames(the_wass_table[['Scenario1']])) |&gt; \n    dplyr::select(Premium, proxy_disparity, lr_disparity, prem_disparity) |&gt;\n  dplyr::rename(\n    `Proxy Disparity` = proxy_disparity,\n    `LR Disparity` = lr_disparity,\n    `Premium Disparity` = prem_disparity\n  ),\n  \n  escape = FALSE,  # enables LaTeX/math rendering\n  options = list(\n    autoWidth = TRUE,\n    dom = 't',      # ONLY table, no filter, no search, no pagination\n    columnDefs = list(\n      list(className = 'dt-center', targets = 0),\n      list(className = 'dt-right', targets = 1:3)\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold;',\n    'Scenario 1: Disparity measures as wasserstein distance between D subpopulation distributions. The quantity behind the distribution depends on the dimension measured. '\n  )\n)\n\n\n\n\n\n\n\n\n\n\nCode that generate the measurment table for scenario 2\ndatatable(\n  the_wass_table[['Scenario2']] |&gt; \n    as.data.frame() |&gt; \n    mutate(Premium = rownames(the_wass_table[['Scenario2']])) |&gt; \n    dplyr::select(Premium, proxy_disparity, lr_disparity, prem_disparity) |&gt;\n  dplyr::rename(\n    `Proxy Disparity` = proxy_disparity,\n    `LR Disparity` = lr_disparity,\n    `Premium Disparity` = prem_disparity\n  ),\n  \n  escape = FALSE,  # enables LaTeX/math rendering\n  options = list(\n    autoWidth = TRUE,\n    dom = 't',      # ONLY table, no filter, no search, no pagination\n    columnDefs = list(\n      list(className = 'dt-center', targets = 0),\n      list(className = 'dt-right', targets = 1:3)\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold;',\n    'Scenario 2: Disparity measures as wasserstein distance between D subpopulation distributions. The quantity behind the distribution depends on the dimension measured. '\n  )\n)\n\n\n\n\n\n\n\n\n\n\nCode that generate the measurment table for scenario 3\ndatatable(\n  the_wass_table[['Scenario3']] |&gt; \n    as.data.frame() |&gt; \n    mutate(Premium = rownames(the_wass_table[['Scenario3']])) |&gt; \n    dplyr::select(Premium, proxy_disparity, lr_disparity, prem_disparity) |&gt;\n  dplyr::rename(\n    `Proxy Disparity` = proxy_disparity,\n    `LR Disparity` = lr_disparity,\n    `Premium Disparity` = prem_disparity\n  ),\n  \n  escape = FALSE,  # enables LaTeX/math rendering\n  options = list(\n    autoWidth = TRUE,\n    dom = 't',      # ONLY table, no filter, no search, no pagination\n    columnDefs = list(\n      list(className = 'dt-center', targets = 0),\n      list(className = 'dt-right', targets = 1:3)\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold;',\n    'Scenario 3: Disparity measures as wasserstein distance between D subpopulation distributions. The quantity behind the distribution depends on the dimension measured. '\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring the dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "4_dimensions.html#vizualization-of-alignment-with-dimensions",
    "href": "4_dimensions.html#vizualization-of-alignment-with-dimensions",
    "title": "4  Measuring the dimensions of fairness",
    "section": "4.5 Vizualization of alignment with dimensions",
    "text": "4.5 Vizualization of alignment with dimensions\nNext, we visualize premium alignment across fairness dimensions. We first standardize the measurements for comparability, then reverse their direction so that higher values indicate better alignment.\n\n\nCode for the alignment with dimensions\nthe_alignement_table &lt;- setNames(nm = names(the_wass_table)) |&gt; lapply(function(the_scen){\n  the_wass_table[[the_scen]] |&gt; \n  data.frame() |&gt; \n  mutate(proxy_disparity = (1 - proxy_disparity/max(proxy_disparity)) |&gt; round(4),\n         lr_disparity = (1 - lr_disparity/max(lr_disparity)) |&gt; round(3),\n         prem_disparity = (1 - prem_disparity/max(prem_disparity)) |&gt; round(3)) |&gt; \n    mutate(Premium = rownames(the_wass_table[['Scenario3']])) |&gt; \n    dplyr::select(Premium, proxy_disparity, lr_disparity, prem_disparity) |&gt;\n  dplyr::rename(\n    `Causality` = proxy_disparity,\n    `Actuarial fairness` = lr_disparity,\n    `Solidarity` = prem_disparity\n  )\n}) \n\n\n\n\nCode for the illustration of alignment\nif(!file.exists('figs/radar_grid_1x3.png')){\n library(fmsb)\nlibrary(colorspace)\n\n# Output\ndev_id &lt;-png(paste0(\"figs/radar_grid_1x3.png\"), width = 7, height = 4.0, units = \"in\", res = 300)\n\n# Then define layout\npar(mfrow = c(1, 3), mar = c(0.8, 0.8, 0.0, 0.8), oma = c(1.0, 0.0, 0.0, 0.0))\n\n# Define palette\nthe_vec &lt;- c(RColorBrewer::brewer.pal(5, 'Spectral'), \"black\") %&gt;% darken(0.2)\nlty_vec &lt;- c(\"solid\", \"solid\", \"solid\", \"12\", \"12\",  \"32\")\n\nfor (the_scen in names(the_alignement_table)) {\n  the_local_table &lt;- the_alignement_table$Scenario1\n\n  data_2 &lt;- the_local_table %&gt;% select(`Actuarial fairness`, Causality, Solidarity)\n  data_radar &lt;- rbind(\n      c(1, 1, 1),\n      c(0, 0, 0),\n      data_2\n    ) %&gt;% as.data.frame()\n  rownames(data_radar) &lt;- c(\"Max\", \"Min\", the_local_table$Premium)\n  \n  radarchart(data_radar,\n               maxmin = TRUE,\n               cglty = 1,\n               cglcol = \"grey91\",\n               plty = lty_vec,\n               pty = 32,\n               pcol = adjust_transparency(the_vec, 0.6),\n               plwd = c(4.5, 3, 3, 1.5),\n               pfcol = adjust_transparency(the_vec, 0.05),\n               vlcex = 1,\n               vlabels = c(\"Actuarial fairness\", \"Causality\", \"Solidarity\"))\n  mtext(paste0('Scenario ', which(the_scen == names(the_alignement_table))), side = 3, line = -3.5, cex = 1.1)\n}\n\n# Reset for legend\npar(mfrow = c(1, 1), mar = c(0, 0, 0, 0))\n\n## Add a single legend in last panel\nlegend(\"bottom\",\n       title = \"Premium\",\n       legend = latex2exp::TeX(c(\"$\\\\widehat{\\\\mu}^B$\",\"$\\\\widehat{\\\\mu}^U$\", \"$\\\\widehat{\\\\mu}^A$\",\n                                 \"$\\\\widehat{\\\\mu}^H$\", \"$\\\\widehat{\\\\mu}^C$\", \"\\\\pi\")),\n       bty = \"n\", col = the_vec,\n       text.col = \"black\",\n       cex = 0.75,\n       lty = lty_vec, \n       lwd = c(4.5, 3, 3, 1.5),\n       xpd = TRUE,\n       inset = c(0.2, 0))\n\ndev.off() \n}\n\n\n\n\n\nFigure 4.1: Alignment of the different benchmark premiums and the given commercial price with the dimensions of fairness.\n\n\n\n\n\n\n\nScenario 1Scenario 2Scenario 3\n\n\n\n\nCode that generate the alignment table for scenario 1\ndatatable(\n  the_alignement_table[['Scenario1']],\n  \n  escape = FALSE,  # enables LaTeX/math rendering\n  options = list(\n    autoWidth = TRUE,\n    dom = 't',      # ONLY table, no filter, no search, no pagination\n    columnDefs = list(\n      list(className = 'dt-center', targets = 0),\n      list(className = 'dt-right', targets = 1:3)\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold;',\n    'Scenario 1: Alignment scores across fairness dimensions for each premium. Measures are standardized; a score of 0 denotes the weakest alignment among all premiums for that dimension.'\n  )\n)\n\n\n\n\n\n\n\n\n\n\nCode that generate the alignment table for scenario 2\ndatatable(\n  the_alignement_table[['Scenario2']],\n  \n  escape = FALSE,  # enables LaTeX/math rendering\n  options = list(\n    autoWidth = TRUE,\n    dom = 't',      # ONLY table, no filter, no search, no pagination\n    columnDefs = list(\n      list(className = 'dt-center', targets = 0),\n      list(className = 'dt-right', targets = 1:3)\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold;',\n    'Scenario 2: Alignment scores across fairness dimensions for each premium. Measures are standardized; a score of 0 denotes the weakest alignment among all premiums for that dimension.'\n  )\n)\n\n\n\n\n\n\n\n\n\n\nCode that generate the alignment table for scenario 3\ndatatable(\n  the_alignement_table[['Scenario3']],\n  \n  escape = FALSE,  # enables LaTeX/math rendering\n  options = list(\n    autoWidth = TRUE,\n    dom = 't',      # ONLY table, no filter, no search, no pagination\n    columnDefs = list(\n      list(className = 'dt-center', targets = 0),\n      list(className = 'dt-right', targets = 1:3)\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold;',\n    'Scenario 3: Alignment scores across fairness dimensions for each premium. Measures are standardized; a score of 0 denotes the weakest alignment among all premiums for that dimension.'\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCôté, Marie-Pier, Olivier Côté, and Arthur Charpentier. 2024. “Selection Bias in Insurance: Why Portfolio-Specific Fairness Fails to Extend Market-Wide.” Available at SSRN: 10.2139/Ssrn.5018749.\n\n\nLindholm, M., R. Richman, A. Tsanakas, and M. V. Wüthrich. 2024. “Sensitivity-Based Measures of Discrimination in Insurance Pricing.” Available at SSRN: Ssrn.com/Abstract=4897265.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring the dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "2_training_spectrum.html#unaware-premium",
    "href": "2_training_spectrum.html#unaware-premium",
    "title": "2  Estimating the five fairness premiums",
    "section": "2.2 Unaware premium",
    "text": "2.2 Unaware premium\nThe unaware price, \\(\\mu^U(\\mathbf{x})\\), excludes direct use of \\(D\\). It is defined as the best predictor of \\(\\widehat{\\mu}^B(\\mathbf{X}, D)\\) given \\(\\mathbf{x}\\):\n\\[\\begin{equation*}  \n    \\widehat{\\mu}^U(\\mathbf{x}) = E\\{\\widehat{\\mu}^B(\\mathbf{x}, D) | X = \\mathbf{x}\\}.  \n\\end{equation*}\\]\nThis maintains consistency with the best estimate premium, whether \\(\\widehat{\\mu}^B\\) is data-driven or based on indicated rates (Complement 5 of the main paper). One can use a to predict \\(\\widehat{\\mu}^B(\\mathbf{X}, D)\\) based on \\(\\mathbf{X}\\). The loss function defaults to mean squared error as it is a reasonable distributional assumption for this response variable.\nAlternatively (what we do), one can estimate the propensity and explicitly weight the best-estimate premiums based on predicted propensity. This also keeps consistency with the best-estimate.\n\n\nEstimating the propensity function\nsource('___lgb_pdx_estimate.R') \n\n## clean the preds repo\nunlink(file.path('preds', \"*_pdx.json\"))\n\n  hyperparameter_grid &lt;- expand.grid(\n    learning_rate = c(0.01),\n    feature_fraction = c(0.75),\n    bagging_fraction = c(0.75),\n    max_depth = c(5),\n    lambda_l1 = c(0.5),\n    lambda_l2 = c(0.5)\n  )\n\npdx_lgb &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name){\n  list_df &lt;- list('train' = sims[[name]],\n                  'valid' = valid[[name]],\n                  'test' = test[[name]])\n  the_pdx_lightgbm_fun(list_data = list_df, name = name,\n                       hyperparameter_grid = hyperparameter_grid)\n})\n\n\nPdx for scenario:  Scenario1  \nhyperparam  1 / 1 \nBest valid binary logloss: 0.5808773  \noptimal ntree: 391  \nTraining time: 15.42748  sec. \nPdx for scenario:  Scenario2  \nhyperparam  1 / 1 \nBest valid binary logloss: 0.5808827  \noptimal ntree: 416  \nTraining time: 15.39385  sec. \nPdx for scenario:  Scenario3  \nhyperparam  1 / 1 \nBest valid binary logloss: 0.6352072  \noptimal ntree: 433  \nTraining time: 19.27892  sec. \n\n\n\n\nTraining the propensity function on experimental data (for later use in section 5)\n# Define grid for hyperparameter optimization\nhyperparameter_grid_sims &lt;- expand.grid(\n    learning_rate = c(0.01),\n    bagging_fraction = c(0.75),\n    max_depth = c(6),\n    lambda_l1 = c(0.5),\n    lambda_l2 = c(0.5)\n  )\n\npdx_sims &lt;- lapply(seq_along(sim_samples), function(idx){\n    list_df &lt;- list('train' = sim_samples[[idx]]$train,\n                  'valid' = sim_samples[[idx]]$valid,\n                  'test' = sim_samples[[idx]]$test)\n  the_pdx_lightgbm_fun(list_data = list_df, name = paste0('sim', idx),\n                       hyperparameter_grid = hyperparameter_grid_sims)\n})",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "2_training_spectrum.html#sec-est_aw",
    "href": "2_training_spectrum.html#sec-est_aw",
    "title": "2  Estimating the five fairness premiums",
    "section": "2.3 Aware premium",
    "text": "2.3 Aware premium\nThe aware premium, \\(\\widehat{\\mu}^A(\\mathbf{x})\\), requires knowledge of the marginal distribution of \\(D\\). The aware price, a particular case of the ``discrimination-free’’ price of Lindholm et al. (2022), is computed as:\n\\[\\begin{equation*}\n    \\widehat{\\mu}^A(\\mathbf{x}) = \\sum_{d\\in \\mathcal{D}} \\widehat{\\mu}^B(\\mathbf{x}, d) \\widehat{\\Pr}(D = d).\n\\end{equation*}\\] As discussed earlier, we assume \\(D\\) is discrete or discretized. If the training sample is representative of the target population (portfolio, market, or region?), empirical proportions are consistent estimators of \\(\\widehat{\\Pr}(D = d)\\). This is also an estimator suggested in Section 5 of Lindholm et al. (2022).\n\n\nComputing the empirical proportions for protected supopulations\nmarg_dist &lt;- sims %&gt;% lapply(function(data){\n  data$D %&gt;% table %&gt;%  prop.table\n})\n\n## for later use\nsaveRDS(marg_dist, 'preds/the_empirical_proportions.rds')\n\n## Computing the empirical proportions for protected supopulations on experimental data (for later use in section 5)\nmarg_dist_sims &lt;- seq_along(sim_samples) %&gt;% lapply(function(idx){\n  sim_samples[[idx]]$train$D %&gt;% table %&gt;%  prop.table\n})\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis method generalizes the use of \\(D\\) as a control variable to prevent distortions in estimated effect of \\(\\mathbf{X}\\) due to omitted variable bias. It applies to all model types and has a causal interpretation under the assumptions that: (i) possible values of \\(\\mathbf{X}\\) are observed across all groups of \\(D\\) (no extrapolation), (ii) \\(D\\) causes both \\(\\mathbf{X}\\) and \\(Y\\), and (iii) there are no relevant unobserved variables. Under the same assumptions, other methods are estimators of the aware premium. One example is inverse probability weighting, which re-weights observations to (artificially) remove the association between \\(\\mathbf{X}\\) and \\(D\\), preventing proxy effects.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "2_training_spectrum.html#sec-est_corr",
    "href": "2_training_spectrum.html#sec-est_corr",
    "title": "2  Estimating the five fairness premiums",
    "section": "2.4 Corrective premium",
    "text": "2.4 Corrective premium\nWe estimate the corrective premium, \\(\\widehat{\\mu}^C(\\mathbf{x}, d)\\), designed to enforce strong demographic parity by aligning premium distributions across protected groups. Various methods can achieve this, but we recommend the optimal transport approach of Hu, Ratz, and Charpentier (2024). Its advantage lies in modifying \\(\\widehat{\\mu^B}(\\mathbf{x}, d)\\) while keeping the overall spectrum estimation anchored to the initial best-estimate premium. As an incremental adjustment, it minimizes modeling complexity while enforcing demographic parity, yielding a simple estimator for the corrective premium.\nThe corrective premium is computed by training a transport function that shifts best-estimate premiums for each protected group toward a common barycenter, ensuring demographic parity through corrective direct discrimination. See Hu, Ratz, and Charpentier (2024) for details. This optimal transport method is implemented in Python via the Equipy package of Fernandes Machado et al. (2025). The following chunk illustrates how this method, despite its complexity, translates into concise Python code.\n\n\n\nListing 2.1: Illustrative Python code for wasserstein transport toward corrective premiums.\n\n\nimport numpy as np\nimport pandas as pd\nfrom equipy.fairness import FairWasserstein\n\n# Load best-estimate premiums and associated sensitive attributes\nbest_est_train = pd.read_csv('best_est_prem_train.csv')  # Training premiums\nsens_train = pd.read_csv('sensitive_train.csv')  # discrete sensitive data\n\n# Train Fair Wasserstein transport to adjust premiums\nbarycenter = FairWasserstein()\nbarycenter.fit(best_est.values, sens.values)\n\n# Load best-estimate premiums and associated sensitive attributes\nbest_est_test = pd.read_csv('best_est_prem_test.csv')  # Training premiums\nsens_test = pd.read_csv('sensitive_test.csv')  # discrete sensitive data\n\n# Apply transformation to obtain fairness-adjusted premiums\ncorrective_premiums_test = barycenter.transform(best_est_test.values, sens_test.values)\n\n# Save results\npd.DataFrame(corrective_premiums).to_csv('corr_prem_test.csv', index=False)\n\n\n\n\n\n\n\n\n\nOptimal transport and Wasserstein distance : technical details\n\n\n\nLet \\(Z\\) be a quantity of interest. Let \\(\\nu_0\\) and \\(\\nu_1\\) be two laws of \\(Z\\) (finite \\(p\\)-th moments, \\(p\\ge1\\)).\nProbabilistic definition (pairing).\nOver all joint laws with the right marginals,\n\\[\nW_p(\\nu_0,\\nu_1)\n=\\Big(\\inf_{\\,\\mathcal L(Z^{(0)})=\\nu_0,\\ \\mathcal L(Z^{(1)})=\\nu_1}\\\n\\mathbb E\\big(\\,\\lVert Z^{(0)}-Z^{(1)}\\rVert^p\\,\\big)\\Big)^{1/p}.\n\\]\nInterpretation: the smallest average \\(p\\)-power gap under any admissible pairing.\nUnits: same as \\(Z\\); \\(W_p=0\\) iff \\(\\nu_0=\\nu_1\\).\nUnivariate formula (what we compute).\nLet \\(F\\) and \\(G\\) be the CDFs of \\(\\nu_0\\) and \\(\\nu_1\\), with quantiles \\(F^{-1}\\) and \\(G^{-1}\\). Then\n\\[\nW_p(\\nu_0,\\nu_1)=\\left(\\int_{0}^{1}\\!\\big|F^{-1}-G^{-1}\\big|^p\\,du\\right)^{1/p}.\n\\] For \\(p=1\\), \\[\nW_1(\\nu_0,\\nu_1)=\\int_{0}^{1}\\!\\big|F^{-1}(u)-G^{-1}(u)\\big|\\,du\n\\] Interpretation: \\(W_1\\) is the average absolute gap between matched quantiles (interpretable in dollars if \\(Z\\) is a monetary amount).\nEmpirical, univariate (equal sample sizes).\nGiven samples \\(z^{(0)}_{1:n}\\) from \\(\\mu\\) and \\(z^{(1)}_{1:n}\\) from \\(\\nu\\), sort: \\[\nz^{(0)}_{(1)}\\le\\cdots\\le z^{(0)}_{(n)},\\qquad\nz^{(1)}_{(1)}\\le\\cdots\\le z^{(1)}_{(n)}.\n\\] Then \\[\n\\widehat W_p^p=\\frac{1}{n}\\sum_{i=1}^{n}\\big|z^{(1)}_{(i)}-z^{(0)}_{(i)}\\big|^p.\n\\]\nUnequal exposures (weighted quantiles).\nPick a grid size \\(m\\in\\mathbb N\\) and midpoints \\(u_k=(k-\\tfrac12)/m\\) for \\(k=1,\\dots,m\\) (so \\(\\Delta u=1/m\\)).\nIf \\(Q_1\\) and \\(Q_0\\) are the (exposure-weighted) quantile functions of the two groups, then \\[\n\\widehat W_p^{\\,p}\\ \\approx\\ \\frac{1}{m}\\sum_{k=1}^{m}\\big|Q_1(u_k)-Q_0(u_k)\\big|^p.\n\\]\nRelevance for actuaries: it compares entire distributions (center and tails), reports in dollar units, and is heavily theoretically grounded.\n\n\n\n\nComputing the optimal transport mapping for the scenarios\nsource_python(\"___opt_transp.py\")\n\n## now, the folder 'transported' exist, with one epsilon_y file per scenario\n\n\nTo predict corrective premium on other datasets, one can alternatively train a lightgbm model of mapping from \\((X, D)\\) to the resulting corrective premiums from Equipy on the training sample.\n\n\nComputing the optimal transport mapping for the scenarios\nsource('___lgb_mapping_to_corr.R') \n\ncorr_mapping_lgb &lt;- setNames(nm = names(sims)) %&gt;%\n  lapply(the_mapping_to_corr_lightgbm_fun)\n\n\nMapping to corr. for scenario:  Scenario1  \nData import: 0.4800718  sec. \nData conversion: 0.01207399  sec. \nLgb training : 19.99445  sec. \nMapping to corr. for scenario:  Scenario2  \nData import: 0.389869  sec. \nData conversion: 0.01291299  sec. \nLgb training : 37.44629  sec. \nMapping to corr. for scenario:  Scenario3  \nData import: 0.450701  sec. \nData conversion: 0.133497  sec. \nLgb training : 26.2301  sec.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "2_training_spectrum.html#hyperaware-premium",
    "href": "2_training_spectrum.html#hyperaware-premium",
    "title": "2  Estimating the five fairness premiums",
    "section": "2.5 Hyperaware premium",
    "text": "2.5 Hyperaware premium\nThe hyperaware premium, \\(\\widehat{\\mu}^H(\\mathbf{x})\\), is the best non-directly discriminatory approximation of the corrective premium. We construct a lightgbm using only \\(\\mathbf{X}\\) to predict the corrective premium, aiming at demographic parity without direct reliance on \\(D\\):\n\\[\\begin{equation*}\n    \\widehat{\\mu}^H(\\mathbf{x}) = E\\{\\widehat{\\mu}^C(\\mathbf{x}, D) | X = \\mathbf{x}\\}.\n\\end{equation*}\\]\nSince the response variable is a premium, MSE is a correct choice of loss function in most cases. Just as for the unaware premium, one can explicitly aggregate corrective premiums across protected groups using estimated propensities as weights. This is what we do here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "2_training_spectrum.html#visualization-the-estimated-spectrum",
    "href": "2_training_spectrum.html#visualization-the-estimated-spectrum",
    "title": "2  Estimating the five fairness premiums",
    "section": "2.6 Visualization the estimated spectrum",
    "text": "2.6 Visualization the estimated spectrum\nBy combining all the trained models as component into a trained spectrum of fairness, we can define the premium function that will serve to predict the premiums\n\n\nDefining the estimated premium and metrics functions\npremiums &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name){\n  premium_generator(best = best_lgb[[name]]$pred_fun, \n                  pdx = pdx_lgb[[name]]$pred_fun, \n                  maps_to_corr = corr_mapping_lgb[[name]], \n                  marg = marg_dist[[name]])\n})\n\nquants &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name){\n  quant_generator(premiums = premiums[[name]])\n})\n\n\n## For experimental data (future use in section 5)\npremiums_sims &lt;- setNames(obj = seq_along(sim_samples),\n                        nm = names(sim_samples)) %&gt;% lapply(function(idx){\n  premium_generator(best = best_sims[[idx]]$pred_fun, \n                  pdx = pdx_sims[[idx]]$pred_fun, \n                  maps_to_corr = corr_mapping_lgb[['Scenario1']], ## We put anything, won't be used anyway as we focus on proxy vulnerabiltiy for section 5. \n                  marg = marg_dist_sims[[idx]])\n})\n\nquants_sims &lt;- setNames(obj = seq_along(sim_samples),\n                        nm = names(sim_samples)) %&gt;% lapply(function(idx){\n  quant_generator(premiums = premiums_sims[[idx]])\n})\n\n\n\n\nComputation of estimated premiums and local metrics across the grid\ndf_to_g_file &lt;- \"preds/df_to_g.json\"\n\n# Check if the JSON file exists\nif (file.exists(df_to_g_file)) {\n  message(sprintf(\"[%s] File exists. Reading df_to_g from %s\", Sys.time(), df_to_g_file))\n  df_to_g &lt;- fromJSON(df_to_g_file)\n} else {\n\n## From the first section of the online supplement\npreds_grid_stats_theo &lt;- fromJSON('preds/preds_grid_stats_theo.json')\n  \ndf_to_g &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name) {\n  message(sprintf(\"[%s] Processing: %s\", Sys.time(), name))\n  \n  local_scenario_df &lt;- preds_grid_stats_theo[[name]]\n  \n  # Start time for this scenario\n  start_time &lt;- Sys.time()\n  \n  # Step 1: Compute premiums\n  message(sprintf(\"[%s] Step 1: Computing premiums\", Sys.time()))\n  premium_results &lt;- setNames(nm = levels_for_premiums) %&gt;%\n    sapply(function(s) {\n      message(sprintf(\"[%s] Computing premium: %s\", Sys.time(), s))\n      premiums[[name]][[s]](\n        x1 = local_scenario_df$x1,\n        x2 = local_scenario_df$x2,\n        d = local_scenario_df$d\n      )\n    })\n  \n  # Step 2: Compute PDX\n  message(sprintf(\"[%s] Step 2: Computing PDX\", Sys.time()))\n  pdx_results &lt;- pdx_lgb[[name]]$pred_fun(\n    data.frame(\n      X1 = local_scenario_df$x1,\n      X2 = local_scenario_df$x2,\n      D = local_scenario_df$d\n    )\n  )\n  \n  # Combine results\n  message(sprintf(\"[%s] Step 5: Combining results\", Sys.time()))\n  result &lt;- data.frame(\n    local_scenario_df,\n    premium_results,\n    pdx = pdx_results\n  )\n  \n  # Log completion time\n  end_time &lt;- Sys.time()\n  message(sprintf(\"[%s] Finished processing: %s (Duration: %.2f seconds)\", end_time, name, as.numeric(difftime(end_time, start_time, units = \"secs\"))))\n  \n  return(result)\n})\n\n# Save the entire df_to_g object to JSON\n  message(sprintf(\"[%s] Saving df_to_g to %s\", Sys.time(), df_to_g_file))\n  toJSON(df_to_g, pretty = TRUE, auto_unbox = TRUE) %&gt;% write(df_to_g_file)\n  rm(df_to_g_theo)\n}\n\ngrid_stats_path &lt;- 'preds/preds_grid_stats.json'\n\n# Check and load or compute preds_grid_stats\nif (file.exists(grid_stats_path)) {\n  preds_grid_stats &lt;- fromJSON(grid_stats_path)\n} else {\n  preds_grid_stats &lt;- setNames(nm = names(df_to_g)) %&gt;% \n    lapply(function(name) {\n      data.frame(df_to_g[[name]], \n                 setNames(nm = levels_for_quants) %&gt;% \n                       sapply(function(s) {\n                         quants[[name]][[s]](x1 = df_to_g[[name]]$x1,\n                                             x2 = df_to_g[[name]]$x2,\n                                             d = df_to_g[[name]]$d)\n                       }))\n    })\n  toJSON(preds_grid_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(grid_stats_path)\n}\n\n\n\n\nComputing estimated premiums and local metrics for the simulations\npop_to_g_file &lt;- \"preds/pop_to_g.json\"\n\n# Check if the JSON file exists\nif (file.exists(pop_to_g_file)) {\n  message(sprintf(\"[%s] File exists. Reading pop_to_g from %s\", Sys.time(), pop_to_g_file))\n  pop_to_g &lt;- fromJSON(pop_to_g_file)\n} else {\n## From the first section of the online supplement\npreds_pop_stats_theo &lt;- fromJSON('preds/preds_pop_stats_theo.json')\n  \npop_to_g &lt;- setNames(nm = names(sims)) %&gt;% lapply(function(name) {\n  message(sprintf(\"[%s] Processing: %s\", Sys.time(), name))\n  \n  # Start time for this simulation\n  start_time &lt;- Sys.time()\n  \n  list_data &lt;- list('train' = sims[[name]],\n                    'valid' = valid[[name]],\n                    'test' = test[[name]])\n  \n  result &lt;- setNames(nm = names(list_data)) %&gt;% lapply(function(nm){\n    \n    data &lt;- list_data[[nm]]\n    \n    # Step 1: Compute premiums\n  message(sprintf(\"[%s] Step 1: Computing premiums\", Sys.time()))\n  premium_results &lt;- setNames(nm = levels_for_premiums) %&gt;%\n    sapply(function(s) {\n      message(sprintf(\"[%s] Computing premium: %s\", Sys.time(), s))\n      premiums[[name]][[s]](\n        x1 = data$X1,\n        x2 = data$X2,\n        d = data$D\n      )\n    })\n  \n  # Step 2: Compute PDX\n  message(sprintf(\"[%s] Step 2: Computing PDX\", Sys.time()))\n  pdx_results &lt;- pdx_lgb[[name]]$pred_fun(\n    data.frame(\n      X1 = data$X1,\n      X2 = data$X2,\n      D = data$D\n    )\n  )\n  \n  # Combine results\n  message(sprintf(\"[%s] Step 5: Combining results\", Sys.time()))\n  data.frame(\n    preds_pop_stats_theo[[name]][[nm]],\n    premium_results,\n    pdx = pdx_results\n  )\n  }) \n  \n  \n  # Log completion time\n  end_time &lt;- Sys.time()\n  message(sprintf(\"[%s] Finished processing: %s (Duration: %.2f seconds)\", end_time, name, as.numeric(difftime(end_time, start_time, units = \"secs\"))))\n  \n  return(result)\n})\n\n# Save the entire df_to_g object to JSON\n  message(sprintf(\"[%s] Saving pop_to_g to %s\", Sys.time(), pop_to_g_file))\n  toJSON(pop_to_g, pretty = TRUE, auto_unbox = TRUE) %&gt;% write(pop_to_g_file)\n  \n  rm(pop_to_g_theo)\n}\n\n\npop_stats_path &lt;- 'preds/preds_pop_stats.json'\n\n# Check and load or compute preds_pop_stats\nif (file.exists(pop_stats_path)) {\n  preds_pop_stats &lt;- fromJSON(pop_stats_path)\n} else {\n  preds_pop_stats &lt;- setNames(nm = names(sims)) %&gt;% \n    lapply(function(name) {\n      setNames(nm = names(pop_to_g[[name]])) %&gt;%  \n        lapply(function(set) {\n          local_df &lt;- pop_to_g[[name]][[set]]\n          \n          data.frame(local_df, \n                 setNames(nm = levels_for_quants) %&gt;% \n                       sapply(function(s) {\n                         quants[[name]][[s]](x1 = local_df$X1,\n                                             x2 = local_df$X2,\n                                             d =  local_df$D)\n                       }))\n        })\n    })\n  toJSON(preds_pop_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(pop_stats_path)\n}\n\n\n\n\nComputing estimated premiums and local metrics for the experiment\nsims_to_g_file &lt;- \"preds/sims_to_g.json\"\n\n# Check if the JSON file exists\nif (file.exists(sims_to_g_file)) {\n  message(sprintf(\"[%s] File exists. Reading sims_to_g from %s\", Sys.time(), sims_to_g_file))\n  sims_to_g &lt;- fromJSON(sims_to_g_file)\n} else {\n## From the first section of the online supplement\npreds_sims_stats_theo &lt;- fromJSON('preds/preds_sims_stats_theo.json')\nsims_to_g &lt;- setNames(object = seq_along(sim_samples), \n                      nm = names(sim_samples)) %&gt;% lapply(function(idx) {\n  message(sprintf(\"[%s] Processing: %s\", Sys.time(), paste0(idx, '/', length(sim_samples))))\n  \n  samples_to_ret &lt;- setNames(nm = names(sim_samples[[idx]])) %&gt;% lapply(function(nm){\n  data &lt;- preds_sims_stats_theo[[idx]][[nm]]\n    \n    # Step 1: Compute premiums\n  message(sprintf(\"[%s] Step 1: Computing premiums\", Sys.time()))\n  premium_results &lt;- setNames(nm = levels_for_premiums) %&gt;%\n    sapply(function(s) {\n      message(sprintf(\"[%s] Computing premium: %s\", Sys.time(), s))\n      premiums_sims[[idx]][[s]](\n        x1 = data$X1,\n        x2 = data$X2,\n        d = data$D\n      )\n    })\n  \n  # Step 2: Compute PDX\n  message(sprintf(\"[%s] Step 2: Computing PDX\", Sys.time()))\n  pdx_results &lt;- pdx_sims[[idx]]$pred_fun(\n    data.frame(\n      X1 = data$X1,\n      X2 = data$X2,\n      D = data$D\n    )\n  )\n  \n  \n  # Combine results\n  message(sprintf(\"[%s] Step 5: Combining results\", Sys.time()))\n  result &lt;- data.frame(\n    data,\n    premium_results,\n    pdx = pdx_results\n  )\n    \n  })\n  \n  return(samples_to_ret)\n})\n\n# Save the entire df_to_g object to JSON\n  message(sprintf(\"[%s] Saving sims_to_g to %s\", Sys.time(), sims_to_g_file))\n  toJSON(sims_to_g, pretty = TRUE, auto_unbox = TRUE) %&gt;% write(sims_to_g_file)\n}\n\nsims_stats_path &lt;- 'preds/preds_sims_stats.json'\n\n# Check and load or compute preds_pop_stats\nif (file.exists(sims_stats_path)) {\n  preds_sims_stats &lt;- fromJSON(sims_stats_path)\n} else {\n  preds_sims_stats &lt;- setNames(nm = names(sims_to_g)) %&gt;% \n    lapply(function(name) {\n      setNames(nm = names(sims_to_g[[name]])) %&gt;%  \n        lapply(function(set) {\n          local_df &lt;- sims_to_g[[name]][[set]]\n          \n          data.frame(local_df, \n                 setNames(nm = levels_for_quants) %&gt;% \n                       sapply(function(s) {\n                         quants_sims[[name]][[s]](x1 = local_df$X1,\n                                             x2 = local_df$X2,\n                                             d =  local_df$D)\n                       }))\n        })\n    })\n  toJSON(preds_sims_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(sims_stats_path)\n}\n\n\n\n\nR code producing the estimated propensity illustration across scenarios\nto_save_pdx_perpop &lt;- names(df_to_g) %&gt;% \n  lapply(function(name){\n    cols &lt;- the_CAS_colors\n    pop_id &lt;- which(names(df_to_g) == name)\n    \n    ## keep only axis for last plot\n    if(pop_id == 1){ # If it's the last, apply correct xlabels\n      the_y_scale &lt;- ylim(0,1)\n      the_y_label &lt;- latex2exp::TeX(\"$\\\\widehat{P}(D = 1|x_1, x_2)$\")\n    } else { # otherwise, remove everything\n      the_y_scale &lt;- scale_y_continuous(labels = NULL, limits = c(0,1))\n      the_y_label &lt;- NULL\n    }\n    \n    the_pops &lt;- c('Scenario 1', 'Scenario 2', 'Scenario 3')\n    \n    ## lets graph\n    df_to_g[[name]] %&gt;% \n      mutate(the_population = name) %&gt;% \n  filter(x1 &gt;= -9, x1 &lt;= 11,\n         d == 1) %&gt;% \n  group_by(x1, x2) %&gt;% summarise(pdx = mean(pdx)) %&gt;%  ungroup %&gt;% \n  ggplot(aes(x = x1, y = pdx,\n             lty = factor(x2),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(x2),\n             color = factor(x2))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '31', '21', '11'), name = latex2exp::TeX('$x_2$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$x_2$')) +\n  scale_linewidth_manual(values = c(2, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.65, 0.75, 0.85, 0.9), name = latex2exp::TeX('$x_2$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"),\n       y = the_y_label,\n       title = latex2exp::TeX(the_pops[pop_id])) + \n  scale_x_continuous(breaks = c(-3:3)*3 + 1)  + # see above\n  theme_minimal() + \n  the_y_scale +\n  theme(plot.title = element_text(hjust=0.5))\n  }) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           nrow = 1,\n                           widths = 15, heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nif (!dir.exists('figs')) dir.create('figs')\nggsave(filename = \"figs/graph_pdx_perpop.png\",\n       plot = to_save_pdx_perpop,\n       height = 3.25,\n       width = 7.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 2.1: Estimated propensity in terms of \\(x_1\\) and \\(x_2\\) for simulations\n\n\n\n\n\n\n\n\nR code producing the estimated best-estimate, unaware, and aware illustration.\nto_save_premiumsdense_BUA_perpop &lt;- names(df_to_g) %&gt;% lapply(function(pop_name){\n  \n  ## the colors\n    cols &lt;- RColorBrewer::brewer.pal(5, 'Spectral')[1:3] %&gt;%  colorspace::darken(0.25)\n      # the_CAS_colors_full[c(6, 5, 2)]\n  pop_id &lt;- which(names(df_to_g) == pop_name)\n  \n  ## keep only axis for last plot\n    if(pop_name == head(names(df_to_g), 1)){ # If it's the last, apply correct xlabels\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = scales::dollar,\n                     limits = c(90, 140))\n      the_y_label &lt;- \n  latex2exp::TeX(\"$\\\\widehat{\\\\mu}(x_1, x_2, d)$\")\n    } else { # otherwise, remove everything\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = NULL,\n                     limits = c(90, 140))\n      the_y_label &lt;- NULL\n    }\n  \n  to_illustrate &lt;- df_to_g[[pop_name]] %&gt;%\n  reshape2::melt(measure.vars = paste0(levels_for_premiums[1:3])) %&gt;% \n    mutate(variable = factor(variable,\n                             levels = paste0(levels_for_premiums[1:3]),\n                             labels = labels_for_premiums[1:3])) %&gt;% \n  filter(x1 &lt;= 10, x1 &gt;= -8) \n  \n  to_illustrate$value[to_illustrate$pdx &lt; 0.1] &lt;- NA\n  \n  to_illustrate %&gt;% \n  ggplot(aes(x = x1, y = value,\n             lty = factor(d),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(d),\n             color = factor(variable))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '32'), name = latex2exp::TeX('$d$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$\\\\mu$'), labels = latex2exp::TeX) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.35, 0.7), name = latex2exp::TeX('$d$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"), y = the_y_label,\n       title = paste0('Scenario ', pop_id)) +\n  scale_x_continuous(breaks = c( -3, 0, 3, 6), limits = c(-4, 7)) +\n  the_y_scale +\n  theme_minimal()\n}) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           ncol = 3,\n                           widths = c(18, 15, 15), heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nggsave(filename = \"figs/graph_premiumsdense_BUA_perpop.png\",\n       plot = to_save_premiumsdense_BUA_perpop,\n       height = 4,\n       width = 10.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 2.2: Estimated best-estimate \\(\\widehat{\\mu}^B\\), unaware \\(\\widehat{\\mu}^U\\), and aware \\(\\widehat{\\mu}^A\\) premiums for scenarios 1, 2, and 3 in the Example as a function of \\(x_1\\), \\(x_2\\), and \\(d\\).\n\n\n\n\n\n\n\n\nR code producing the estimated aware, hyperaware, corrective illustration.\nto_save_premiumsdense_AHC_perpop &lt;- names(df_to_g) %&gt;% lapply(function(pop_name){\n  \n  ## the colors\n    cols &lt;- RColorBrewer::brewer.pal(5, 'Spectral')[3:5] %&gt;%  colorspace::darken(0.25)\n  pop_id &lt;- which(names(df_to_g) == pop_name)\n  \n  ## keep only axis for last plot\n    if(pop_name == head(names(df_to_g), 1)){ # If it's the last, apply correct xlabels\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = scales::dollar,\n                     limits = c(90, 130))\n      the_y_label &lt;- \n  latex2exp::TeX(\"$\\\\widehat{\\\\mu}(x_1, x_2, d)$\")\n    } else { # otherwise, remove everything\n      the_y_scale &lt;- scale_y_continuous(breaks = c(90, 110, 130),\n                     labels = NULL,\n                     limits = c(90, 130))\n      the_y_label &lt;- NULL\n    }\n  \n  to_illustrate &lt;- df_to_g[[pop_name]] %&gt;%\n  reshape2::melt(measure.vars = paste0(levels_for_premiums[3:5])) %&gt;% \n    mutate(variable = factor(variable,\n                             levels = paste0(levels_for_premiums[3:5]),\n                             labels = labels_for_premiums[3:5])) %&gt;% \n  filter(x1 &lt;= 10, x1 &gt;= -8) \n  # group_by(x1, d, variable) %&gt;% summarise(value = mean(value)) %&gt;%  ungroup %&gt;% \n  \n  ## mask part of the line \n  to_illustrate$value[to_illustrate$pdx &lt; 0.1] &lt;- NA\n  \n  to_illustrate %&gt;% \n  ggplot(aes(x = x1, y = value,\n             lty = factor(d),\n             linewidth = factor(x2),\n             shape = factor(x2),\n             alpha = factor(d),\n             color = factor(variable))) +\n  geom_line() +\n  scale_linetype_manual(values = c('solid', '32'), name = latex2exp::TeX('$d$')) +\n  scale_color_manual(values = cols, name = latex2exp::TeX('$\\\\mu$'), labels = latex2exp::TeX) +\n  scale_linewidth_manual(values = c(1.5, 1, 0.85, 0.55), name = latex2exp::TeX('$x_2$')) +  \n  scale_alpha_manual(values = c(0.35, 0.7), name = latex2exp::TeX('$d$')) + \n  labs(x = latex2exp::TeX(\"$x_1$\"), y = the_y_label,\n       title = paste0('Scenario ', pop_id)) +\n  scale_x_continuous(breaks = c( -3, 0, 3, 6), limits = c(-4, 7)) +\n  the_y_scale +\n  theme_minimal()\n}) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           ncol = 3,\n                           widths = c(18, 15, 15),\n                            heights = 1,\n                           common.legend = T,\n                           legend = 'right')\n\nggsave(filename = \"figs/graph_premiumsdense_AHC_perpop.png\",\n       plot = to_save_premiumsdense_AHC_perpop,\n       height = 4,\n       width = 10.55,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 2.3: Estimated aware \\(\\widehat{\\mu}^A\\), hyperaware \\(\\widehat{\\mu}^H\\), and corrective \\(\\widehat{\\mu}^C\\) premiums for scenarios 1, 2, and 3 in the Example as a function of \\(x_1\\), \\(x_2\\), and \\(d\\).\n\n\n\n\n\n\n\n\n\n\nChevalier, Dominik, and Marie-Pier Côté. 2024. “From Point to Probabilistic Gradient Boosting for Claim Frequency and Severity Prediction.” arXiv Preprint arXiv:2412.14916. https://arxiv.org/abs/2412.14916.\n\n\nFernandes Machado, Agathe, Suzie Grondin, Philipp Ratz, Arthur Charpentier, and François Hu. 2025. “EquiPy: Sequential Fairness Using Optimal Transport in Python.” arXiv Preprint arXiv:2503.09866. https://arxiv.org/abs/2503.09866.\n\n\nHu, Francois, Philipp Ratz, and Arthur Charpentier. 2024. “A Sequentially Fair Mechanism for Multiple Sensitive Attributes.” Proceedings of the AAAI Conference on Artificial Intelligence 38 (11): 12502–10. https://doi.org/10.1609/aaai.v38i11.29143.\n\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. “LightGBM: A Highly Efficient Gradient Boosting Decision Tree.” Advances in Neural Information Processing Systems 30: 3146–54.\n\n\nLindholm, Mathias, Ronald Richman, Andreas Tsanakas, and Mario V Wüthrich. 2022. “Discrimination-Free Insurance Pricing.” ASTIN Bulletin 52 (1): 55–89.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "5_partitioning.html#partitioning-policyholders-following-proxy_vulnerability",
    "href": "5_partitioning.html#partitioning-policyholders-following-proxy_vulnerability",
    "title": "5  Partitioning",
    "section": "5.2 Experiment on partioning proxy vulnerability for scenario 1",
    "text": "Figure 5.1: Estimated propensity in terms of \\(x_1\\) and \\(x_2\\) for simulations\n\n\n\n\n\n\n\n\nCode for the visualisation of the evtrees\nlibrary(rpart)\nlibrary(ggparty, partykit)\n\ntemp_tree &lt;- c('evtree', 'rpart') %&gt;% lapply(function(the_algo){\n  names(pregroup_grid_stats) %&gt;% lapply(function(pop_name){\npop_id &lt;- which(names(pregroup_grid_stats) == pop_name)\n\n# Compute sequential terminal node IDs\nparty_tree &lt;- my_trees$proxy_vuln[[pop_name]][[paste0('best_', the_algo)]]$model\nif (the_algo == 'rpart'){\nparty_tree &lt;- partykit::as.party(party_tree)\n}\n\nterminal_ids &lt;- nodeids(party_tree, terminal = TRUE)  # Original terminal node IDs\nsequential_ids &lt;- seq_along(terminal_ids)  # Create sequential IDs\nid_mapping &lt;- data.frame(terminal_id = terminal_ids, sequential_id = sequential_ids)\n\n\n## Compute average prediction per terminal node\n# Extract predictions and terminal node IDs\npredictions &lt;- fitted(party_tree)\navg_prediction &lt;- aggregate(`(response)` ~ `(fitted)`,\n                            data = predictions,\n                            FUN = mean)\n\ntree_plot &lt;- ggparty(party_tree) +\n  geom_edge() +\n  geom_edge_label(mapping = aes(label = !!sym(\"breaks_label\")),\n                  size = 3) +\n  geom_node_label(\n    line_list = list(\n      aes(label = splitvar),\n      aes(label = paste(\"N =\", nodesize))\n    ),\n    line_gpar = list(\n      list(size = 10),\n      list(size = 8)\n    ),\n    ids = \"inner\",\n  ) +\n  geom_node_label(\n    line_list = list(\n      aes(label = paste0(\"Node \",\n                         match(id, id_mapping$terminal_id),\n                         \", N = \",\n                         nodesize)),\n      aes(label = paste0(\"Avg Pred. = \",\n                                     round(avg_prediction$`(response)`[match(id, avg_prediction$`(fitted)`)], 2)))\n    ),\n    line_gpar = list(\n      list(size = 8),\n      list(size = 10)\n    ),\n    ids = \"terminal\", nudge_y = -0.45, nudge_x = 0.01,\n    label.size = 0.15,\n    size = 3\n  ) +\n  geom_node_plot(\n    gglist = list(\n      geom_boxplot(aes(x = \"\", y = resp,\n                       color = ..middle..,\n                       fill = ..middle..),  # Color by median\n                   outlier.color = \"black\"\n                   , alpha = 0.7\n                   ),\n      theme_minimal(),\n      scale_fill_gradient2(\n        low = \"#D7CC39\", mid = \"grey75\", high = \"#CAA8F5\",\n        midpoint = 0, name = \"Median Value\"\n      ),\n      scale_color_gradient2(\n        low = colorspace::darken(\"#D7CC39\", 0.3), mid = colorspace::darken(\"grey75\", 0.3),\n        high = colorspace::darken(\"#CAA8F5\", 0.3),\n      midpoint = 0, name = \"Median Value\"\n      ),\n      xlab(\"\"), ylab(latex2exp::TeX(\"$\\\\widehat{\\\\Delta}_{proxy}(X_1, X_2)$\")),\n      scale_y_continuous(labels = scales::dollar),\n      theme(axis.text.x = element_blank(),\n            axis.title.y = element_text(margin = margin(l = -10)),\n            axis.title.x = element_text(margin = margin(r = 20)))\n    ),\n    shared_axis_labels = TRUE\n  ) +\n  ggtitle(latex2exp::TeX(paste0('Partition of proxy vulnerable individuals for scenario ', pop_id))) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5)\n  )\n  \n}) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           nrow = 3,\n                           widths = 15, heights = 1,\n                           common.legend = T,\n                           legend = 'right') %&gt;% \nggsave(filename = paste0(\"figs/graph_trees_\", the_algo,\"_proxy.png\"),\n       plot = .,\n       height = 16,\n       width = 12,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n})\nrm(temp_tree)\n\n\n\n5.1.1 Saving important quantites\n\n\nSaving dictionnaries of partition rules and predictions\ndictionnary_leaves_trees &lt;- setNames(nm = names(my_trees)) %&gt;%  lapply(function(resp_tree){\n  setNames(nm = names(my_trees[[resp_tree]])) %&gt;% lapply(function(pop_name){\n      temp_to_pred &lt;- pregroup_grid_stats[[pop_name]]\n      names(temp_to_pred) &lt;- toupper(names(temp_to_pred))\n      \n      model_ev &lt;- my_trees[[resp_tree]][[pop_name]]$best_evtree$model\n      model_rpart &lt;- my_trees[[resp_tree]][[pop_name]]$best_rpart$model \n      model_rpart_prune &lt;- prune(model_rpart,\n                        cp = model_rpart$cptable[which(model_rpart$cptable[, \"nsplit\"] + 1 == 8), \"CP\"]) %&gt;% as.party()\n      model_rpart &lt;- model_rpart %&gt;% as.party()\n      \n      to_return_evtree &lt;- data.frame('node_or' = predict(model_ev, newdata = temp_to_pred,\n                                  type = 'node') %&gt;% unname,\n                                  'pred' = predict(model_ev, newdata = temp_to_pred,\n                                                   type = 'response') %&gt;% unname %&gt;%\n                                    round(3)) %&gt;% distinct() %&gt;% arrange(-pred) %&gt;%\n        mutate('node_new' = 1:n())\n      \n      to_return_rpart &lt;- data.frame('node_or' = predict(model_rpart, newdata = temp_to_pred,\n                                  type = 'node') %&gt;% unname,\n                                  'pred' = predict(model_rpart, newdata = temp_to_pred, type = 'response') %&gt;% unname %&gt;%\n                                    round(3)) %&gt;% distinct() %&gt;% arrange(-pred) %&gt;%\n        mutate('node_new' = 1:n())\n      \n      to_return_rpart_prune &lt;- data.frame('node_or' = predict(model_rpart_prune, newdata = temp_to_pred,\n                                  type = 'node') %&gt;% unname,\n                                  'pred' = predict(model_rpart_prune, newdata = temp_to_pred,\n                                                   type = 'response') %&gt;% unname %&gt;%\n                                    round(3)) %&gt;% distinct() %&gt;% arrange(-pred) %&gt;%\n        mutate('node_new' = 1:n())\n      \n      paths_ev &lt;- extract_paths_for_all_terminals(tree = model_ev)\n      paths_rpart &lt;- extract_paths_for_all_terminals(tree = model_rpart)\n      paths_rpart_prune &lt;- extract_paths_for_all_terminals(tree = model_rpart_prune)\n      \n       list('evtree' = list('dict' = to_return_evtree,\n                            'model' = model_ev,\n                            'paths' = paths_ev),\n           'rpart' = list('dict' = to_return_rpart,\n                          'model' = model_rpart,\n                          'paths' = paths_rpart),\n           'rpart_prune' = list('dict' = to_return_rpart_prune,\n                                'model' = model_rpart_prune,\n                                'paths' = paths_rpart_prune))\n    })\n})\n\nsaveRDS(dictionnary_leaves_trees, 'evtree/dictionnary_leaves_trees.rds')\n\n### Applying partition to the data\ngroup_grid_path = 'preds/group_grid_stats.json'\ngroup_pop_path = 'preds/group_pop_stats.json'\n\n\n# Check and load or compute group_grid_stats\nif (file.exists(group_grid_path)) {\n  temp_grid_stats &lt;- fromJSON(group_grid_path) \n  \n  \n  group_grid_stats &lt;- setNames(nm = names(temp_grid_stats)) |&gt; lapply(function(pop_name){\n    temp_grid_stats[[pop_name]] |&gt; \n      mutate(proxy_g_evtree = proxy_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_evtree)), decreasing = T)),\n           proxy_g_rpart = proxy_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_rpart)), decreasing = T)),\n           cload_g_evtree = cload_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_evtree)), decreasing = T)),\n           cload_g_rpart = cload_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_rpart)), decreasing = T)))\n  })\n  \n  rm(temp_grid_stats)\n    \n} else {\n  group_grid_stats &lt;- setNames(nm = names(pregroup_grid_stats)) %&gt;% lapply(function(pop_name){\n    temp_to_pred &lt;-   pregroup_grid_stats[[pop_name]]\n    names(temp_to_pred) &lt;- toupper(names(temp_to_pred))\n    \n    pred_proxy_g_evtree &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_proxy_g_rpart &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_evtree &lt;- predict(my_trees$comm_load[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_rpart &lt;- predict(my_trees$comm_load[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    \n    data.frame(pregroup_grid_stats[[pop_name]],\n                 proxy_g_evtree = pred_proxy_g_evtree %&gt;% factor(., levels = sort(unique(pred_proxy_g_evtree), decreasing = T)),\n                 proxy_g_rpart = pred_proxy_g_rpart %&gt;% factor(., levels = sort(unique(pred_proxy_g_rpart), decreasing = T)),\n                 cload_g_evtree = pred_cload_g_evtree %&gt;% factor(., levels = sort(unique(pred_cload_g_evtree), decreasing = T)),\n                 cload_g_rpart = pred_cload_g_rpart %&gt;% factor(., levels = sort(unique(pred_cload_g_rpart), decreasing = T))\n               )\n  })\n  toJSON(group_grid_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(group_grid_path)\n}\n\n# Check and load or compute group_pop_stats\nif (file.exists(group_pop_path)) {\n   temp_pop_stats &lt;- fromJSON(group_pop_path) \n  \n  \n  group_pop_stats &lt;- setNames(nm = names(temp_pop_stats)) |&gt; lapply(function(pop_name){\n    setNames(nm = names(temp_pop_stats[[pop_name]])) |&gt; lapply(function(the_set){\n      temp_pop_stats[[pop_name]][[the_set]] |&gt; \n        mutate(proxy_g_evtree = proxy_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_evtree)), decreasing = T)),\n           proxy_g_rpart = proxy_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_rpart)), decreasing = T)),\n           cload_g_evtree = cload_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_evtree)), decreasing = T)),\n           cload_g_rpart = cload_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_rpart)), decreasing = T)))\n    }) \n  })\n  \n  rm(temp_pop_stats)\n  \n} else {\n  group_pop_stats &lt;- setNames(nm = names(pregroup_pop_stats)) %&gt;% lapply(function(pop_name){\n    setNames(nm = names(pregroup_pop_stats[[pop_name]])) %&gt;% lapply(function(set){\n      \n      temp_to_pred &lt;- pregroup_pop_stats[[pop_name]][[set]]\n      \n    pred_proxy_g_evtree &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_proxy_g_rpart &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_evtree &lt;- predict(my_trees$comm_load[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_rpart &lt;- predict(my_trees$comm_load[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    \n    data.frame(pregroup_pop_stats[[pop_name]][[set]],\n                 proxy_g_evtree = pred_proxy_g_evtree %&gt;% factor(., levels = sort(unique(pred_proxy_g_evtree), decreasing = T)),\n                 proxy_g_rpart = pred_proxy_g_rpart %&gt;% factor(., levels = sort(unique(pred_proxy_g_rpart), decreasing = T)),\n                cload_g_evtree = pred_cload_g_evtree %&gt;% factor(., levels = sort(unique(pred_cload_g_evtree), decreasing = T)),\n                 cload_g_rpart = pred_cload_g_rpart %&gt;% factor(., levels = sort(unique(pred_cload_g_rpart), decreasing = T))\n                )\n    })  \n})\n  toJSON(group_pop_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(group_pop_path)\n}\n\n\n\n\n5.1.2 Visualizing the partition\n\n\nGraph for the illustration of the partitioning\nn_bottom &lt;- 5\nn_top &lt;- 5\n\nsetNames(nm = names(group_pop_stats)) %&gt;%  lapply(function(pop_name){\n  ## the colors\npop_id &lt;- which(names(group_pop_stats) == pop_name)\n    \nlocal_to_g &lt;- group_grid_stats[[pop_name]] %&gt;% \nfilter(x1 &lt;= 8, x1 &gt;= -5, d == 1) \n\nif(pop_name == head(names(group_grid_stats), 1)){\n  the_y_scale_top &lt;- scale_y_continuous(labels = scales::dollar, breaks = c(-5, 0, 5, 10), limits = c(-6, 14))\n  the_y_label_top &lt;- latex2exp::TeX(\"$\\\\Delta_{proxy}(x_1, x_2)$\")\n  the_y_scale &lt;- scale_y_discrete()\n  the_y_label &lt;- latex2exp::TeX('$x_2$')\n} else {\n  the_y_scale_top &lt;- scale_y_continuous(labels = NULL, breaks = c(-5, 0, 5, 10), limits = c(-6, 14))\n  the_y_label_top &lt;- NULL\n    the_y_scale &lt;-scale_y_discrete(labels = NULL)\n  the_y_label &lt;- NULL\n}\n\nlocal_pop_g &lt;- group_pop_stats[[pop_name]]$valid\n\nlocal_to_g$proxy_g_evtree_g &lt;- local_to_g$proxy_g_evtree\nlocal_pop_g$proxy_g_evtree_g &lt;- local_pop_g$proxy_g_evtree\nlevels(local_to_g$proxy_g_evtree_g) &lt;- recode_bottom_top_middle(levels = levels(local_to_g$proxy_g_evtree), \n                                                          numeric = TRUE, \n                                                          n_bottom = n_bottom, \n                                                          n_top = n_top, \n                                                          factor_values = local_to_g$proxy_g_evtree)\nlevels(local_pop_g$proxy_g_evtree_g) &lt;- recode_bottom_top_middle(levels = levels(local_pop_g$proxy_g_evtree), \n                                                          numeric = TRUE, \n                                                          n_bottom = n_bottom, \n                                                          n_top = n_top, \n                                                          factor_values = local_pop_g$proxy_g_evtree)\n\nlevels(local_to_g$proxy_g_evtree_g)[n_top + 1] &lt;- levels(local_pop_g$proxy_g_evtree_g)[n_top + 1]\n\ng_proxy &lt;- local_to_g %&gt;% \n  mutate(code = paste0(x2, '_', as.numeric(proxy_g_evtree_g))) %&gt;% \n  ggplot(aes(x = x1, y = proxy_vuln,\n             group = factor(code),\n             color = factor(proxy_g_evtree_g))) + \n  geom_line(aes(x = x1, y = proxy_vuln_t,\n                lty = factor(x2), group = factor(x2)),\n                color = 'black', size = 0.8) +\n  geom_line(size = 3, alpha = 0.78, lineend = \"round\", linejoin = \"round\") + \n  theme_classic() + \n  labs(x = latex2exp::TeX('$x_1$'),\n       y = the_y_label_top,\n       title = paste0('Scenario ', pop_id)) + \n  scale_color_manual(values = RColorBrewer::brewer.pal(n_bottom + n_top + 1, 'Spectral')  %&gt;% colorspace::darken(0.05),\n                     name = latex2exp::TeX('$\\\\widehat{\\\\Delta}^{ev}_{proxy}(\\\\textbf{x})$'),\n                     labels = levels(local_to_g$proxy_g_evtree_g) %&gt;% as.numeric %&gt;% round(2)) + \n  scale_linetype_manual(values = c('12',  '21', '32', 'solid'), name = latex2exp::TeX('$x_2$')) +\n  the_y_scale_top + \n  # geom_abline(slope = 0, intercept = 0, lty = '34', color= 'black', size= 0.7, alpha = 0.2) + \n  scale_x_continuous(labels = NULL, breaks = c(-3:3)*3 + 1) + # see above \n  guides(\n    linetype = guide_legend(order = 1), # x2 legend on top\n    color = guide_legend(order = 2)    # k legend below x2\n  )\n\ng_population &lt;- local_pop_g %&gt;%  \n  ggplot(aes(y = factor(X2), x = X1,\n             color = factor(proxy_g_evtree_g),\n             fill = factor(proxy_g_evtree_g))) + \n  geom_jitter(#position = position_identity(), \n              width = 0, height = 0.4, alpha = 0.2) + \n  scale_color_manual(values = RColorBrewer::brewer.pal(n_bottom + n_top + 1, 'Spectral') %&gt;% colorspace::darken(0.05),\n                     name = latex2exp::TeX('$\\\\widehat{\\\\Delta}^{ev}_{proxy}(\\\\textbf{x})$'),\n                     labels = levels(local_to_g$proxy_g_evtree) %&gt;% as.numeric %&gt;% round(1)) + \n  scale_fill_brewer(palette = 'Spectral', name = latex2exp::TeX('$k$')) + \n  theme_classic() + \n  the_y_scale +\n  scale_x_continuous(breaks = c(-3:3)*3 + 1, limits = c(-5, 8)) + \n  labs(x = latex2exp::TeX(\"$x_1$\") ,\n       y = the_y_label) + \n  theme( axis.title.y = element_text(\n      margin = margin(t = 50), # Add padding\n    )) \nggpubr::ggarrange(g_proxy, g_population, \n                  nrow = 2, common.legend = T,\n                  legend = 'right',\n                  heights = c(4, 3),\n                  align = \"v\") \n}) %&gt;%  \n  ggpubr::ggarrange(plotlist = .,\n                    ncol = 3,\n                    widths = c(6, 5, 5)) %&gt;% \n  ggsave(filename = \"figs/graph_proxy_clusters_and_pop_scenario.png\",\n       plot = .,\n       height = 6.25,\n       width = 11.50,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 5.2: Partition of proxy vulnerability across the three scenarios (columns) in the example. The top row compares theoretical proxy vulnerability (black) with estimated values from the lightgbm, the latter colored by the group formed by the tree. The bottom row shows the partition in the \\((x_1, x_2)\\) domain, with noise added to~\\(x_2\\) for clarity.\n\n\n\n\n\n\n\n\n5.2 Experiment on partioning proxy vulnerability for scenario 1\nTo support the partitioning methodology proposed in section 6 of the main paper, we present a simulation study to identify best practices and gain insights into optimal implementation.\nFollowing the set of equations of Scenario 1 in Chapter 1, we simulate \\(M = 100\\) set of \\(N = 3000\\) samples split into \\((N_{\\text{train}}, N_{\\text{valid}}, N_{\\text{test}}) = (2000, 500, 500)\\) for train, validation, and test. Our aim is to assess the capacity of the methodology to recover proxy-vulnerable distinct subpopulations, precisely identify the most at-risk groups, and predict accurately the proxy vulnerability. We compute the BIC (under Gaussian assumption) of the estimated proxy vulnerability as compared with the test theoretical proxy vulnerability, and the accuracy in the partitioning as compared with the true proxy vulnerable groups : the eight subpopulations formed by the crossing of \\(\\{X_1 \\leq 1, X_1 &gt; 1\\}\\) and \\(X_2 \\in \\{1, 2, 3, 4\\}\\).\nOn each sample set, we estimated unaware and aware premiums using the methodology described Chapter 2 and we computed proxy vulnerability \\(\\widehat{\\Delta}_{\\text{proxy}}(x_1, x_2)\\) via Eq. 1 of the main paper. We partitioned the feature space using \\((X_1, X_2, D)\\). Models used rpart (locally optimal) and evtree (globally optimal) regression trees, with hyperparameters tuned via validation BIC (Gaussian assumption): minimum leaf size proportion \\(w \\in \\{0.03, 0.05\\}\\), tree depth \\(d \\in \\{3, 4\\}\\), and complexity parameter \\(\\alpha \\in \\{1, 2\\}\\). For rpart, we pruned a deep tree to minimize validation error. We compared performance under fixed (\\(k = 8\\)) and optimized leaf counts, retaining the best model per case for each implementation (four total).\n\n\nCode for the partition experiment.\nif (!file.exists(\"preds/proxy_sims_results.csv\")) {\n  source(\"___evtree_experiment.R\")\n  \n  # Load data\n  preds_sims_stats &lt;- jsonlite::fromJSON(\"preds/preds_sims_stats.json\")\n  \n  # Process data\n  proxy_sims_results &lt;- process_data_evtree_experiment(preds_sims_stats)\n  \n  # Save results to CSV\n  write.csv(proxy_sims_results, \"preds/proxy_sims_results.csv\", row.names = FALSE)\n  cat(\"Results saved to preds/proxy_sims_results.csv\")\n\n  } else {\n  proxy_sims_results &lt;- read_csv(\"preds/proxy_sims_results.csv\")\n  cat(\"Results read from preds/proxy_sims_results.csv\")\n}\n\n\nResults read from preds/proxy_sims_results.csv\n\n\nThe following table presents the results of our experiment. When the number of groups is correctly set to \\(k = 8\\), the \\(8\\times8\\) accuracy and relaxed \\(8\\times8\\) accuracy (which counts adjacent diagonals in the confusion matrix as correct) confirm that the method effectively groups individuals, with little difference between and . However, when \\(k\\) is unknown, \\(k = 8\\) was never seen as optimal. Validation metrics (based on estimated proxy vulnerability) favor larger \\(k\\), while oracle performance (based on theoretical proxy vulnerability) suggests better \\(R^2\\) at \\(k = 8\\). Whether \\(k\\) is known or not, partitioning identifies truly vulnerable individuals (top 12% by theoretical proxy vulnerability) with over 93% precision, fulfilling its primary objective. Since \\(k\\) is unknown in practice, strong regularization is essential to prevent excessive partitioning.\n\n\nCode for producing the experimental result table\nsummary_results &lt;- proxy_sims_results %&gt;% \n  summarise(\n  # Metrics for \"known k\" (k = 8) evtree\n  k_8 = mean(proxy_sims_results$num_leaf_8, na.rm = TRUE),\n  k_8_sd = sd(proxy_sims_results$num_leaf_8, na.rm = TRUE),\n  minbucket_8 = mean(proxy_sims_results$minbucket_8, na.rm = TRUE),\n  minbucket_8_sd = sd(proxy_sims_results$minbucket_8, na.rm = TRUE),\n  maxdepth_8 = mean(proxy_sims_results$maxdepth_8, na.rm = TRUE),\n  maxdepth_8_sd = sd(proxy_sims_results$maxdepth_8, na.rm = TRUE),\n  alpha_8 = mean(proxy_sims_results$alpha_8, na.rm = TRUE),\n  alpha_8_sd = sd(proxy_sims_results$alpha_8, na.rm = TRUE),\n  validation_mse_8 = mean(proxy_sims_results$validation_mse_8, na.rm = TRUE),\n  validation_mse_8_sd = sd(proxy_sims_results$validation_mse_8, na.rm = TRUE),\n  oracle_r2_8 = mean(proxy_sims_results$r2_oracle_8, na.rm = TRUE),\n  oracle_r2_8_sd = sd(proxy_sims_results$r2_oracle_8, na.rm = TRUE),\n  acc_test_8 = mean(proxy_sims_results$accuracy_test_8, na.rm = TRUE),\n  acc_test_8_sd = sd(proxy_sims_results$accuracy_test_8, na.rm = TRUE),\n  relaxed_acc_test_8 = mean(proxy_sims_results$relaxed_accuracy_test_8,\n                            na.rm = TRUE),\n  relaxed_acc_test_8_sd = sd(proxy_sims_results$relaxed_accuracy_test_8,\n                             na.rm = TRUE),\n  top_acc_8 = mean(proxy_sims_results$top_acc_8, na.rm = TRUE),\n  top_acc_8_sd = sd(proxy_sims_results$top_acc_8, na.rm = TRUE),\n  bottom_acc_8 = mean(proxy_sims_results$bottom_acc_8, na.rm = TRUE),\n  bottom_acc_8_sd = sd(proxy_sims_results$bottom_acc_8, na.rm = TRUE),\n  time_8 = mean(proxy_sims_results$time_8, na.rm = TRUE),  # Time assumed same for any/known\n  time_8_sd = sd(proxy_sims_results$time_8, na.rm = TRUE),\n  \n  \n    \n  # Metrics for \"known k\" (k = 8) rpart\n  k_r8 = mean(proxy_sims_results$num_leaf_r8, na.rm = TRUE),\n  k_r8_sd = sd(proxy_sims_results$num_leaf_r8, na.rm = TRUE),\n  minbucket_r8 = mean(proxy_sims_results$minbucket_r8, na.rm = TRUE),\n  minbucket_r8_sd = sd(proxy_sims_results$minbucket_r8, na.rm = TRUE),\n  maxdepth_r8 = mean(proxy_sims_results$maxdepth_r8, na.rm = TRUE),\n  maxdepth_r8_sd = sd(proxy_sims_results$maxdepth_r8, na.rm = TRUE),\n  cp_8 = mean(proxy_sims_results$cp_8, na.rm = TRUE),\n  cp_8_sd = sd(proxy_sims_results$cp_8, na.rm = TRUE),\n  validation_mse_r8 = mean(proxy_sims_results$validation_mse_r8, na.rm = TRUE),\n  validation_mse_r8_sd = sd(proxy_sims_results$validation_mse_r8, na.rm = TRUE),\n  oracle_r2_r8 = mean(proxy_sims_results$r2_oracle_r8, na.rm = TRUE),\n  oracle_r2_r8_sd = sd(proxy_sims_results$r2_oracle_r8, na.rm = TRUE),\n  acc_test_r8 = mean(proxy_sims_results$accuracy_test_r8, na.rm = TRUE),\n  acc_test_r8_sd = sd(proxy_sims_results$accuracy_test_r8, na.rm = TRUE),\n  relaxed_acc_test_r8 = mean(proxy_sims_results$relaxed_accuracy_test_r8, na.rm = TRUE),\n  relaxed_acc_test_r8_sd = sd(proxy_sims_results$relaxed_accuracy_test_r8, na.rm = TRUE),\n  top_acc_r8 = mean(proxy_sims_results$top_acc_r8, na.rm = TRUE),\n  top_acc_r8_sd = sd(proxy_sims_results$top_acc_r8, na.rm = TRUE),\n  bottom_acc_r8 = mean(proxy_sims_results$bottom_acc_r8, na.rm = TRUE),\n  bottom_acc_r8_sd = sd(proxy_sims_results$bottom_acc_r8, na.rm = TRUE),\n  time_r8 = mean(proxy_sims_results$time_r8, na.rm = TRUE),  # Time assumed same for any/known\n  time_r8_sd = sd(proxy_sims_results$time_r8, na.rm = TRUE),\n\n  # Metrics for \"any k\" rpart\n  k_rany = mean(proxy_sims_results$num_leaf_rany, na.rm = TRUE),\n  k_rany_sd = sd(proxy_sims_results$num_leaf_rany, na.rm = TRUE),\n  minbucket_rany = mean(proxy_sims_results$minbucket_rany, na.rm = TRUE),\n  minbucket_rany_sd = sd(proxy_sims_results$minbucket_rany, na.rm = TRUE),\n  maxdepth_rany = mean(proxy_sims_results$maxdepth_rany, na.rm = TRUE),\n  maxdepth_rany_sd = sd(proxy_sims_results$maxdepth_rany, na.rm = TRUE),\n  cp_any = mean(proxy_sims_results$cp_any, na.rm = TRUE),\n  cp_any_sd = sd(proxy_sims_results$cp_any, na.rm = TRUE),\n  validation_mse_rany = mean(proxy_sims_results$validation_mse_rany, na.rm = TRUE),\n  validation_mse_rany_sd = sd(proxy_sims_results$validation_mse_rany, na.rm = TRUE),\n  oracle_r2_rany = mean(proxy_sims_results$r2_oracle_rany, na.rm = TRUE),\n  oracle_r2_rany_sd = sd(proxy_sims_results$r2_oracle_rany, na.rm = TRUE),\n  top_acc_rany = mean(proxy_sims_results$top_acc_rany, na.rm = TRUE),\n  top_acc_rany_sd = sd(proxy_sims_results$top_acc_rany, na.rm = TRUE),\n  bottom_acc_rany = mean(proxy_sims_results$bottom_acc_rany, na.rm = TRUE),\n  bottom_acc_rany_sd = sd(proxy_sims_results$bottom_acc_rany, na.rm = TRUE),\n  time_rany = mean(proxy_sims_results$time_rany, na.rm = TRUE),  # Time is shared, no distinction\n  time_rany_sd = sd(proxy_sims_results$time_rany, na.rm = TRUE),\n  \n    # Metrics for \"any k\" evtree\n  k_any = mean(proxy_sims_results$num_leaf_any, na.rm = TRUE),\n  k_any_sd = sd(proxy_sims_results$num_leaf_any, na.rm = TRUE),\n  minbucket_any = mean(proxy_sims_results$minbucket_any, na.rm = TRUE),\n  minbucket_any_sd = sd(proxy_sims_results$minbucket_any, na.rm = TRUE),\n  maxdepth_any = mean(proxy_sims_results$maxdepth_any, na.rm = TRUE),\n  maxdepth_any_sd = sd(proxy_sims_results$maxdepth_any, na.rm = TRUE),\n  alpha_any = mean(proxy_sims_results$alpha_any, na.rm = TRUE),\n  alpha_any_sd = sd(proxy_sims_results$alpha_any, na.rm = TRUE),\n  validation_mse_any = mean(proxy_sims_results$validation_mse_any, na.rm = TRUE),\n  validation_mse_any_sd = sd(proxy_sims_results$validation_mse_any, na.rm = TRUE),\n  oracle_r2_any = mean(proxy_sims_results$r2_oracle_any, na.rm = TRUE),\n  oracle_r2_any_sd = sd(proxy_sims_results$r2_oracle_any, na.rm = TRUE),\n  top_acc_any = mean(proxy_sims_results$top_acc_any, na.rm = TRUE),\n  top_acc_any_sd = sd(proxy_sims_results$top_acc_any, na.rm = TRUE),\n  bottom_acc_any = mean(proxy_sims_results$bottom_acc_any, na.rm = TRUE),\n  bottom_acc_any_sd = sd(proxy_sims_results$bottom_acc_any, na.rm = TRUE),\n  time_any = mean(proxy_sims_results$time_any, na.rm = TRUE),  # Time is shared, no distinction\n  time_any_sd = sd(proxy_sims_results$time_any, na.rm = TRUE),\n  \n  # Add new metrics for eo, e8, ro, r8\n    recall_top_pct_eo = mean(proxy_sims_results$recall_top_pct_eo, na.rm = TRUE),\n    recall_top_pct_eo_sd = sd(proxy_sims_results$recall_top_pct_eo, na.rm = TRUE),\n    prec_top_pct_eo = mean(proxy_sims_results$prec_top_pct_eo, na.rm = TRUE),\n    prec_top_pct_eo_sd = sd(proxy_sims_results$prec_top_pct_eo, na.rm = TRUE),\n    acc_top_pct_eo = mean(proxy_sims_results$acc_top_pct_eo, na.rm = TRUE),\n    acc_top_pct_eo_sd = sd(proxy_sims_results$acc_top_pct_eo, na.rm = TRUE),\n    effpct_top_pct_eo = mean(proxy_sims_results$effpct_top_pct_eo, na.rm = TRUE),\n    effpct_top_pct_eo_sd = sd(proxy_sims_results$effpct_top_pct_eo, na.rm = TRUE),\n    \n    recall_top_pct_e8 = mean(proxy_sims_results$recall_top_pct_e8, na.rm = TRUE),\n    recall_top_pct_e8_sd = sd(proxy_sims_results$recall_top_pct_e8, na.rm = TRUE),\n    prec_top_pct_e8 = mean(proxy_sims_results$prec_top_pct_e8, na.rm = TRUE),\n    prec_top_pct_e8_sd = sd(proxy_sims_results$prec_top_pct_e8, na.rm = TRUE),\n    acc_top_pct_e8 = mean(proxy_sims_results$acc_top_pct_e8, na.rm = TRUE),\n    acc_top_pct_e8_sd = sd(proxy_sims_results$acc_top_pct_e8, na.rm = TRUE),\n    effpct_top_pct_e8 = mean(proxy_sims_results$effpct_top_pct_e8, na.rm = TRUE),\n    effpct_top_pct_e8_sd = sd(proxy_sims_results$effpct_top_pct_e8, na.rm = TRUE),\n    \n    recall_top_pct_ro = mean(proxy_sims_results$recall_top_pct_ro, na.rm = TRUE),\n    recall_top_pct_ro_sd = sd(proxy_sims_results$recall_top_pct_ro, na.rm = TRUE),\n    prec_top_pct_ro = mean(proxy_sims_results$prec_top_pct_ro, na.rm = TRUE),\n    prec_top_pct_ro_sd = sd(proxy_sims_results$prec_top_pct_ro, na.rm = TRUE),\n    acc_top_pct_ro = mean(proxy_sims_results$acc_top_pct_ro, na.rm = TRUE),\n    acc_top_pct_ro_sd = sd(proxy_sims_results$acc_top_pct_ro, na.rm = TRUE),\n    effpct_top_pct_ro = mean(proxy_sims_results$effpct_top_pct_ro, na.rm = TRUE),\n    effpct_top_pct_ro_sd = sd(proxy_sims_results$effpct_top_pct_ro, na.rm = TRUE),\n    \n    recall_top_pct_r8 = mean(proxy_sims_results$recall_top_pct_r8, na.rm = TRUE),\n    recall_top_pct_r8_sd = sd(proxy_sims_results$recall_top_pct_r8, na.rm = TRUE),\n    prec_top_pct_r8 = mean(proxy_sims_results$prec_top_pct_r8, na.rm = TRUE),\n    prec_top_pct_r8_sd = sd(proxy_sims_results$prec_top_pct_r8, na.rm = TRUE),\n    acc_top_pct_r8 = mean(proxy_sims_results$acc_top_pct_r8, na.rm = TRUE),\n    acc_top_pct_r8_sd = sd(proxy_sims_results$acc_top_pct_r8, na.rm = TRUE),\n    effpct_top_pct_r8 = mean(proxy_sims_results$effpct_top_pct_r8, na.rm = TRUE),\n    effpct_top_pct_r8_sd = sd(proxy_sims_results$effpct_top_pct_r8, na.rm = TRUE)\n)\n\n# Step 3: Transform results into a cleaner format\nsummary_table &lt;- data.frame(\n  Metric = c(\n    \"Number of Leaves (k)\", \"Min Split\", \"Max Depth\", \"Alpha\", \n    \"Validation BIC\", \"Oracle R²\", \"Accuracy Test\", \"Relaxed Accuracy Test\",\n    \"Top Accuracy\", \"Bottom Accuracy\", \"Detect recall\", \"Detect effective %\", \"Time\"\n  ),\n  Rpart_Known_k_Mean = c(\n    summary_results$k_r8, summary_results$minbucket_r8, summary_results$maxdepth_r8,\n    summary_results$cp_8, summary_results$validation_mse_r8, summary_results$oracle_r2_r8,\n    summary_results$acc_test_r8, summary_results$relaxed_acc_test_r8,\n    summary_results$top_acc_r8, summary_results$bottom_acc_r8, summary_results$recall_top_pct_r8, summary_results$effpct_top_pct_r8, summary_results$time_r8\n  ),\n  Rpart_Known_k_SD = c(\n    summary_results$k_r8_sd, summary_results$minbucket_r8_sd, summary_results$maxdepth_r8_sd,\n    summary_results$cp_8_sd, summary_results$validation_mse_r8_sd, summary_results$oracle_r2_r8_sd,\n    summary_results$acc_test_r8_sd, summary_results$relaxed_acc_test_r8_sd,\n    summary_results$top_acc_r8_sd, summary_results$bottom_acc_r8_sd,  summary_results$recall_top_pct_r8_sd, summary_results$effpct_top_pct_r8_sd, summary_results$time_r8_sd\n  ),\n  Ev_Known_k_Mean = c(\n    summary_results$k_8, summary_results$minbucket_8, summary_results$maxdepth_8,\n    summary_results$alpha_8, summary_results$validation_mse_8, summary_results$oracle_r2_8,\n    summary_results$acc_test_8, summary_results$relaxed_acc_test_8,\n    summary_results$top_acc_8, summary_results$bottom_acc_8,summary_results$recall_top_pct_e8, summary_results$effpct_top_pct_e8,  summary_results$time_8\n  ),\n  Ev_Known_k_SD = c(\n    summary_results$k_8_sd, summary_results$minbucket_8_sd, summary_results$maxdepth_8_sd,\n    summary_results$alpha_8_sd, summary_results$validation_mse_8_sd, summary_results$oracle_r2_8_sd,\n    summary_results$acc_test_8_sd, summary_results$relaxed_acc_test_8_sd,\n    summary_results$top_acc_8_sd, summary_results$bottom_acc_8_sd,summary_results$recall_top_pct_e8_sd, summary_results$effpct_top_pct_e8_sd,  summary_results$time_8_sd\n  ),\n   Rpart_Any_k_Mean = c(\n    summary_results$k_rany, summary_results$minbucket_rany, summary_results$maxdepth_rany,\n    summary_results$cp_any, summary_results$validation_mse_rany, summary_results$oracle_r2_rany,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_rany, summary_results$bottom_acc_rany, summary_results$recall_top_pct_ro, summary_results$effpct_top_pct_ro, summary_results$time_rany\n  ),\n  Rpart_Any_k_SD = c(\n    summary_results$k_rany_sd, summary_results$minbucket_rany_sd, summary_results$maxdepth_rany_sd,\n    summary_results$cp_any_sd, summary_results$validation_mse_rany_sd, summary_results$oracle_r2_rany_sd,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_rany_sd, summary_results$bottom_acc_rany_sd, summary_results$recall_top_pct_ro_sd, summary_results$effpct_top_pct_ro_sd, summary_results$time_rany_sd\n  ),\n  Ev_Any_k_Mean = c(\n    summary_results$k_any, summary_results$minbucket_any, summary_results$maxdepth_any,\n    summary_results$alpha_any, summary_results$validation_mse_any, summary_results$oracle_r2_any,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_any, summary_results$bottom_acc_any,summary_results$recall_top_pct_eo, summary_results$effpct_top_pct_eo,  summary_results$time_any\n  ),\n  Ev_Any_k_SD = c(\n    summary_results$k_any_sd, summary_results$minbucket_any_sd, summary_results$maxdepth_any_sd,\n    summary_results$alpha_any_sd, summary_results$validation_mse_any_sd, summary_results$oracle_r2_any_sd,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_any_sd, summary_results$bottom_acc_any_sd, summary_results$recall_top_pct_eo_sd, summary_results$effpct_top_pct_eo_sd,  summary_results$time_any_sd\n  ),\n  group = c('Hyperparam.', 'Hyperparam.', 'Hyperparam.', 'Hyperparam.', \n            'Efficiency', 'Oracle perf.', 'Oracle perf.', 'Oracle perf.', 'Oracle perf.', 'Oracle perf.',\n            'Efficiency', 'Efficiency', 'Efficiency') |&gt; factor(levels = c('Hyperparam.', 'Efficiency', 'Oracle perf.'))\n)\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n## round\nsummary_table[, 2:9] &lt;- round(summary_table[,2:9], 3)\ntable_to_g &lt;- summary_table[,c('Metric', \n                               \"Rpart_Known_k_Mean\", \"Rpart_Known_k_SD\", \n                               \"Ev_Known_k_Mean\", \"Ev_Known_k_SD\", \n                               \"Rpart_Any_k_Mean\", \"Rpart_Any_k_SD\", \n                               \"Ev_Any_k_Mean\", \"Ev_Any_k_SD\", \"group\")] |&gt; \n  arrange(group)\n\n# Add group column to the beginning\ntable_to_g &lt;- table_to_g |&gt; relocate(group)\n\n# Row breaks after last row of each group\ngroup_lines &lt;- c(5, 9)\n\n# Create table with custom header\nkbl(table_to_g |&gt; dplyr::select(-group), col.names = NULL,\n    caption = \"Results of experimental testing. For each of the $N = 100$ samples, we obtained the best regression trees when forcing $k = 8$ or when $k$ was part of the tuned hyperparameters.\",\n    label = \"experiment\") %&gt;%\n  add_header_above(c(\n    \"Metric\" = 1,\n    \"Mean\" = 1, \"SD\" = 1,\n    \"Mean\" = 1, \"SD\" = 1,\n    \"Mean\" = 1, \"SD\" = 1,\n    \"Mean\" = 1, \"SD\" = 1\n  )) %&gt;%\n  add_header_above(c(\n    \" \" = 1,\n    \"Greedy tree \\n (rpart)\" = 2,\n    \"Optimal tree \\n (evtree)\" = 2,\n    \"Greedy tree \\n (rpart)\" = 2,\n    \"Optimal tree \\n (evtree)\" = 2\n  )) %&gt;% add_header_above(c(\n    \" \" = 1,\n    \"Known k\" = 4,\n    \"Unknown k\" = 4\n  ), escape = FALSE) %&gt;%\n  group_rows(index = table(table_to_g$group)) %&gt;%\n  row_spec(group_lines, extra_css = \"border-top: 2px solid black;\") %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\nKnown k\nUnknown k\n\n\n\nGreedy tree  (rpart)\nOptimal tree  (evtree)\nGreedy tree  (rpart)\nOptimal tree  (evtree)\n\n\nMetric\nMean\nSD\nMean\nSD\nMean\nSD\nMean\nSD\n\n\nResults of experimental testing. For each of the $N = 100$ samples, we obtained the best regression trees when forcing $k = 8$ or when $k$ was part of the tuned hyperparameters.\n\n  Hyperparam.\n\n    Number of Leaves (k) \n    8.000 \n    0.000 \n    8.000 \n    0.000 \n    14.290 \n    1.192 \n    12.290 \n    1.908 \n  \n  \n    Min Split \n    0.037 \n    0.010 \n    0.037 \n    0.010 \n    0.032 \n    0.005 \n    0.031 \n    0.005 \n  \n  \n    Max Depth \n    3.150 \n    0.359 \n    3.150 \n    0.359 \n    3.990 \n    0.100 \n    3.970 \n    0.171 \n  \n  \n    Alpha \n    0.005 \n    0.003 \n    1.470 \n    0.502 \n    0.000 \n    0.000 \n    1.390 \n    0.490 \n  \n  Efficiency\n\n    Validation BIC \n    90.892 \n    155.067 \n    Inf \n    NaN \n    39.851 \n    165.704 \n    7.816 \n    158.082 \n  \n  \n    Detect recall \n    0.951 \n    0.118 \n    0.959 \n    0.104 \n    0.907 \n    0.153 \n    0.926 \n    0.136 \n  \n  \n    Detect effective % \n    0.171 \n    0.068 \n    0.183 \n    0.072 \n    0.152 \n    0.042 \n    0.161 \n    0.052 \n  \n  \n    Time \n    0.016 \n    0.007 \n    23.083 \n    7.086 \n    0.015 \n    0.007 \n    43.037 \n    14.681 \n  \n  Oracle perf.\n\n    Oracle R² \n    0.905 \n    0.036 \n    0.904 \n    0.038 \n    0.897 \n    0.034 \n    0.894 \n    0.035 \n  \n  \n    Accuracy Test \n    0.601 \n    0.165 \n    0.649 \n    0.167 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Relaxed Accuracy Test \n    0.971 \n    0.051 \n    0.976 \n    0.047 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Top Accuracy \n    0.941 \n    0.076 \n    0.932 \n    0.079 \n    0.957 \n    0.064 \n    0.951 \n    0.067 \n  \n  \n    Bottom Accuracy \n    0.942 \n    0.077 \n    0.939 \n    0.076 \n    0.967 \n    0.056 \n    0.964 \n    0.051 \n  \n\n\n\n\n\n\n\n\n\nGrubinger, Thomas, Achim Zeileis, and Karl-Peter Pfeiffer. 2014. “Evtree: Evolutionary Learning of Globally Optimal Classification and Regression Trees in r.” Journal of Statistical Software 61 (1): 1–29. https://doi.org/10.18637/jss.v061.i01.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Partitioning</span>"
    ]
  },
  {
    "objectID": "5_partitioning.html#experiment-on-partioning-proxy-vulnerability-for-scenario-1",
    "href": "5_partitioning.html#experiment-on-partioning-proxy-vulnerability-for-scenario-1",
    "title": "5  Partitioning",
    "section": "5.3 Experiment on partioning proxy vulnerability for scenario 1",
    "text": "5.3 Experiment on partioning proxy vulnerability for scenario 1\nTo support the partitioning methodology proposed in section 6 of the main paper, we present a simulation study to identify best practices and gain insights into optimal implementation.\nFollowing the set of equations of Scenario 1 in Chapter 1, we simulate \\(M = 100\\) set of \\(N = 3000\\) samples split into \\((N_{\\text{train}}, N_{\\text{valid}}, N_{\\text{test}}) = (2000, 500, 500)\\) for train, validation, and test. Our aim is to assess the capacity of the methodology to recover proxy-vulnerable distinct subpopulations, precisely identify the most at-risk groups, and predict accurately the proxy vulnerability. We compute the BIC (under Gaussian assumption) of the estimated proxy vulnerability as compared with the test theoretical proxy vulnerability, and the accuracy in the partitioning as compared with the true proxy vulnerable groups : the eight subpopulations formed by the crossing of \\(\\{X_1 \\leq 1, X_1 &gt; 1\\}\\) and \\(X_2 \\in \\{1, 2, 3, 4\\}\\).\nOn each sample set, we estimated unaware and aware premiums using the methodology described Chapter 2 and we computed proxy vulnerability \\(\\widehat{\\Delta}_{\\text{proxy}}(x_1, x_2)\\) via Eq. 1 of the main paper. We partitioned the feature space using \\((X_1, X_2, D)\\). Models used rpart (locally optimal) and evtree (globally optimal) regression trees, with hyperparameters tuned via validation BIC (Gaussian assumption): minimum leaf size proportion \\(w \\in \\{0.03, 0.05\\}\\), tree depth \\(d \\in \\{3, 4\\}\\), and complexity parameter \\(\\alpha \\in \\{1, 2\\}\\). For rpart, we pruned a deep tree to minimize validation error. We compared performance under fixed (\\(k = 8\\)) and optimized leaf counts, retaining the best model per case for each implementation (four total).\n\n\nCode for the partition experiment.\nif (!file.exists(\"preds/proxy_sims_results.csv\")) {\n  source(\"___evtree_experiment.R\")\n  \n  # Load data\n  preds_sims_stats &lt;- jsonlite::fromJSON(\"preds/preds_sims_stats.json\")\n  \n  # Process data\n  proxy_sims_results &lt;- process_data_evtree_experiment(preds_sims_stats)\n  \n  # Save results to CSV\n  write.csv(proxy_sims_results, \"preds/proxy_sims_results.csv\", row.names = FALSE)\n  cat(\"Results saved to preds/proxy_sims_results.csv\")\n\n  } else {\n  proxy_sims_results &lt;- read_csv(\"preds/proxy_sims_results.csv\")\n  cat(\"Results read from preds/proxy_sims_results.csv\")\n}\n\n\nResults read from preds/proxy_sims_results.csv\n\n\nThe following table presents the results of our experiment. When the number of groups is correctly set to \\(k = 8\\), the \\(8\\times8\\) accuracy and relaxed \\(8\\times8\\) accuracy (which counts adjacent diagonals in the confusion matrix as correct) confirm that the method effectively groups individuals, with little difference between and . However, when \\(k\\) is unknown, \\(k = 8\\) was never seen as optimal. Validation metrics (based on estimated proxy vulnerability) favor larger \\(k\\), while oracle performance (based on theoretical proxy vulnerability) suggests better \\(R^2\\) at \\(k = 8\\). Whether \\(k\\) is known or not, partitioning identifies truly vulnerable individuals (top 12% by theoretical proxy vulnerability) with over 93% precision, fulfilling its primary objective. Since \\(k\\) is unknown in practice, strong regularization is essential to prevent excessive partitioning.\n\n\nCode for producing the experimental result table\nsummary_results &lt;- proxy_sims_results %&gt;% \n  summarise(\n  # Metrics for \"known k\" (k = 8) evtree\n  k_8 = mean(proxy_sims_results$num_leaf_8, na.rm = TRUE),\n  k_8_sd = sd(proxy_sims_results$num_leaf_8, na.rm = TRUE),\n  minbucket_8 = mean(proxy_sims_results$minbucket_8, na.rm = TRUE),\n  minbucket_8_sd = sd(proxy_sims_results$minbucket_8, na.rm = TRUE),\n  maxdepth_8 = mean(proxy_sims_results$maxdepth_8, na.rm = TRUE),\n  maxdepth_8_sd = sd(proxy_sims_results$maxdepth_8, na.rm = TRUE),\n  alpha_8 = mean(proxy_sims_results$alpha_8, na.rm = TRUE),\n  alpha_8_sd = sd(proxy_sims_results$alpha_8, na.rm = TRUE),\n  validation_mse_8 = mean(proxy_sims_results$validation_mse_8, na.rm = TRUE),\n  validation_mse_8_sd = sd(proxy_sims_results$validation_mse_8, na.rm = TRUE),\n  oracle_r2_8 = mean(proxy_sims_results$r2_oracle_8, na.rm = TRUE),\n  oracle_r2_8_sd = sd(proxy_sims_results$r2_oracle_8, na.rm = TRUE),\n  acc_test_8 = mean(proxy_sims_results$accuracy_test_8, na.rm = TRUE),\n  acc_test_8_sd = sd(proxy_sims_results$accuracy_test_8, na.rm = TRUE),\n  relaxed_acc_test_8 = mean(proxy_sims_results$relaxed_accuracy_test_8,\n                            na.rm = TRUE),\n  relaxed_acc_test_8_sd = sd(proxy_sims_results$relaxed_accuracy_test_8,\n                             na.rm = TRUE),\n  top_acc_8 = mean(proxy_sims_results$top_acc_8, na.rm = TRUE),\n  top_acc_8_sd = sd(proxy_sims_results$top_acc_8, na.rm = TRUE),\n  bottom_acc_8 = mean(proxy_sims_results$bottom_acc_8, na.rm = TRUE),\n  bottom_acc_8_sd = sd(proxy_sims_results$bottom_acc_8, na.rm = TRUE),\n  time_8 = mean(proxy_sims_results$time_8, na.rm = TRUE),  # Time assumed same for any/known\n  time_8_sd = sd(proxy_sims_results$time_8, na.rm = TRUE),\n  \n  \n    \n  # Metrics for \"known k\" (k = 8) rpart\n  k_r8 = mean(proxy_sims_results$num_leaf_r8, na.rm = TRUE),\n  k_r8_sd = sd(proxy_sims_results$num_leaf_r8, na.rm = TRUE),\n  minbucket_r8 = mean(proxy_sims_results$minbucket_r8, na.rm = TRUE),\n  minbucket_r8_sd = sd(proxy_sims_results$minbucket_r8, na.rm = TRUE),\n  maxdepth_r8 = mean(proxy_sims_results$maxdepth_r8, na.rm = TRUE),\n  maxdepth_r8_sd = sd(proxy_sims_results$maxdepth_r8, na.rm = TRUE),\n  cp_8 = mean(proxy_sims_results$cp_8, na.rm = TRUE),\n  cp_8_sd = sd(proxy_sims_results$cp_8, na.rm = TRUE),\n  validation_mse_r8 = mean(proxy_sims_results$validation_mse_r8, na.rm = TRUE),\n  validation_mse_r8_sd = sd(proxy_sims_results$validation_mse_r8, na.rm = TRUE),\n  oracle_r2_r8 = mean(proxy_sims_results$r2_oracle_r8, na.rm = TRUE),\n  oracle_r2_r8_sd = sd(proxy_sims_results$r2_oracle_r8, na.rm = TRUE),\n  acc_test_r8 = mean(proxy_sims_results$accuracy_test_r8, na.rm = TRUE),\n  acc_test_r8_sd = sd(proxy_sims_results$accuracy_test_r8, na.rm = TRUE),\n  relaxed_acc_test_r8 = mean(proxy_sims_results$relaxed_accuracy_test_r8, na.rm = TRUE),\n  relaxed_acc_test_r8_sd = sd(proxy_sims_results$relaxed_accuracy_test_r8, na.rm = TRUE),\n  top_acc_r8 = mean(proxy_sims_results$top_acc_r8, na.rm = TRUE),\n  top_acc_r8_sd = sd(proxy_sims_results$top_acc_r8, na.rm = TRUE),\n  bottom_acc_r8 = mean(proxy_sims_results$bottom_acc_r8, na.rm = TRUE),\n  bottom_acc_r8_sd = sd(proxy_sims_results$bottom_acc_r8, na.rm = TRUE),\n  time_r8 = mean(proxy_sims_results$time_r8, na.rm = TRUE),  # Time assumed same for any/known\n  time_r8_sd = sd(proxy_sims_results$time_r8, na.rm = TRUE),\n\n  # Metrics for \"any k\" rpart\n  k_rany = mean(proxy_sims_results$num_leaf_rany, na.rm = TRUE),\n  k_rany_sd = sd(proxy_sims_results$num_leaf_rany, na.rm = TRUE),\n  minbucket_rany = mean(proxy_sims_results$minbucket_rany, na.rm = TRUE),\n  minbucket_rany_sd = sd(proxy_sims_results$minbucket_rany, na.rm = TRUE),\n  maxdepth_rany = mean(proxy_sims_results$maxdepth_rany, na.rm = TRUE),\n  maxdepth_rany_sd = sd(proxy_sims_results$maxdepth_rany, na.rm = TRUE),\n  cp_any = mean(proxy_sims_results$cp_any, na.rm = TRUE),\n  cp_any_sd = sd(proxy_sims_results$cp_any, na.rm = TRUE),\n  validation_mse_rany = mean(proxy_sims_results$validation_mse_rany, na.rm = TRUE),\n  validation_mse_rany_sd = sd(proxy_sims_results$validation_mse_rany, na.rm = TRUE),\n  oracle_r2_rany = mean(proxy_sims_results$r2_oracle_rany, na.rm = TRUE),\n  oracle_r2_rany_sd = sd(proxy_sims_results$r2_oracle_rany, na.rm = TRUE),\n  top_acc_rany = mean(proxy_sims_results$top_acc_rany, na.rm = TRUE),\n  top_acc_rany_sd = sd(proxy_sims_results$top_acc_rany, na.rm = TRUE),\n  bottom_acc_rany = mean(proxy_sims_results$bottom_acc_rany, na.rm = TRUE),\n  bottom_acc_rany_sd = sd(proxy_sims_results$bottom_acc_rany, na.rm = TRUE),\n  time_rany = mean(proxy_sims_results$time_rany, na.rm = TRUE),  # Time is shared, no distinction\n  time_rany_sd = sd(proxy_sims_results$time_rany, na.rm = TRUE),\n  \n    # Metrics for \"any k\" evtree\n  k_any = mean(proxy_sims_results$num_leaf_any, na.rm = TRUE),\n  k_any_sd = sd(proxy_sims_results$num_leaf_any, na.rm = TRUE),\n  minbucket_any = mean(proxy_sims_results$minbucket_any, na.rm = TRUE),\n  minbucket_any_sd = sd(proxy_sims_results$minbucket_any, na.rm = TRUE),\n  maxdepth_any = mean(proxy_sims_results$maxdepth_any, na.rm = TRUE),\n  maxdepth_any_sd = sd(proxy_sims_results$maxdepth_any, na.rm = TRUE),\n  alpha_any = mean(proxy_sims_results$alpha_any, na.rm = TRUE),\n  alpha_any_sd = sd(proxy_sims_results$alpha_any, na.rm = TRUE),\n  validation_mse_any = mean(proxy_sims_results$validation_mse_any, na.rm = TRUE),\n  validation_mse_any_sd = sd(proxy_sims_results$validation_mse_any, na.rm = TRUE),\n  oracle_r2_any = mean(proxy_sims_results$r2_oracle_any, na.rm = TRUE),\n  oracle_r2_any_sd = sd(proxy_sims_results$r2_oracle_any, na.rm = TRUE),\n  top_acc_any = mean(proxy_sims_results$top_acc_any, na.rm = TRUE),\n  top_acc_any_sd = sd(proxy_sims_results$top_acc_any, na.rm = TRUE),\n  bottom_acc_any = mean(proxy_sims_results$bottom_acc_any, na.rm = TRUE),\n  bottom_acc_any_sd = sd(proxy_sims_results$bottom_acc_any, na.rm = TRUE),\n  time_any = mean(proxy_sims_results$time_any, na.rm = TRUE),  # Time is shared, no distinction\n  time_any_sd = sd(proxy_sims_results$time_any, na.rm = TRUE),\n  \n  # Add new metrics for eo, e8, ro, r8\n    recall_top_pct_eo = mean(proxy_sims_results$recall_top_pct_eo, na.rm = TRUE),\n    recall_top_pct_eo_sd = sd(proxy_sims_results$recall_top_pct_eo, na.rm = TRUE),\n    prec_top_pct_eo = mean(proxy_sims_results$prec_top_pct_eo, na.rm = TRUE),\n    prec_top_pct_eo_sd = sd(proxy_sims_results$prec_top_pct_eo, na.rm = TRUE),\n    acc_top_pct_eo = mean(proxy_sims_results$acc_top_pct_eo, na.rm = TRUE),\n    acc_top_pct_eo_sd = sd(proxy_sims_results$acc_top_pct_eo, na.rm = TRUE),\n    effpct_top_pct_eo = mean(proxy_sims_results$effpct_top_pct_eo, na.rm = TRUE),\n    effpct_top_pct_eo_sd = sd(proxy_sims_results$effpct_top_pct_eo, na.rm = TRUE),\n    \n    recall_top_pct_e8 = mean(proxy_sims_results$recall_top_pct_e8, na.rm = TRUE),\n    recall_top_pct_e8_sd = sd(proxy_sims_results$recall_top_pct_e8, na.rm = TRUE),\n    prec_top_pct_e8 = mean(proxy_sims_results$prec_top_pct_e8, na.rm = TRUE),\n    prec_top_pct_e8_sd = sd(proxy_sims_results$prec_top_pct_e8, na.rm = TRUE),\n    acc_top_pct_e8 = mean(proxy_sims_results$acc_top_pct_e8, na.rm = TRUE),\n    acc_top_pct_e8_sd = sd(proxy_sims_results$acc_top_pct_e8, na.rm = TRUE),\n    effpct_top_pct_e8 = mean(proxy_sims_results$effpct_top_pct_e8, na.rm = TRUE),\n    effpct_top_pct_e8_sd = sd(proxy_sims_results$effpct_top_pct_e8, na.rm = TRUE),\n    \n    recall_top_pct_ro = mean(proxy_sims_results$recall_top_pct_ro, na.rm = TRUE),\n    recall_top_pct_ro_sd = sd(proxy_sims_results$recall_top_pct_ro, na.rm = TRUE),\n    prec_top_pct_ro = mean(proxy_sims_results$prec_top_pct_ro, na.rm = TRUE),\n    prec_top_pct_ro_sd = sd(proxy_sims_results$prec_top_pct_ro, na.rm = TRUE),\n    acc_top_pct_ro = mean(proxy_sims_results$acc_top_pct_ro, na.rm = TRUE),\n    acc_top_pct_ro_sd = sd(proxy_sims_results$acc_top_pct_ro, na.rm = TRUE),\n    effpct_top_pct_ro = mean(proxy_sims_results$effpct_top_pct_ro, na.rm = TRUE),\n    effpct_top_pct_ro_sd = sd(proxy_sims_results$effpct_top_pct_ro, na.rm = TRUE),\n    \n    recall_top_pct_r8 = mean(proxy_sims_results$recall_top_pct_r8, na.rm = TRUE),\n    recall_top_pct_r8_sd = sd(proxy_sims_results$recall_top_pct_r8, na.rm = TRUE),\n    prec_top_pct_r8 = mean(proxy_sims_results$prec_top_pct_r8, na.rm = TRUE),\n    prec_top_pct_r8_sd = sd(proxy_sims_results$prec_top_pct_r8, na.rm = TRUE),\n    acc_top_pct_r8 = mean(proxy_sims_results$acc_top_pct_r8, na.rm = TRUE),\n    acc_top_pct_r8_sd = sd(proxy_sims_results$acc_top_pct_r8, na.rm = TRUE),\n    effpct_top_pct_r8 = mean(proxy_sims_results$effpct_top_pct_r8, na.rm = TRUE),\n    effpct_top_pct_r8_sd = sd(proxy_sims_results$effpct_top_pct_r8, na.rm = TRUE)\n)\n\n# Step 3: Transform results into a cleaner format\nsummary_table &lt;- data.frame(\n  Metric = c(\n    \"Number of Leaves (k)\", \"Min Split\", \"Max Depth\", \"Alpha\", \n    \"Validation BIC\", \"Oracle R²\", \"Accuracy Test\", \"Relaxed Accuracy Test\",\n    \"Top Accuracy\", \"Bottom Accuracy\", \"Detect recall\", \"Detect effective %\", \"Time\"\n  ),\n  Rpart_Known_k_Mean = c(\n    summary_results$k_r8, summary_results$minbucket_r8, summary_results$maxdepth_r8,\n    summary_results$cp_8, summary_results$validation_mse_r8, summary_results$oracle_r2_r8,\n    summary_results$acc_test_r8, summary_results$relaxed_acc_test_r8,\n    summary_results$top_acc_r8, summary_results$bottom_acc_r8, summary_results$recall_top_pct_r8, summary_results$effpct_top_pct_r8, summary_results$time_r8\n  ),\n  Rpart_Known_k_SD = c(\n    summary_results$k_r8_sd, summary_results$minbucket_r8_sd, summary_results$maxdepth_r8_sd,\n    summary_results$cp_8_sd, summary_results$validation_mse_r8_sd, summary_results$oracle_r2_r8_sd,\n    summary_results$acc_test_r8_sd, summary_results$relaxed_acc_test_r8_sd,\n    summary_results$top_acc_r8_sd, summary_results$bottom_acc_r8_sd,  summary_results$recall_top_pct_r8_sd, summary_results$effpct_top_pct_r8_sd, summary_results$time_r8_sd\n  ),\n  Ev_Known_k_Mean = c(\n    summary_results$k_8, summary_results$minbucket_8, summary_results$maxdepth_8,\n    summary_results$alpha_8, summary_results$validation_mse_8, summary_results$oracle_r2_8,\n    summary_results$acc_test_8, summary_results$relaxed_acc_test_8,\n    summary_results$top_acc_8, summary_results$bottom_acc_8,summary_results$recall_top_pct_e8, summary_results$effpct_top_pct_e8,  summary_results$time_8\n  ),\n  Ev_Known_k_SD = c(\n    summary_results$k_8_sd, summary_results$minbucket_8_sd, summary_results$maxdepth_8_sd,\n    summary_results$alpha_8_sd, summary_results$validation_mse_8_sd, summary_results$oracle_r2_8_sd,\n    summary_results$acc_test_8_sd, summary_results$relaxed_acc_test_8_sd,\n    summary_results$top_acc_8_sd, summary_results$bottom_acc_8_sd,summary_results$recall_top_pct_e8_sd, summary_results$effpct_top_pct_e8_sd,  summary_results$time_8_sd\n  ),\n   Rpart_Any_k_Mean = c(\n    summary_results$k_rany, summary_results$minbucket_rany, summary_results$maxdepth_rany,\n    summary_results$cp_any, summary_results$validation_mse_rany, summary_results$oracle_r2_rany,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_rany, summary_results$bottom_acc_rany, summary_results$recall_top_pct_ro, summary_results$effpct_top_pct_ro, summary_results$time_rany\n  ),\n  Rpart_Any_k_SD = c(\n    summary_results$k_rany_sd, summary_results$minbucket_rany_sd, summary_results$maxdepth_rany_sd,\n    summary_results$cp_any_sd, summary_results$validation_mse_rany_sd, summary_results$oracle_r2_rany_sd,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_rany_sd, summary_results$bottom_acc_rany_sd, summary_results$recall_top_pct_ro_sd, summary_results$effpct_top_pct_ro_sd, summary_results$time_rany_sd\n  ),\n  Ev_Any_k_Mean = c(\n    summary_results$k_any, summary_results$minbucket_any, summary_results$maxdepth_any,\n    summary_results$alpha_any, summary_results$validation_mse_any, summary_results$oracle_r2_any,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_any, summary_results$bottom_acc_any,summary_results$recall_top_pct_eo, summary_results$effpct_top_pct_eo,  summary_results$time_any\n  ),\n  Ev_Any_k_SD = c(\n    summary_results$k_any_sd, summary_results$minbucket_any_sd, summary_results$maxdepth_any_sd,\n    summary_results$alpha_any_sd, summary_results$validation_mse_any_sd, summary_results$oracle_r2_any_sd,\n    NA, NA,  # Accuracy metrics not available for Any k\n    summary_results$top_acc_any_sd, summary_results$bottom_acc_any_sd, summary_results$recall_top_pct_eo_sd, summary_results$effpct_top_pct_eo_sd,  summary_results$time_any_sd\n  ),\n  group = c('Hyperparam.', 'Hyperparam.', 'Hyperparam.', 'Hyperparam.', \n            'Efficiency', 'Oracle perf.', 'Oracle perf.', 'Oracle perf.', 'Oracle perf.', 'Oracle perf.',\n            'Efficiency', 'Efficiency', 'Efficiency') |&gt; factor(levels = c('Hyperparam.', 'Efficiency', 'Oracle perf.'))\n)\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n## round\nsummary_table[, 2:9] &lt;- round(summary_table[,2:9], 3)\ntable_to_g &lt;- summary_table[,c('Metric', \n                               \"Rpart_Known_k_Mean\", \"Rpart_Known_k_SD\", \n                               \"Ev_Known_k_Mean\", \"Ev_Known_k_SD\", \n                               \"Rpart_Any_k_Mean\", \"Rpart_Any_k_SD\", \n                               \"Ev_Any_k_Mean\", \"Ev_Any_k_SD\", \"group\")] |&gt; \n  arrange(group)\n\n# Add group column to the beginning\ntable_to_g &lt;- table_to_g |&gt; relocate(group)\n\n# Row breaks after last row of each group\ngroup_lines &lt;- c(5, 9)\n\n# Create table with custom header\nkbl(table_to_g |&gt; dplyr::select(-group), col.names = NULL,\n    caption = \"Results of experimental testing. For each of the $N = 100$ samples, we obtained the best regression trees when forcing $k = 8$ or when $k$ was part of the tuned hyperparameters.\",\n    label = \"experiment\") %&gt;%\n  add_header_above(c(\n    \"Metric\" = 1,\n    \"Mean\" = 1, \"SD\" = 1,\n    \"Mean\" = 1, \"SD\" = 1,\n    \"Mean\" = 1, \"SD\" = 1,\n    \"Mean\" = 1, \"SD\" = 1\n  )) %&gt;%\n  add_header_above(c(\n    \" \" = 1,\n    \"Greedy tree \\n (rpart)\" = 2,\n    \"Optimal tree \\n (evtree)\" = 2,\n    \"Greedy tree \\n (rpart)\" = 2,\n    \"Optimal tree \\n (evtree)\" = 2\n  )) %&gt;% add_header_above(c(\n    \" \" = 1,\n    \"Known k\" = 4,\n    \"Unknown k\" = 4\n  ), escape = FALSE) %&gt;%\n  group_rows(index = table(table_to_g$group)) %&gt;%\n  row_spec(group_lines, extra_css = \"border-top: 2px solid black;\") %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\nKnown k\nUnknown k\n\n\n\nGreedy tree  (rpart)\nOptimal tree  (evtree)\nGreedy tree  (rpart)\nOptimal tree  (evtree)\n\n\nMetric\nMean\nSD\nMean\nSD\nMean\nSD\nMean\nSD\n\n\nResults of experimental testing. For each of the $N = 100$ samples, we obtained the best regression trees when forcing $k = 8$ or when $k$ was part of the tuned hyperparameters.\n\n  Hyperparam.\n\n    Number of Leaves (k) \n    8.000 \n    0.000 \n    8.000 \n    0.000 \n    14.290 \n    1.192 \n    12.290 \n    1.908 \n  \n  \n    Min Split \n    0.037 \n    0.010 \n    0.037 \n    0.010 \n    0.032 \n    0.005 \n    0.031 \n    0.005 \n  \n  \n    Max Depth \n    3.150 \n    0.359 \n    3.150 \n    0.359 \n    3.990 \n    0.100 \n    3.970 \n    0.171 \n  \n  \n    Alpha \n    0.005 \n    0.003 \n    1.470 \n    0.502 \n    0.000 \n    0.000 \n    1.390 \n    0.490 \n  \n  Efficiency\n\n    Validation BIC \n    90.892 \n    155.067 \n    Inf \n    NaN \n    39.851 \n    165.704 \n    7.816 \n    158.082 \n  \n  \n    Detect recall \n    0.951 \n    0.118 \n    0.959 \n    0.104 \n    0.907 \n    0.153 \n    0.926 \n    0.136 \n  \n  \n    Detect effective % \n    0.171 \n    0.068 \n    0.183 \n    0.072 \n    0.152 \n    0.042 \n    0.161 \n    0.052 \n  \n  \n    Time \n    0.016 \n    0.007 \n    23.083 \n    7.086 \n    0.015 \n    0.007 \n    43.037 \n    14.681 \n  \n  Oracle perf.\n\n    Oracle R² \n    0.905 \n    0.036 \n    0.904 \n    0.038 \n    0.897 \n    0.034 \n    0.894 \n    0.035 \n  \n  \n    Accuracy Test \n    0.601 \n    0.165 \n    0.649 \n    0.167 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Relaxed Accuracy Test \n    0.971 \n    0.051 \n    0.976 \n    0.047 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Top Accuracy \n    0.941 \n    0.076 \n    0.932 \n    0.079 \n    0.957 \n    0.064 \n    0.951 \n    0.067 \n  \n  \n    Bottom Accuracy \n    0.942 \n    0.077 \n    0.939 \n    0.076 \n    0.967 \n    0.056 \n    0.964 \n    0.051 \n  \n\n\n\n\n\n\n\n\nGrubinger, Thomas, Achim Zeileis, and Karl-Peter Pfeiffer. 2014. “Evtree: Evolutionary Learning of Globally Optimal Classification and Regression Trees in r.” Journal of Statistical Software 61 (1): 1–29. https://doi.org/10.18637/jss.v061.i01.\n\n\nLin, Jimmy, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. 2022. “Generalized and Scalable Optimal Sparse Decision Trees.” arXiv Preprint arXiv:2006.08690. https://arxiv.org/abs/2006.08690.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Partitioning</span>"
    ]
  },
  {
    "objectID": "5_partitioning.html#partitioning-policyholders-following-proxy-vulnerability",
    "href": "5_partitioning.html#partitioning-policyholders-following-proxy-vulnerability",
    "title": "5  Partitioning",
    "section": "5.2 Partitioning policyholders following proxy vulnerability",
    "text": "5.2 Partitioning policyholders following proxy vulnerability\n\n\n\nRelated section\nPre-pricing partitioning is illustrated in Section 6.1 of the main paper.\n\nWe apply an optimal partitioning algorithm, evtree from Grubinger, Zeileis, and Pfeiffer (2014), to policyholders based on proxy vulnerability in the three scenarios of the example. We use \\((X_1, X_2)\\) as the feature space for partitioning and impose strong regularization to limit the number of groups.\nFigure 5.1 presents the results for the three scenarios. The top row shows estimated proxy vulnerability, with colors indicating the groups resulting from the optimal partition of proxy vulnerability. While the left panel may not match intuition in terms of the number of groups in scenario 1, the predicted values of proxy vulnerability based on the evtree align with expectations: darker red indicates individuals most vulnerable to proxy effects. The bottom row of Figure 5.1 depicts the partition in the \\((x_1, x_2)\\) domain. The structure aligns with the example design: high proxy vulnerability for individuals with \\(x_2 = 4\\) and large \\(x_1\\), and important variation in proxy vulnerability across \\(x_2\\).\n\n\nTraining the evtrees per scenario\nsource(\"___train_evtree_scenario.R\")\n\npregroup_pop_stats_small &lt;- setNames(nm = names(pregroup_pop_stats)) %&gt;% lapply(function(pop_name){\n  setNames(nm = names(pregroup_pop_stats[[pop_name]])) %&gt;% lapply(function(the_set){\n    the_frac &lt;- ifelse(the_set == 'train', 0.1 , 1)\n    pregroup_pop_stats[[pop_name]][[the_set]] %&gt;% \n      sample_frac(the_frac)\n  })\n})\n\n# Define hyperparameter grid\nparam_grid &lt;- expand.grid(\n  minbucket = c(0.03, 0.05) * nrow(pregroup_pop_stats_small$Scenario1$train), \n  maxdepth = c(3, 4),\n  alpha = c(1, 2),\n  ntrees = 25,\n  stringsAsFactors = FALSE\n)\n\n\noutput_dir &lt;- \"evtree\" # Directory to save models\nresponse_vars &lt;- c(\"proxy_vuln\", 'comm_load') # List of response variables\n  \n# Call process_populations with actual inputs\nmy_trees &lt;- process_populations(preds_pop_stats = pregroup_pop_stats_small,\n                                response_vars = response_vars,\n                                param_grid = param_grid,\n                                output_dir)\n\n\nINFO [2025-10-07 15:46:19] Processing response variable: proxy_vuln\nINFO [2025-10-07 15:46:19] Processing population: Scenario1 for response: proxy_vuln\nINFO [2025-10-07 15:46:19] Model for population Scenario1 and response proxy_vuln already exists. Loading...\nINFO [2025-10-07 15:46:20] Processing population: Scenario2 for response: proxy_vuln\nINFO [2025-10-07 15:46:20] Model for population Scenario2 and response proxy_vuln already exists. Loading...\nINFO [2025-10-07 15:46:21] Processing population: Scenario3 for response: proxy_vuln\nINFO [2025-10-07 15:46:21] Model for population Scenario3 and response proxy_vuln already exists. Loading...\nINFO [2025-10-07 15:46:22] Processing response variable: comm_load\nINFO [2025-10-07 15:46:22] Processing population: Scenario1 for response: comm_load\nINFO [2025-10-07 15:46:22] Model for population Scenario1 and response comm_load already exists. Loading...\nINFO [2025-10-07 15:46:25] Processing population: Scenario2 for response: comm_load\nINFO [2025-10-07 15:46:25] Model for population Scenario2 and response comm_load already exists. Loading...\nINFO [2025-10-07 15:46:26] Processing population: Scenario3 for response: comm_load\nINFO [2025-10-07 15:46:26] Model for population Scenario3 and response comm_load already exists. Loading...\n\n\n\n\nCode for the visualisation of the evtrees\nlibrary(rpart)\nlibrary(ggparty, partykit)\n\ntemp_tree &lt;- c('evtree', 'rpart') %&gt;% lapply(function(the_algo){\n  names(pregroup_grid_stats) %&gt;% lapply(function(pop_name){\npop_id &lt;- which(names(pregroup_grid_stats) == pop_name)\n\n# Compute sequential terminal node IDs\nparty_tree &lt;- my_trees$proxy_vuln[[pop_name]][[paste0('best_', the_algo)]]$model\nif (the_algo == 'rpart'){\nparty_tree &lt;- partykit::as.party(party_tree)\n}\n\nterminal_ids &lt;- nodeids(party_tree, terminal = TRUE)  # Original terminal node IDs\nsequential_ids &lt;- seq_along(terminal_ids)  # Create sequential IDs\nid_mapping &lt;- data.frame(terminal_id = terminal_ids, sequential_id = sequential_ids)\n\n\n## Compute average prediction per terminal node\n# Extract predictions and terminal node IDs\npredictions &lt;- fitted(party_tree)\navg_prediction &lt;- aggregate(`(response)` ~ `(fitted)`,\n                            data = predictions,\n                            FUN = mean)\n\ntree_plot &lt;- ggparty(party_tree) +\n  geom_edge() +\n  geom_edge_label(mapping = aes(label = !!sym(\"breaks_label\")),\n                  size = 3) +\n  geom_node_label(\n    line_list = list(\n      aes(label = splitvar),\n      aes(label = paste(\"N =\", nodesize))\n    ),\n    line_gpar = list(\n      list(size = 10),\n      list(size = 8)\n    ),\n    ids = \"inner\",\n  ) +\n  geom_node_label(\n    line_list = list(\n      aes(label = paste0(\"Node \",\n                         match(id, id_mapping$terminal_id),\n                         \", N = \",\n                         nodesize)),\n      aes(label = paste0(\"Avg Pred. = \",\n                                     round(avg_prediction$`(response)`[match(id, avg_prediction$`(fitted)`)], 2)))\n    ),\n    line_gpar = list(\n      list(size = 8),\n      list(size = 10)\n    ),\n    ids = \"terminal\", nudge_y = -0.45, nudge_x = 0.01,\n    label.size = 0.15,\n    size = 3\n  ) +\n  geom_node_plot(\n    gglist = list(\n      geom_boxplot(aes(x = \"\", y = resp,\n                       color = ..middle..,\n                       fill = ..middle..),  # Color by median\n                   outlier.color = \"black\"\n                   , alpha = 0.7\n                   ),\n      theme_minimal(),\n      scale_fill_gradient2(\n        low = \"#D7CC39\", mid = \"grey75\", high = \"#CAA8F5\",\n        midpoint = 0, name = \"Median Value\"\n      ),\n      scale_color_gradient2(\n        low = colorspace::darken(\"#D7CC39\", 0.3), mid = colorspace::darken(\"grey75\", 0.3),\n        high = colorspace::darken(\"#CAA8F5\", 0.3),\n      midpoint = 0, name = \"Median Value\"\n      ),\n      xlab(\"\"), ylab(latex2exp::TeX(\"$\\\\widehat{\\\\Delta}_{proxy}(X_1, X_2)$\")),\n      scale_y_continuous(labels = scales::dollar),\n      theme(axis.text.x = element_blank(),\n            axis.title.y = element_text(margin = margin(l = -10)),\n            axis.title.x = element_text(margin = margin(r = 20)))\n    ),\n    shared_axis_labels = TRUE\n  ) +\n  ggtitle(latex2exp::TeX(paste0('Partition of proxy vulnerable individuals for scenario ', pop_id))) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5)\n  )\n  \n}) %&gt;% ggpubr::ggarrange(plotlist = .,\n                           nrow = 3,\n                           widths = 15, heights = 1,\n                           common.legend = T,\n                           legend = 'right') %&gt;% \nggsave(filename = paste0(\"figs/graph_trees_\", the_algo,\"_proxy.png\"),\n       plot = .,\n       height = 16,\n       width = 12,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n})\nrm(temp_tree)\n\n\n\n5.2.1 Saving important quantites\n\n\nSaving dictionnaries of partition rules and predictions\ndictionnary_leaves_trees &lt;- setNames(nm = names(my_trees)) %&gt;%  lapply(function(resp_tree){\n  setNames(nm = names(my_trees[[resp_tree]])) %&gt;% lapply(function(pop_name){\n      temp_to_pred &lt;- pregroup_grid_stats[[pop_name]]\n      names(temp_to_pred) &lt;- toupper(names(temp_to_pred))\n      \n      model_ev &lt;- my_trees[[resp_tree]][[pop_name]]$best_evtree$model\n      model_rpart &lt;- my_trees[[resp_tree]][[pop_name]]$best_rpart$model \n      model_rpart_prune &lt;- prune(model_rpart,\n                        cp = model_rpart$cptable[which(model_rpart$cptable[, \"nsplit\"] + 1 == 8), \"CP\"]) %&gt;% as.party()\n      model_rpart &lt;- model_rpart %&gt;% as.party()\n      \n      to_return_evtree &lt;- data.frame('node_or' = predict(model_ev, newdata = temp_to_pred,\n                                  type = 'node') %&gt;% unname,\n                                  'pred' = predict(model_ev, newdata = temp_to_pred,\n                                                   type = 'response') %&gt;% unname %&gt;%\n                                    round(3)) %&gt;% distinct() %&gt;% arrange(-pred) %&gt;%\n        mutate('node_new' = 1:n())\n      \n      to_return_rpart &lt;- data.frame('node_or' = predict(model_rpart, newdata = temp_to_pred,\n                                  type = 'node') %&gt;% unname,\n                                  'pred' = predict(model_rpart, newdata = temp_to_pred, type = 'response') %&gt;% unname %&gt;%\n                                    round(3)) %&gt;% distinct() %&gt;% arrange(-pred) %&gt;%\n        mutate('node_new' = 1:n())\n      \n      to_return_rpart_prune &lt;- data.frame('node_or' = predict(model_rpart_prune, newdata = temp_to_pred,\n                                  type = 'node') %&gt;% unname,\n                                  'pred' = predict(model_rpart_prune, newdata = temp_to_pred,\n                                                   type = 'response') %&gt;% unname %&gt;%\n                                    round(3)) %&gt;% distinct() %&gt;% arrange(-pred) %&gt;%\n        mutate('node_new' = 1:n())\n      \n      paths_ev &lt;- extract_paths_for_all_terminals(tree = model_ev)\n      paths_rpart &lt;- extract_paths_for_all_terminals(tree = model_rpart)\n      paths_rpart_prune &lt;- extract_paths_for_all_terminals(tree = model_rpart_prune)\n      \n       list('evtree' = list('dict' = to_return_evtree,\n                            'model' = model_ev,\n                            'paths' = paths_ev),\n           'rpart' = list('dict' = to_return_rpart,\n                          'model' = model_rpart,\n                          'paths' = paths_rpart),\n           'rpart_prune' = list('dict' = to_return_rpart_prune,\n                                'model' = model_rpart_prune,\n                                'paths' = paths_rpart_prune))\n    })\n})\n\nsaveRDS(dictionnary_leaves_trees, 'evtree/dictionnary_leaves_trees.rds')\n\n### Applying partition to the data\ngroup_grid_path = 'preds/group_grid_stats.json'\ngroup_pop_path = 'preds/group_pop_stats.json'\n\n\n# Check and load or compute group_grid_stats\nif (file.exists(group_grid_path)) {\n  temp_grid_stats &lt;- fromJSON(group_grid_path) \n  \n  \n  group_grid_stats &lt;- setNames(nm = names(temp_grid_stats)) |&gt; lapply(function(pop_name){\n    temp_grid_stats[[pop_name]] |&gt; \n      mutate(proxy_g_evtree = proxy_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_evtree)), decreasing = T)),\n           proxy_g_rpart = proxy_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_rpart)), decreasing = T)),\n           cload_g_evtree = cload_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_evtree)), decreasing = T)),\n           cload_g_rpart = cload_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_rpart)), decreasing = T)))\n  })\n  \n  rm(temp_grid_stats)\n    \n} else {\n  group_grid_stats &lt;- setNames(nm = names(pregroup_grid_stats)) %&gt;% lapply(function(pop_name){\n    temp_to_pred &lt;-   pregroup_grid_stats[[pop_name]]\n    names(temp_to_pred) &lt;- toupper(names(temp_to_pred))\n    \n    pred_proxy_g_evtree &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_proxy_g_rpart &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_evtree &lt;- predict(my_trees$comm_load[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_rpart &lt;- predict(my_trees$comm_load[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    \n    data.frame(pregroup_grid_stats[[pop_name]],\n                 proxy_g_evtree = pred_proxy_g_evtree %&gt;% factor(., levels = sort(unique(pred_proxy_g_evtree), decreasing = T)),\n                 proxy_g_rpart = pred_proxy_g_rpart %&gt;% factor(., levels = sort(unique(pred_proxy_g_rpart), decreasing = T)),\n                 cload_g_evtree = pred_cload_g_evtree %&gt;% factor(., levels = sort(unique(pred_cload_g_evtree), decreasing = T)),\n                 cload_g_rpart = pred_cload_g_rpart %&gt;% factor(., levels = sort(unique(pred_cload_g_rpart), decreasing = T))\n               )\n  })\n  toJSON(group_grid_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(group_grid_path)\n}\n\n# Check and load or compute group_pop_stats\nif (file.exists(group_pop_path)) {\n   temp_pop_stats &lt;- fromJSON(group_pop_path) \n  \n  \n  group_pop_stats &lt;- setNames(nm = names(temp_pop_stats)) |&gt; lapply(function(pop_name){\n    setNames(nm = names(temp_pop_stats[[pop_name]])) |&gt; lapply(function(the_set){\n      temp_pop_stats[[pop_name]][[the_set]] |&gt; \n        mutate(proxy_g_evtree = proxy_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_evtree)), decreasing = T)),\n           proxy_g_rpart = proxy_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(proxy_g_rpart)), decreasing = T)),\n           cload_g_evtree = cload_g_evtree %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_evtree)), decreasing = T)),\n           cload_g_rpart = cload_g_rpart %&gt;% factor(., levels = sort(unique(as.numeric(cload_g_rpart)), decreasing = T)))\n    }) \n  })\n  \n  rm(temp_pop_stats)\n  \n} else {\n  group_pop_stats &lt;- setNames(nm = names(pregroup_pop_stats)) %&gt;% lapply(function(pop_name){\n    setNames(nm = names(pregroup_pop_stats[[pop_name]])) %&gt;% lapply(function(set){\n      \n      temp_to_pred &lt;- pregroup_pop_stats[[pop_name]][[set]]\n      \n    pred_proxy_g_evtree &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_proxy_g_rpart &lt;- predict(my_trees$proxy_vuln[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_evtree &lt;- predict(my_trees$comm_load[[pop_name]]$best_evtree$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    pred_cload_g_rpart &lt;- predict(my_trees$comm_load[[pop_name]]$best_rpart$model,\n                                          newdata = temp_to_pred) %&gt;% round(3) %&gt;% unname\n    \n    data.frame(pregroup_pop_stats[[pop_name]][[set]],\n                 proxy_g_evtree = pred_proxy_g_evtree %&gt;% factor(., levels = sort(unique(pred_proxy_g_evtree), decreasing = T)),\n                 proxy_g_rpart = pred_proxy_g_rpart %&gt;% factor(., levels = sort(unique(pred_proxy_g_rpart), decreasing = T)),\n                cload_g_evtree = pred_cload_g_evtree %&gt;% factor(., levels = sort(unique(pred_cload_g_evtree), decreasing = T)),\n                 cload_g_rpart = pred_cload_g_rpart %&gt;% factor(., levels = sort(unique(pred_cload_g_rpart), decreasing = T))\n                )\n    })  \n})\n  toJSON(group_pop_stats, pretty = TRUE, auto_unbox = TRUE) %&gt;% \n    write(group_pop_path)\n}\n\n\n\n\n5.2.2 Visualizing the partition\n\n\nGraph for the illustration of the partitioning\nn_bottom &lt;- 5\nn_top &lt;- 5\n\nsetNames(nm = names(group_pop_stats)) %&gt;%  lapply(function(pop_name){\n  ## the colors\npop_id &lt;- which(names(group_pop_stats) == pop_name)\n    \nlocal_to_g &lt;- group_grid_stats[[pop_name]] %&gt;% \nfilter(x1 &lt;= 8, x1 &gt;= -5, d == 1) \n\nif(pop_name == head(names(group_grid_stats), 1)){\n  the_y_scale_top &lt;- scale_y_continuous(labels = scales::dollar, breaks = c(-5, 0, 5, 10), limits = c(-6, 14))\n  the_y_label_top &lt;- latex2exp::TeX(\"$\\\\Delta_{proxy}(x_1, x_2)$\")\n  the_y_scale &lt;- scale_y_discrete()\n  the_y_label &lt;- latex2exp::TeX('$x_2$')\n} else {\n  the_y_scale_top &lt;- scale_y_continuous(labels = NULL, breaks = c(-5, 0, 5, 10), limits = c(-6, 14))\n  the_y_label_top &lt;- NULL\n    the_y_scale &lt;-scale_y_discrete(labels = NULL)\n  the_y_label &lt;- NULL\n}\n\nlocal_pop_g &lt;- group_pop_stats[[pop_name]]$valid\n\nlocal_to_g$proxy_g_evtree_g &lt;- local_to_g$proxy_g_evtree\nlocal_pop_g$proxy_g_evtree_g &lt;- local_pop_g$proxy_g_evtree\nlevels(local_to_g$proxy_g_evtree_g) &lt;- recode_bottom_top_middle(levels = levels(local_to_g$proxy_g_evtree), \n                                                          numeric = TRUE, \n                                                          n_bottom = n_bottom, \n                                                          n_top = n_top, \n                                                          factor_values = local_to_g$proxy_g_evtree)\nlevels(local_pop_g$proxy_g_evtree_g) &lt;- recode_bottom_top_middle(levels = levels(local_pop_g$proxy_g_evtree), \n                                                          numeric = TRUE, \n                                                          n_bottom = n_bottom, \n                                                          n_top = n_top, \n                                                          factor_values = local_pop_g$proxy_g_evtree)\n\nlevels(local_to_g$proxy_g_evtree_g)[n_top + 1] &lt;- levels(local_pop_g$proxy_g_evtree_g)[n_top + 1]\n\ng_proxy &lt;- local_to_g %&gt;% \n  mutate(code = paste0(x2, '_', as.numeric(proxy_g_evtree_g))) %&gt;% \n  ggplot(aes(x = x1, y = proxy_vuln,\n             group = factor(code),\n             color = factor(proxy_g_evtree_g))) + \n  geom_line(aes(x = x1, y = proxy_vuln_t,\n                lty = factor(x2), group = factor(x2)),\n                color = 'black', size = 0.8) +\n  geom_line(size = 3, alpha = 0.78, lineend = \"round\", linejoin = \"round\") + \n  theme_classic() + \n  labs(x = latex2exp::TeX('$x_1$'),\n       y = the_y_label_top,\n       title = paste0('Scenario ', pop_id)) + \n  scale_color_manual(values = RColorBrewer::brewer.pal(n_bottom + n_top + 1, 'Spectral')  %&gt;% colorspace::darken(0.05),\n                     name = latex2exp::TeX('$\\\\widehat{\\\\Delta}^{ev}_{proxy}(\\\\textbf{x})$'),\n                     labels = levels(local_to_g$proxy_g_evtree_g) %&gt;% as.numeric %&gt;% round(2)) + \n  scale_linetype_manual(values = c('12',  '21', '32', 'solid'), name = latex2exp::TeX('$x_2$')) +\n  the_y_scale_top + \n  # geom_abline(slope = 0, intercept = 0, lty = '34', color= 'black', size= 0.7, alpha = 0.2) + \n  scale_x_continuous(labels = NULL, breaks = c(-3:3)*3 + 1) + # see above \n  guides(\n    linetype = guide_legend(order = 1), # x2 legend on top\n    color = guide_legend(order = 2)    # k legend below x2\n  )\n\ng_population &lt;- local_pop_g %&gt;%  \n  ggplot(aes(y = factor(X2), x = X1,\n             color = factor(proxy_g_evtree_g),\n             fill = factor(proxy_g_evtree_g))) + \n  geom_jitter(#position = position_identity(), \n              width = 0, height = 0.4, alpha = 0.2) + \n  scale_color_manual(values = RColorBrewer::brewer.pal(n_bottom + n_top + 1, 'Spectral') %&gt;% colorspace::darken(0.05),\n                     name = latex2exp::TeX('$\\\\widehat{\\\\Delta}^{ev}_{proxy}(\\\\textbf{x})$'),\n                     labels = levels(local_to_g$proxy_g_evtree) %&gt;% as.numeric %&gt;% round(1)) + \n  scale_fill_brewer(palette = 'Spectral', name = latex2exp::TeX('$k$')) + \n  theme_classic() + \n  the_y_scale +\n  scale_x_continuous(breaks = c(-3:3)*3 + 1, limits = c(-5, 8)) + \n  labs(x = latex2exp::TeX(\"$x_1$\") ,\n       y = the_y_label) + \n  theme( axis.title.y = element_text(\n      margin = margin(t = 50), # Add padding\n    )) \nggpubr::ggarrange(g_proxy, g_population, \n                  nrow = 2, common.legend = T,\n                  legend = 'right',\n                  heights = c(4, 3),\n                  align = \"v\") \n}) %&gt;%  \n  ggpubr::ggarrange(plotlist = .,\n                    ncol = 3,\n                    widths = c(6, 5, 5)) %&gt;% \n  ggsave(filename = \"figs/graph_proxy_clusters_and_pop_scenario.png\",\n       plot = .,\n       height = 6.25,\n       width = 11.50,\n       units = \"in\",\n       device = \"png\", dpi = 500)\n\n\n\n\n\nFigure 5.1: Partition of proxy vulnerability across the three scenarios (columns) in the example. The top row compares theoretical proxy vulnerability (black) with estimated values from the lightgbm, the latter colored by the group formed by the tree. The bottom row shows the partition in the \\((x_1, x_2)\\) domain, with noise added to~\\(x_2\\) for clarity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Partitioning</span>"
    ]
  },
  {
    "objectID": "6_integrated_framework.html#download-disparity-tables",
    "href": "6_integrated_framework.html#download-disparity-tables",
    "title": "6  Integrated framework",
    "section": "",
    "text": "Related section\nThe comprehensive monitoring table of the case study is illustrated in Figure 14 of the main paper.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can download the Excel disparity tables for each scenario below.\n\n\n\n ⬇ Scenario 1 \n ⬇ Scenario 2 \n ⬇ Scenario 3 \n\n\n\n\nFigure 6.1: Summary of the methodology of the paper",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Integrated framework</span>"
    ]
  },
  {
    "objectID": "9_references.html",
    "href": "9_references.html",
    "title": "References",
    "section": "",
    "text": "Open main paper 🔗\n\n  \n\n\n\n\n\n\n\n\n\nChevalier, Dominik, and Marie-Pier Côté. 2024. “From Point to\nProbabilistic Gradient Boosting for Claim Frequency and Severity\nPrediction.” arXiv Preprint arXiv:2412.14916. https://arxiv.org/abs/2412.14916.\n\n\nCôté, Marie-Pier, Olivier Côté, and Arthur Charpentier. 2024.\n“Selection Bias in Insurance: Why Portfolio-Specific Fairness\nFails to Extend Market-Wide.” Available at SSRN: 10.2139/Ssrn.5018749.\n\n\nFernandes Machado, Agathe, Suzie Grondin, Philipp Ratz, Arthur\nCharpentier, and François Hu. 2025. “EquiPy: Sequential Fairness\nUsing Optimal Transport in Python.” arXiv Preprint arXiv:2503.09866. https://arxiv.org/abs/2503.09866.\n\n\nGrubinger, Thomas, Achim Zeileis, and Karl-Peter Pfeiffer. 2014.\n“Evtree: Evolutionary Learning of Globally Optimal Classification\nand Regression Trees in r.” Journal of Statistical\nSoftware 61 (1): 1–29. https://doi.org/10.18637/jss.v061.i01.\n\n\nHu, Francois, Philipp Ratz, and Arthur Charpentier. 2024. “A\nSequentially Fair Mechanism for Multiple Sensitive Attributes.”\nProceedings of the AAAI Conference on Artificial Intelligence\n38 (11): 12502–10. https://doi.org/10.1609/aaai.v38i11.29143.\n\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. “LightGBM: A Highly\nEfficient Gradient Boosting Decision Tree.” Advances in\nNeural Information Processing Systems 30: 3146–54.\n\n\nLin, Jimmy, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer.\n2022. “Generalized and Scalable Optimal Sparse Decision\nTrees.” arXiv Preprint arXiv:2006.08690. https://arxiv.org/abs/2006.08690.\n\n\nLindholm, Mathias, Ronald Richman, Andreas Tsanakas, and Mario V\nWüthrich. 2022. “Discrimination-Free Insurance Pricing.”\nASTIN Bulletin 52 (1): 55–89.\n\n\nLindholm, M., R. Richman, A. Tsanakas, and M. V. Wüthrich. 2024.\n“Sensitivity-Based Measures of Discrimination in Insurance\nPricing.” Available at SSRN: Ssrn.com/Abstract=4897265.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "2_training_spectrum.html#sec-est_best",
    "href": "2_training_spectrum.html#sec-est_best",
    "title": "2  Estimating the five fairness premiums",
    "section": "",
    "text": "Related section\nThe estimation strategy for the five families of fair premium is described in Section 4.3 of the main paper.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimating the five fairness premiums</span>"
    ]
  },
  {
    "objectID": "4_dimensions.html#measuring-actuarial-fairness-disparities",
    "href": "4_dimensions.html#measuring-actuarial-fairness-disparities",
    "title": "4  Measuring the dimensions of fairness",
    "section": "",
    "text": "Related section\nThe measurement associated with the dimensions of fairness are described in Section 4.3 of the main paper.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring the dimensions of fairness</span>"
    ]
  },
  {
    "objectID": "5_partitioning.html#supervised-partitioning-for-fairness-insights",
    "href": "5_partitioning.html#supervised-partitioning-for-fairness-insights",
    "title": "5  Partitioning",
    "section": "",
    "text": "Related section\nThe partitioning methodology is introduced in Section 6 of the main paper.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Partitioning</span>"
    ]
  }
]